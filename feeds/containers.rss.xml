<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>visibilityspots - containers</title><link>https://visibilityspots.github.io/blog/</link><description>Linux &amp; Open-Source enthusiast | Scouting | Longboarding</description><lastBuildDate>Thu, 27 Jul 2023 00:00:00 +0200</lastBuildDate><item><title>Nomad spread scheduler</title><link>https://visibilityspots.github.io/blog/nomad-spread-scheduler.html</link><description>&lt;p&gt;I'm maintaining a &lt;a href="../nomad-arm-cluster.html"&gt;nomad cluster&lt;/a&gt; already a few years now at home, based on some thin clients and a few raspberry pi's.&lt;/p&gt;
&lt;p&gt;The workload is growing from uses cases of the &lt;a href="../planespotting.html"&gt;plane-spotting&lt;/a&gt; services towards a &lt;a href="../dockerized-cloudflared-pi-hole.html"&gt;pi-hole&lt;/a&gt; setup, &lt;a href="https://github.com/dani-garcia/vaultwarden"&gt;vaultwarden&lt;/a&gt;, &lt;a href="https://www.home-assistant.io/"&gt;homeassistant&lt;/a&gt; and many more use cases.&lt;/p&gt;
&lt;p&gt;One of the issues I encountered was based on the default &lt;a href="https://www.nomadproject.io/docs/concepts/scheduling/scheduling"&gt;scheduling algorithm&lt;/a&gt;. Raspberry pi's are not known as the most efficient solution to run a huge workload. Default nomad will schedule new containers on one compute node until the resource limits of that node are consumed and only then will start consuming another node. This …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Wed, 05 Jul 2023 22:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2023-07-05:/blog/nomad-spread-scheduler.html</guid><category>containers</category><category>nomad</category><category>hashicorp</category><category>container</category><category>orchestration</category><category>spread</category><category>algorithm</category><category>scheduling</category></item><item><title>Traefik SSL grading</title><link>https://visibilityspots.github.io/blog/traefik-ssl-grading.html</link><description>&lt;p&gt;Recently I discovered that many of the services I deployed upon my &lt;a href="https://visibilityspots.github.io/blog/nomad-arm-cluster.html"&gt;nomad cluster&lt;/a&gt;  didn't had the SSL A grading I expected them to have. Somehow I asumed the &lt;a href="https://visibilityspots.github.io/blog/traefik-nomad-route53.html"&gt;traefik letsencrypt&lt;/a&gt; implementation got the A rating by default.&lt;/p&gt;
&lt;p&gt;After running the &lt;a href="https://github.com/drwetter/testssl.sh"&gt;testssl.sh&lt;/a&gt; container it turns out they don't;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$&lt;span class="w"&gt; &lt;/span&gt;docker&lt;span class="w"&gt; &lt;/span&gt;run&lt;span class="w"&gt; &lt;/span&gt;--rm&lt;span class="w"&gt; &lt;/span&gt;-ti&lt;span class="w"&gt; &lt;/span&gt;drwetter/testssl.sh&lt;span class="w"&gt; &lt;/span&gt;domain.org
&lt;span class="w"&gt; &lt;/span&gt;Rating&lt;span class="w"&gt; &lt;/span&gt;specs&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;not&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;complete&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;SSL&lt;span class="w"&gt; &lt;/span&gt;Labs&lt;span class="s1"&gt;&amp;#39;s &amp;#39;&lt;/span&gt;SSL&lt;span class="w"&gt; &lt;/span&gt;Server&lt;span class="w"&gt; &lt;/span&gt;Rating&lt;span class="w"&gt; &lt;/span&gt;Guide&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;version&lt;span class="w"&gt; &lt;/span&gt;2009q&lt;span class="w"&gt; &lt;/span&gt;from&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;2020&lt;/span&gt;-01-30&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;Specification&lt;span class="w"&gt; &lt;/span&gt;documentation&lt;span class="w"&gt;  &lt;/span&gt;https://github.com/ssllabs/research/wiki/SSL-Server-Rating-Guide
&lt;span class="w"&gt; &lt;/span&gt;Protocol&lt;span class="w"&gt; &lt;/span&gt;Support&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;weighted&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="m"&gt;95&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;28&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;Key&lt;span class="w"&gt; &lt;/span&gt;Exchange&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;weighted&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;Cipher&lt;span class="w"&gt; &lt;/span&gt;Strength&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;weighted&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="m"&gt;90&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;36&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;Final&lt;span class="w"&gt; &lt;/span&gt;Score …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Sat, 20 Aug 2022 12:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2022-08-20:/blog/traefik-ssl-grading.html</guid><category>containers</category><category>traefik</category><category>ssl</category><category>https</category><category>grading</category><category>testssl</category><category>qualys</category><category>ssllabs</category><category>labs</category><category>a</category><category>b</category></item><item><title>Traefik nomad route53 setup</title><link>https://visibilityspots.github.io/blog/traefik-nomad-route53.html</link><description>&lt;p&gt;I have this &lt;a href="https://nomadproject.io"&gt;nomad&lt;/a&gt; cluster running on some spare devices for a while now. Serving my &lt;a href="https://visibilityspots.github.io/blog/planespotting.html"&gt;plane spotting&lt;/a&gt; setup, &lt;a href="https://visibilityspots.github.io/blog/dockerized-cloudflared-pi-hole.html"&gt;dns&lt;/a&gt; setup, mqtt bridge and some other services I experiment with throughout the years. Until today I've always relied on the ip addresses to point my browser and other services towards the different services. For my DNS setup I even had to pin the jobs towards specific hardware using &lt;a href="https://www.nomadproject.io/docs/configuration/client#custom-metadata-network-speed-and-node-class"&gt;meta&lt;/a&gt; data.&lt;/p&gt;
&lt;p&gt;But I've always wanted to implement a proxy in between so I could rely on DNS names instead. This would also increase the flexibility of my DNS setup since for …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Sun, 27 Dec 2020 21:00:00 +0100</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2020-12-27:/blog/traefik-nomad-route53.html</guid><category>containers</category><category>traefik</category><category>proxy</category><category>nomad</category><category>containers</category><category>route53</category></item><item><title>Nomad ARM cluster</title><link>https://visibilityspots.github.io/blog/nomad-arm-cluster.html</link><description>&lt;p&gt;as I already mentioned a few times before I have some hands on experiences with &lt;a href="https://nomadproject.io"&gt;nomad&lt;/a&gt;. A couple of weeks ago I wrote about a &lt;a href="../nomad-local-development.html"&gt;local development&lt;/a&gt; setup based on nomad.&lt;/p&gt;
&lt;p&gt;Since quite some time I do have a thinclient which is running some docker containers through a docker-compose file I use for personal stuff at home. But this thinclient is suffering from all the containers I'm trying to spin up. While over thinking this issue I did realize I have quite some raspberry pi's laying around and figured I could maybe set up a cluster for those containers.&lt;/p&gt;
&lt;p&gt;Since …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Wed, 28 Aug 2019 21:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2019-08-28:/blog/nomad-arm-cluster.html</guid><category>containers</category><category>nomad</category><category>arm</category><category>cluster</category><category>consul</category><category>raspberry</category><category>pi</category><category>thin</category><category>client</category><category>thinclient</category><category>docker</category><category>home</category><category>lab</category><category>homelab</category></item><item><title>Nomad local development</title><link>https://visibilityspots.github.io/blog/nomad-local-development.html</link><description>&lt;p&gt;Been using &lt;a href="https://www.nomadproject.io/"&gt;nomad&lt;/a&gt; for a few years now at the customer I got the question couple of weeks ago from some &lt;a href="https://inuits.eu/team/"&gt;colleagues&lt;/a&gt; if nomad could be used to replace docker-compose.&lt;/p&gt;
&lt;p&gt;The reason behind the question is mainly to not vendor lock yourself for local development with the whole docker eco system.&lt;/p&gt;
&lt;p&gt;Since I like a certain level of challenge and I do believe it's a valid use case I gave it a try.&lt;/p&gt;
&lt;p&gt;This resulted in a &lt;a href="https://github.com/visibilityspots/nomad-local-development"&gt;nomad-local-development&lt;/a&gt; repository. The 2 major hurdles to take where to use one file like docker-compose.yml and dns resolving between the containers.&lt;/p&gt;
&lt;p&gt;The …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Tue, 14 May 2019 19:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2019-05-14:/blog/nomad-local-development.html</guid><category>containers</category><category>nomad</category><category>local</category><category>development</category><category>alternative</category><category>docker</category><category>docker-compose</category></item><item><title>Prometheus export/import</title><link>https://visibilityspots.github.io/blog/prometheus-export-import.html</link><description>&lt;p&gt;bumping into the case where once deployed a full stack application we don't have any direct connection due to no uplink for security reasons.&lt;/p&gt;
&lt;p&gt;So we (you too &lt;a href="https://twitter.com/TomVanHumbeeck"&gt;@Tom&lt;/a&gt;) looked into a way to export the prometheus data into a tar.gz which could be transferred and imported into an instance on our local machine.&lt;/p&gt;
&lt;p&gt;After the initial blog post where we created a tar.gz file from the prometheus storage.tsdb.path on the filesystem &lt;a href="https://twitter.com/roidelapluie"&gt;@roidelapluie&lt;/a&gt; pointed me out about the &lt;a href="https://prometheus.io/docs/prometheus/latest/querying/api/#snapshot"&gt;snapshot&lt;/a&gt; feature.&lt;/p&gt;
&lt;p&gt;So we did a bit of research and came up with this new procedure.&lt;/p&gt;
&lt;p&gt;First of …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Wed, 06 Jun 2018 22:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2018-06-06:/blog/prometheus-export-import.html</guid><category>containers</category><category>prometheus</category><category>export</category><category>data</category><category>import</category><category>metrics</category><category>lock</category><category>waf</category><category>remote</category></item><item><title>Prometheus consul service discovery</title><link>https://visibilityspots.github.io/blog/prometheus-consul.html</link><description>&lt;p&gt;as published a few months ago I worked out a &lt;a href="http://localhost:8000/jenkins-docker-pipeline.html"&gt;dockerized a jenkins farm&lt;/a&gt; where both master as slaves are docker containers working together with services like nexus and such. Next to that setup I've dockerized my home setup where services like pi-hole, home-assistant and others are running as docker containers on a thin client I promoted to my home lab.&lt;/p&gt;
&lt;p&gt;To have an overview about all those containers and the resources they are consuming I pulled in the git repo of &lt;a href="https://github.com/vegasbrianc/prometheus"&gt;Brian Christner&lt;/a&gt; which spins up a whole &lt;a href="https://prometheus.io"&gt;prometheus&lt;/a&gt; stack with some exporters and a grafana instance to visualize …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Mon, 04 Jun 2018 22:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2018-06-04:/blog/prometheus-consul.html</guid><category>containers</category><category>prometheus</category><category>consul</category><category>service</category><category>discovery</category><category>dynamic</category><category>configuration</category></item><item><title>dockerized DNS over HTTPS using pi-hole through cloudflared proxy-dns</title><link>https://visibilityspots.github.io/blog/dockerized-cloudflared-pi-hole.html</link><description>&lt;p&gt;a few months ago I configured a thin client as my home server to replace the previous &lt;a href="../raspberry-pi.html"&gt;raspberry pi&lt;/a&gt; setup.&lt;/p&gt;
&lt;p&gt;During that migration I moved over all native services within docker containers. One of those services being a &lt;a href="https://pi-hole.net"&gt;pi-hole&lt;/a&gt; setup to block ad serving domains on dns level and to have a dns cache within our LAN to gain a bit of speed.&lt;/p&gt;
&lt;p&gt;It has been running ever since without any issue and worked pretty well.&lt;/p&gt;
&lt;p&gt;When cloudflare &lt;a href="https://blog.cloudflare.com/announcing-1111/"&gt;announced&lt;/a&gt; their fast and privacy based DNS resolver I got a bit intrigued by their DNS over HTTPS feature. Especially since our …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Sat, 21 Apr 2018 21:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2018-04-21:/blog/dockerized-cloudflared-pi-hole.html</guid><category>containers</category><category>docker</category><category>compose</category><category>docker-compose</category><category>pi-hole</category><category>pihole</category><category>cloudflared</category><category>proxy-dns</category><category>DoH</category><category>dns</category><category>https</category><category>over</category></item><item><title>Test ansible playbooks with docker</title><link>https://visibilityspots.github.io/blog/test-ansible-playbooks.html</link><description>&lt;p&gt;recently I started working at a new project where the infra is maintained by ansible. When been asked to write some functionality in a playbook I missed my &lt;a href="https://github.com/visibilityspots/vagrant-puppet"&gt;vagrant puppet&lt;/a&gt; setup where I could easily test my puppet code on my local machine.&lt;/p&gt;
&lt;p&gt;Due to my previous project I felt like maybe I could use docker for this purpose on the ansible part. So I looked a bit around and stumbled on the &lt;a href="https://github.com/William-Yeh/docker-ansible"&gt;docker-ansible github repository&lt;/a&gt; of William Yeh. He already did a great job by creating a docker container with ansible preinstalled for a lot of linux distributions.&lt;/p&gt;
&lt;p&gt;I …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Thu, 09 Nov 2017 21:00:00 +0100</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2017-11-09:/blog/test-ansible-playbooks.html</guid><category>containers</category><category>ansible</category><category>docker</category><category>playbook</category><category>test</category></item><item><title>Jenkins docker-pipeline</title><link>https://visibilityspots.github.io/blog/jenkins-docker-pipeline.html</link><description>&lt;p&gt;in a previous blog post I talked about setting up a &lt;a href="https://visibilityspots.github.io/blog/dockerized-jenkins.html"&gt;dockerized jenkins master/slave setup&lt;/a&gt; and setting up a &lt;a href="https://visibilityspots.github.io/blog/nexus-oss-repository-manager.html"&gt;private docker registry using nexus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The next thing on the roadmap was to use this jenkins setup to actually build new docker images for specific software. Before going to the different teams and talking how they now build their software and how this could be done using this new containerized setup I setted up a new jenkins job.&lt;/p&gt;
&lt;p&gt;This jenkins job will build a generic jenkins slave docker container which will be used by the jenkins master to build some …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Mon, 30 Oct 2017 19:00:00 +0100</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2017-10-30:/blog/jenkins-docker-pipeline.html</guid><category>containers</category><category>jenkins</category><category>docker</category><category>pipeline</category><category>plugin</category><category>jenkinsfile</category><category>centos</category><category>master</category><category>slave</category><category>7</category></item><item><title>Nexus OSS repository manager</title><link>https://visibilityspots.github.io/blog/nexus-oss-repository-manager.html</link><description>&lt;p&gt;looking for a global repository store which could store maven projects, yum repositories, docker repositories, we bumped into &lt;a href="https://help.sonatype.com/display/NXRM3/Repository+Manager+3"&gt;Nexus repository manager&lt;/a&gt;. We used the official docker image to see how it can be implemented in the dockerized CI environment.&lt;/p&gt;
&lt;h2&gt;docker repository&lt;/h2&gt;
&lt;p&gt;as a first the docker repository feature could be enabled so we can start building and storing docker images for the different jenkins build slaves and the jenkins master so our work is reproducible and stored in a safe central place.&lt;/p&gt;
&lt;p&gt;We configured 3 repositories in nexus for our docker images seen as a recommended approach in the &lt;a href="https://help.sonatype.com/display/NXRM3/Private+Registry+for+Docker#PrivateRegistryforDocker-HostedRepositoryforDocker(PrivateRegistryforDocker)"&gt;nexus …&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Mon, 11 Sep 2017 19:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2017-09-11:/blog/nexus-oss-repository-manager.html</guid><category>containers</category><category>nexus</category><category>repository</category><category>manager</category><category>nginx</category><category>proxy</category><category>SSL</category><category>https</category><category>docker</category><category>private</category><category>group</category></item><item><title>Dockerized jenkins master/slave setup</title><link>https://visibilityspots.github.io/blog/dockerized-jenkins.html</link><description>&lt;p&gt;started at a new customer we were looking for a more flexible way of having jenkins spinning up slaves on the fly. This in a way a slave is only started and consuming resources when a specific job is running. That way those resources could be used more efficient.&lt;/p&gt;
&lt;p&gt;Also the fact that developers could take control over their build servers by managing the Dockerfiles themselves is a great advantage too. But that's for a later phase. Let's start at the beginning.&lt;/p&gt;
&lt;p&gt;For the docker host a CentOS 7 server has been provisioned and prepared to run the docker daemon …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Wed, 06 Sep 2017 22:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2017-09-06:/blog/dockerized-jenkins.html</guid><category>containers</category><category>docker</category><category>jenkins</category><category>master</category><category>slave</category><category>centos</category><category>7</category></item><item><title>Docker openstackclient-kilo container</title><link>https://visibilityspots.github.io/blog/docker-openstackclient-kilo.html</link><description>&lt;p&gt;A couple of years ago I deployed an openstack cluster based on &lt;a href="https://www.rdoproject.org/"&gt;RDO&lt;/a&gt;. Back in the days we implemented the &lt;a href="https://www.openstack.org/software/kilo/"&gt;kilo&lt;/a&gt; release. Until today we didn't updated yet due to various reasons being no need for the new features, no resources, no time no.. Upgrading would be a better option but we'll have to live with it and since it's running rather well so far we are quite happy with it.&lt;/p&gt;
&lt;p&gt;To manage that cloud I use the clients I installed on my local machine, from nova to cinder they all have different packages available for many different platforms. Only …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jan</dc:creator><pubDate>Mon, 29 May 2017 21:00:00 +0200</pubDate><guid isPermaLink="false">tag:visibilityspots.github.io,2017-05-29:/blog/docker-openstackclient-kilo.html</guid><category>containers</category><category>docker</category><category>hub</category><category>automated</category><category>github</category><category>container</category><category>openstack</category><category>openstackclient</category><category>tools</category><category>source</category><category>at</category><category>run</category><category>rdo</category><category>kilo</category></item></channel></rss>