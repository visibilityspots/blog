<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>visibilityspots - puppet</title><link href="https://visibilityspots.github.io/blog/" rel="alternate"></link><link href="https://visibilityspots.github.io/blog/feeds/puppet.atom.xml" rel="self"></link><id>https://visibilityspots.github.io/blog/</id><updated>2015-10-10T23:00:00+02:00</updated><subtitle>Linux &amp; Open-Source enthusiast | Scouting | Longboarding</subtitle><entry><title>Vagrant puppet setup</title><link href="https://visibilityspots.github.io/blog/vagrant-puppet-setup.html" rel="alternate"></link><published>2015-10-10T23:00:00+02:00</published><updated>2015-10-10T00:00:00+02:00</updated><author><name>Jan</name></author><id>tag:visibilityspots.github.io,2015-10-10:/blog/vagrant-puppet-setup.html</id><summary type="html">&lt;p&gt;We at &lt;a href="https://inuits.eu"&gt;Inuits&lt;/a&gt; are using vagrant for a lot of use cases, neither you are a developer or a sysadmin you for sure will walk into it. Me, myself I do use it merely to automate the many different use cases asked by various projects. It took some time to get myself organized with this pretty nifty piece of software.&lt;/p&gt;
&lt;p&gt;In the beginning I used it with the default virtualization provider &lt;a href="https://virtualbox.org"&gt;virtualbox&lt;/a&gt; later on I switched to &lt;a href="https://visibilityspots.github.io/blog/vagrant-setup.html"&gt;lxc&lt;/a&gt; containers instead. By using those containers I already gained on performance. Spinning up and down new containers to test if an application â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;We at &lt;a href="https://inuits.eu"&gt;Inuits&lt;/a&gt; are using vagrant for a lot of use cases, neither you are a developer or a sysadmin you for sure will walk into it. Me, myself I do use it merely to automate the many different use cases asked by various projects. It took some time to get myself organized with this pretty nifty piece of software.&lt;/p&gt;
&lt;p&gt;In the beginning I used it with the default virtualization provider &lt;a href="https://virtualbox.org"&gt;virtualbox&lt;/a&gt; later on I switched to &lt;a href="https://visibilityspots.github.io/blog/vagrant-setup.html"&gt;lxc&lt;/a&gt; containers instead. By using those containers I already gained on performance. Spinning up and down new containers to test if an application is deployed fully automatically got 2 times as fast as when using vm's.&lt;/p&gt;
&lt;p&gt;But what I struggled with the most where the many different projects. Each time a new piece of software needed to be automated I copied over the puppet base I used the previous time. Which lead to outdated setups for older projects, many duplicate code over and over again. When updating base modules for both the puppet agent as the puppetmaster previous projects got forgotten..&lt;/p&gt;
&lt;p&gt;So I tried to figure out a way I could keep them all up to date with the same code base. We are using &lt;a href="https://git.org"&gt;git&lt;/a&gt; for almost all our projects as our versioning platform. So I figured out I could use the features of git to achieve the goals I've setted for my setup. One code base I could update without interrupting the functionality of the different proof of concepts but with the availability of upgrading those in a easy way.&lt;/p&gt;
&lt;p&gt;So I started my &lt;a href="https://github.com/visibilityspots/vagrant-puppet.git"&gt;vagrant-puppet&lt;/a&gt; project on github. In the master branch a base setup has been configured with a puppetmaster container and a client container. Both are running the latest stable release of puppet 3.x. The puppetmaster is setted up using puppetdb puppetserver or apache/passenger it can be used both with the &lt;a href="https://atlas.hashicorp.com/visibilityspots/boxes/centos-6.x-puppet-3.x"&gt;centos6&lt;/a&gt; or &lt;a href="https://atlas.hashicorp.com/visibilityspots/boxes/centos-7.x-puppet-3.x"&gt;centos7&lt;/a&gt; containers I crafted using the &lt;a href="https://github.com/visibilityspots/vagrant-lxc-base-boxes"&gt;lxc-base-boxes&lt;/a&gt; repository.&lt;/p&gt;
&lt;h1&gt;puppet&lt;/h1&gt;
&lt;p&gt;to automate the different pieces of software we do use puppet, if upstream puppet-modules are available I pull those in through git submodules if not I write my own.&lt;/p&gt;
&lt;p&gt;By using the vagrant rsync functionality I could write my module and hiera data in my own preferred environment since they are synced to the running puppetmaster through rsync.&lt;/p&gt;
&lt;p&gt;This syncronisation can be achieved in two ways. Manually:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ vagrant rsync
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Or setting up a daemon:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ vagrant rsync-auto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That way changes you made through your local environment are synced into the puppetmaster container.&lt;/p&gt;
&lt;h1&gt;upgrade&lt;/h1&gt;
&lt;h2&gt;submodules - upstream puppet modules&lt;/h2&gt;
&lt;p&gt;Every once in a while when a new version of puppet has been released I try to keep my container vagrant boxes up to date. Once those are upgraded I get myself into to master branch and update all the used git submodules with this one liner:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ git submodule foreach git pull origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This way the latest released version of the different used upstream puppet module repositories are fetched into the master branch.&lt;/p&gt;
&lt;p&gt;And try to provision my puppetmaster from scratch, depending on the changes been done in the different puppet modules I need to adopt my &lt;a href="https://github.com/visibilityspots/vagrant-puppet/tree/master/hieradata"&gt;hieradata&lt;/a&gt;. By looking into the hieradata you could see I'm using the puppet roles and profiles principle. With one simple trick in the &lt;a href="https://github.com/visibilityspots/vagrant-puppet/blob/master/puppet/environments/production/manifests/site.pp"&gt;site.pp&lt;/a&gt; pointed out by &lt;a href="https://twitter.com/PeetersSimon"&gt;one&lt;/a&gt; of my colleagues I created a role based hierarchy in my hieradata. Based on the role fact given in the node hiera data the parameters needed to get the functionality of the role configured are fetched from the role's hieradata.&lt;/p&gt;
&lt;p&gt;By doing so the hiera data of a particular role can be easily reused without having to keep them in sync on every node who needs the same role.&lt;/p&gt;
&lt;p&gt;I still need to figure out a way I can achieve the same behavior based on profiles data.&lt;/p&gt;
&lt;p&gt;This way my master branch keeps staying in sync with the latest releases on the different puppet tools.&lt;/p&gt;
&lt;h2&gt;different projects, different branches&lt;/h2&gt;
&lt;p&gt;But I wanted to go a step further, by getting all my different projects in one place to ease the maintainability of them. So I started tinkering about it. The first idea consisted of having them all in one environment like it would be the case in the real world.&lt;/p&gt;
&lt;p&gt;But this has a big disadvantage. It would be a mess in the future when a lot of such proof of concepts are combined in one puppet environment. Also an unneeded level of complexity would been added if you want to show of one particular project to a customer or an interested fellow through the interweb.&lt;/p&gt;
&lt;p&gt;It looked for me this was the perfect use case for branches. Every branch created from the master branch already got a working puppetmaster client setup and can easily be upgraded by merging the master branch into it when upgrades are released.&lt;/p&gt;
&lt;p&gt;By checking out a branch a subset of different submodules can be loaded after the previous ones are cleaned up:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ git checkout feature_branch
$ git clean -d -f -f
$ git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This way the specific puppet modules for a specific projects are loaded with a known working version. When merging from the upgraded master branch the submodules are updated to.&lt;/p&gt;
&lt;h2&gt;merging master branch&lt;/h2&gt;
&lt;p&gt;when the master branch has been upgraded I now can easily merge those updates into the different feature branches:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ git checkout feature_branch
$ git clean -d -f -f
$ git merge origin/master
$ git submodule update --init --recursive
$ vagrant up puppetmaster --provider&lt;span class="o"&gt;=&lt;/span&gt;lxc
$ vagrant up node --provider&lt;span class="o"&gt;=&lt;/span&gt;lxc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By configuring this setup I know have a flexible environment to test deploy and write new puppet code when some piece of software needs to be automated on my local machine with a puppetmaster almost simultaneous to a production one.&lt;/p&gt;</content><category term="puppet"></category><category term="vagrant"></category><category term="rsync"></category><category term="puppetmaster"></category><category term="puppetserver"></category><category term="development"></category><category term="setup"></category></entry><entry><title>Ansible orchestration</title><link href="https://visibilityspots.github.io/blog/ansible-orchestration.html" rel="alternate"></link><published>2014-10-21T23:00:00+02:00</published><updated>2014-11-21T00:00:00+01:00</updated><author><name>Jan</name></author><id>tag:visibilityspots.github.io,2014-10-21:/blog/ansible-orchestration.html</id><summary type="html">&lt;p&gt;I do use &lt;a href="https://docs.puppetlabs.com/#puppetpuppet"&gt;puppet&lt;/a&gt; as our main configuration management tool. Together with &lt;a href="https://docs.puppetlabs.com/#puppetdbpuppetdblatest"&gt;puppetdb&lt;/a&gt; all our services are automatically configured from bottom to top.&lt;/p&gt;
&lt;p&gt;And it rocks, getting automated as much as possible it is like easy as hell to get a server up and running. The only feature it lacked in my opinion is orchestration. I do know about &lt;a href="http://puppetlabs.com/mcollective"&gt;collective&lt;/a&gt; which is made for this purpose.&lt;/p&gt;
&lt;p&gt;Only it's yet again using an agent which fails from time to time and eating resources which can be avoided. It's the same reason I don't use the puppet agent daemon but trigger puppet â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I do use &lt;a href="https://docs.puppetlabs.com/#puppetpuppet"&gt;puppet&lt;/a&gt; as our main configuration management tool. Together with &lt;a href="https://docs.puppetlabs.com/#puppetdbpuppetdblatest"&gt;puppetdb&lt;/a&gt; all our services are automatically configured from bottom to top.&lt;/p&gt;
&lt;p&gt;And it rocks, getting automated as much as possible it is like easy as hell to get a server up and running. The only feature it lacked in my opinion is orchestration. I do know about &lt;a href="http://puppetlabs.com/mcollective"&gt;collective&lt;/a&gt; which is made for this purpose.&lt;/p&gt;
&lt;p&gt;Only it's yet again using an agent which fails from time to time and eating resources which can be avoided. It's the same reason I don't use the puppet agent daemon but trigger puppet every time.&lt;/p&gt;
&lt;h1&gt;orchestration&lt;/h1&gt;
&lt;p&gt;We have puppet running every 15 minutes through cron, main reason is to pick up and install the latest software which has been deployed. The other reason puppet runs after installation is to make sure the configuration files were not manually manipulated and making sure necessary services are still running.&lt;/p&gt;
&lt;p&gt;Using puppet for making sure services are running and configuration files are not being changed an hourly puppet run would be enough. Thing is for those deployment flows it's merely like polling. And I strongly hate polling jobs, 99% of the time they don't have to do anything. So to me it's just useless, a waste of time, energy and resources.&lt;/p&gt;
&lt;p&gt;It meant that developers had to wait in worst case scenario 15 minutes before their changes where deployed on the development environment. Their changes were already processed by jenkins, packages are been made, deployed on the repository only waiting for puppet to install the latest version of them. Nobody complained, but in my opinion it was waaay too long!&lt;/p&gt;
&lt;p&gt;By running puppet immediately after the package is been deployed to the repository the right order of installing, configuring and restarting the necessary services can be executed. This will gain time for deployments next to some hourly puppet cron jobs which are running just to be sure no configuration has changed manually and the services are still running.&lt;/p&gt;
&lt;h1&gt;ansible&lt;/h1&gt;
&lt;p&gt;So I started looking at some solution where I could trigger a puppet run on the hosts configured the software through puppet in the right environment as soon as the package is deployed to the proper repository through jenkins.&lt;/p&gt;
&lt;p&gt;At first I looked into the ssh jenkins plugin, it works but has one big disadvantage. You have to configure ssh credentials for every host in jenkins and therefore you can't use abstract jenkins flows cause you need to configure in each job the specific ssh credentials.&lt;/p&gt;
&lt;p&gt;I looked further and came across &lt;a href="http://www.ansible.com"&gt;ansible&lt;/a&gt;. You don't have to configure a client on every host, neither you have to configure a per server based jenkins configuration to get it working. It was a blessing, the only things you have to do is creating a user, his public ssh key and grant him sudo rights on every server. This can easily be done through puppet!&lt;/p&gt;
&lt;h1&gt;static inventory&lt;/h1&gt;
&lt;p&gt;At first I crawled through our &lt;a href="http://www.theforeman.org"&gt;foreman&lt;/a&gt; instance and copied over the nodes into 2 groups, development and production, the puppet environments. I also configured some stuff like ssh port and user. I refused to configure the root pw in some plain text file on the jenkins node. That's not safe at all in my opinion, instead I created an ssh key pair and distributed the public key on all servers.&lt;/p&gt;
&lt;p&gt;In my fight to automate as much as possible this wasn't the most efficient way of using the inventory. Every time you removed or added a node you had to reconfigure it yourself manually in the first place. Beside the manual intervention you also have to take note how you are gonna perform that manual action? Manipulating configuration data on the production machine is not done, using a git repository which you package or adding them to puppet, which both sounds wrong. The first because it's overkill the second because it's rather data over configuration.&lt;/p&gt;
&lt;h1&gt;dynamic inventory&lt;/h1&gt;
&lt;p&gt;In my quest I got pointed to a &lt;a href="https://github.com/EchoTeam/ansible-plugins"&gt;python&lt;/a&gt; script by a colleague. Unfortunately the script isn't straight forward and the 'maintainers' hides themselves behind their footer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nv"&gt;Notice&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;puppetdb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;inventory&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;plugin&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;is&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;not&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;quite&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;generic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;moment&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Use&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;more&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;an&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;example&lt;/span&gt;.&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once I found out about the &lt;a href="http://docs.ansible.com/developing_inventory.html"&gt;inventory&lt;/a&gt; part of ansible I knew what I was looking for and saw the light by an &lt;a href="https://blog.codecentric.de/en/2014/09/use-ansible-remote-executor-puppet-environment/"&gt;article&lt;/a&gt; on cedecentric.de. Their was only one issue, my jenkins host which needs ansible to run isn't my puppetmaster and therefore can't list the signed certificates as used in his script.&lt;/p&gt;
&lt;p&gt;But I am using &lt;a href="https://docs.puppetlabs.com/puppetdb/latest/index.html"&gt;puppetdb&lt;/a&gt;, and puppetdb has a great &lt;a href="https://docs.puppetlabs.com/puppetdb/2.2/api/index.html"&gt;API&lt;/a&gt;. So I could take advantage of it by using this great API, melting it down into an inventory script and using the json generated output through ansible.&lt;/p&gt;
&lt;p&gt;So I started modifying the code example I found on codecentrec and got it working by writing a &lt;a href="https://github.com/visibilityspots/ansible-puppet-inventory"&gt;puppetdb.sh&lt;/a&gt; dynamic inventory script. Together with the &lt;a href="https://github.com/visibilityspots/puppet-ansible"&gt;puppet-ansible&lt;/a&gt; module it even got automated too!&lt;/p&gt;
&lt;h1&gt;adding it to ansible-core&lt;/h1&gt;
&lt;p&gt;I went to the &lt;a href="http://events.linuxfoundation.org/events/cloudstack-collaboration-conference-europe"&gt;Cloudstack Collaboration Conference&lt;/a&gt; in Budapest where I followed a &lt;a href="https://github.com/runseb/runseb.github.io/blob/master/ONEPAGE.md"&gt;tutorial&lt;/a&gt; by &lt;a href="http://sebgoa.blogspot.hu/"&gt;Sebastien Goasguen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It turned out he wrote an ansible apache-libcloud inventory script and tried to pushing it in to the ansible core. This inspired my to rewrite my bash script in python so it could be added to ansible-core too.&lt;/p&gt;
&lt;p&gt;After fooling around a bit in python I used the &lt;a href="https://github.com/puppet-community/pypuppetdb"&gt;pypuppetdb&lt;/a&gt; library so I don't have to make all the API calls natively myself through urllib request. And it turned out quite fine and I got it up and running in my setup. So those days I'm waiting on feedback from the ansible community to my &lt;a href="https://github.com/ansible/ansible/pull/9593"&gt;pull request&lt;/a&gt; so everyone can benefit of the joy between ansible and puppet.&lt;/p&gt;
&lt;h1&gt;still need some attention&lt;/h1&gt;
&lt;p&gt;I need some time to look which processes it takes to run a command through ansible so I could specify more clear the sudoers file.&lt;/p&gt;
&lt;p&gt;Also the environments should be more abstract in my puppetdb.sh script without having to manually adapt the necessary puppetdb query files.&lt;/p&gt;
&lt;h1&gt;drinking cocktails&lt;/h1&gt;
&lt;p&gt;From now on it only takes less than 5 minutes to push your code, get it through jenkins tests into a package on an apt or yum repository got pulled into a repository and deploy it through puppet using ansible on the development servers. All without any manual action, without any cron job all automated, glued the pieces together.&lt;/p&gt;
&lt;p&gt;I'll dig deeper into the whole deployment process later on, when I found time between drinking cocktails, looking at my daughter and living the dream.&lt;/p&gt;</content><category term="puppet"></category><category term="ansible"></category><category term="orchestration"></category><category term="tool"></category><category term="puppet"></category><category term="dynamic"></category><category term="inventory"></category><category term="puppetdb"></category></entry><entry><title>Upgrade to puppet 3.3.0</title><link href="https://visibilityspots.github.io/blog/puppet-3-upgrade.html" rel="alternate"></link><published>2013-09-20T19:00:00+02:00</published><updated>2013-09-20T00:00:00+02:00</updated><author><name>Jan</name></author><id>tag:visibilityspots.github.io,2013-09-20:/blog/puppet-3-upgrade.html</id><summary type="html">&lt;p&gt;I finally got to the point where I upgraded a whole puppet infrastructure from puppet 2.6.x to the last stable version of puppet, &lt;a class="reference external" href="http://docs.puppetlabs.com/puppet/3/reference/release_notes.html"&gt;3.3.0&lt;/a&gt;. And after finding out the way to go it was surprisingly easy and no big issues came across.&lt;/p&gt;
&lt;p&gt;One of the main reasons to upgrade was to start using the latest version of foreman, were we used 0.4, so we can start provisioning our own development vm's with some fancy cloud solution like for example &lt;a class="reference external" href="http://cloudstack.apache.org/"&gt;cloudstack&lt;/a&gt; using our production puppet tree.&lt;/p&gt;
&lt;p&gt;Before the upgrade we had the puppet-client &amp;amp; server (2.6 â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I finally got to the point where I upgraded a whole puppet infrastructure from puppet 2.6.x to the last stable version of puppet, &lt;a class="reference external" href="http://docs.puppetlabs.com/puppet/3/reference/release_notes.html"&gt;3.3.0&lt;/a&gt;. And after finding out the way to go it was surprisingly easy and no big issues came across.&lt;/p&gt;
&lt;p&gt;One of the main reasons to upgrade was to start using the latest version of foreman, were we used 0.4, so we can start provisioning our own development vm's with some fancy cloud solution like for example &lt;a class="reference external" href="http://cloudstack.apache.org/"&gt;cloudstack&lt;/a&gt; using our production puppet tree.&lt;/p&gt;
&lt;p&gt;Before the upgrade we had the puppet-client &amp;amp; server (2.6.18), puppetdb (1.4), (ruby 1.8.7) and foreman (0.4.2) running on a CentOS 6.3 machine.&lt;/p&gt;
&lt;p&gt;After upgrading we are running puppet-client &amp;amp; server (3.3.0) puppetdb (1.4), ruby (1.8.7) and foreman (1.2) all managed by puppet itself. (feels quite satisfying ;) )&lt;/p&gt;
&lt;p&gt;The very fist time I started upgrading the puppet master, but instead of upgrading the puppet-server package from the yum puppetlabs repository I upgraded only the agent.&lt;/p&gt;
&lt;p&gt;After I figured that out I could kill myself but ran out of time so needed to stop the process.&lt;/p&gt;
&lt;p&gt;The second time I started totally in the wrong direction. I started with foreman, read about needing ruby 1.9.3. So I started looking for a CentOS 6.3 ruby 1.9.3 package.&lt;/p&gt;
&lt;p&gt;Didn't find any started compiling it from source, but that came out on a total mess so I reverted my upgrade and postponed it for some days.&lt;/p&gt;
&lt;p&gt;The final 3Th time I started in the right order. This order I will describe here:&lt;/p&gt;
&lt;p&gt;(Before all those steps, make sure to disable puppet on your clients to have more control during the process)&lt;/p&gt;
&lt;div class="section" id="configuring-the-puppetlabs-repository"&gt;
&lt;h2&gt;Configuring the puppetlabs repository&lt;/h2&gt;
&lt;p&gt;I like to install software from packages, so I started by configuring the &lt;a class="reference external" href="http://docs.puppetlabs.com/guides/puppetlabs_package_repositories.html"&gt;puppetlabs&lt;/a&gt; repository. I use a puppet-repo module for configuring repo's on our machines but you can quite easy install it from the command line.&lt;/p&gt;
&lt;p&gt;This command is executed on a Cent0S 6.3 x86_64 machine:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# rpm -ivh http://yum.puppetlabs.com/el/6.0/products/x86_64/puppetlabs-release-6-7.noarch.rpm
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="upgrading-the-puppetmaster"&gt;
&lt;h2&gt;Upgrading the puppetmaster&lt;/h2&gt;
&lt;p&gt;So after shamelessly updated only the puppet package the first time, this time I did upgrade the puppet-server package without any issue. Be sure to read the &lt;a class="reference external" href="http://docs.puppetlabs.com/guides/upgrading.html"&gt;docs&lt;/a&gt; first about upgrading!&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum update puppet-server
&lt;/pre&gt;
&lt;p&gt;Once the puppetmaster is updated we can try our first puppet runs against the upgraded version.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="start-a-native-puppet-master-process-for-testing"&gt;
&lt;h2&gt;Start a native puppet master process for testing&lt;/h2&gt;
&lt;p&gt;Before I get further in our upgrade process on passenger and stuff I wanted to know if the client is still able to do a puppet run without the passenger setup.&lt;/p&gt;
&lt;p&gt;So I had to start the puppetmaster as a daemon, did a local puppet noop run on the master itself and stopped the puppetmaster daemon after I checked the run.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# puppet resource service puppetmaster ensure=running enable=true
# puppet agent --test --noop
# puppet resource service puppetmaster ensure=stopped enable=true
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="upgrade-the-passenger-setup"&gt;
&lt;h2&gt;Upgrade the passenger setup&lt;/h2&gt;
&lt;p&gt;We are using a passenger setup to have our puppet master in a scalable setup. Therefore we also needed to upgrade passenger on our puppetmaster and adopt the puppetmaster vhost to the upgraded environment.&lt;/p&gt;
&lt;p&gt;To accomplish this I simply followed the &lt;a class="reference external" href="http://docs.puppetlabs.com/guides/passenger.html"&gt;passenger&lt;/a&gt; documentation of puppetlabs which was quite easy to follow.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="client"&gt;
&lt;h2&gt;Client&lt;/h2&gt;
&lt;p&gt;Once the puppetmaster was upgraded I tested a puppet run with a still not updated client against the upgraded puppetmaster. It did the job except from sending reports. Since I planned to upgrade the clients too I did not invest time into this issue.&lt;/p&gt;
&lt;p&gt;There fore I just upgraded the client itself where the puppetlabs repository already was enabled:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# yum update puppet
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="issues"&gt;
&lt;h2&gt;Issues&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;403: authentication error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By running my first 3.3.0 client vs the 3.3.0 master I got an authentication error 403 forbidden request. Did some research on the net, and found about an issue in the puppetmaster's &lt;a class="reference external" href="http://projects.puppetlabs.com/issues/16765"&gt;auth.conf&lt;/a&gt; file. Once I added this to the file:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# allow nodes to retrieve their own node definition
path ~ ^/node/([^/]+)$
method find
allow $1
&lt;/pre&gt;
&lt;p&gt;The run did what I had to do configuring the server by using puppet!&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;undefined method 'symbolize'&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On some clients I got this error message when trying to run puppet. On &lt;a class="reference external" href="http://somethingsinistral.net/blog/the-angry-guide-to-puppet-3/"&gt;somethingsinistral.net&lt;/a&gt; I found out it had to see with multiple puppet versions on your machine. By looking into the installed gems (make sure to check also possible rvm environments) and cleaned the ancient ones out I got the puppet run up and running again.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;icinga &lt;a class="reference external" href="https://github.com/ripienaar/monitoring-scripts/issues/3"&gt;check_puppet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are using ripienaar's icinga check_puppet to monitor the puppet functionality. The became all red indicating puppet had too long not ran on the server. In the troubleshooting process I figured out the nagios user which is running the check over the NRPE protocol wasn't able to read the /var/lib/puppet/state/last_run_summary.yaml file. By checking permissions I found out the default settings of the /var/lib/puppet directory are 0750 when installing puppet.&lt;/p&gt;
&lt;p&gt;Once I've changed them to 755 all check's became green again!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="foreman"&gt;
&lt;h2&gt;Foreman&lt;/h2&gt;
&lt;p&gt;Once the puppet master was running fine again I also upgraded &lt;a class="reference external" href="http://theforeman.org/manuals/1.2/index.html#3.3InstallFromPackages"&gt;theforeman&lt;/a&gt; service running on the same machine as the puppetmaster. This went smoothly once I figured out the ruby and rake commands in the documentation must be replaced with ruby193-rake/ruby193-ruby when installed foreman from their repository.&lt;/p&gt;
&lt;p&gt;Also do not forget to install foreman-mysql / foreman-sqlite etc when using those extra features.&lt;/p&gt;
&lt;/div&gt;
</content><category term="puppet"></category><category term="puppet"></category><category term="upgrade"></category><category term="3.3.0"></category><category term="2.6.x"></category><category term="issues"></category><category term="foreman"></category><category term="passenger"></category><category term="puppetdb"></category></entry><entry><title>Puppet sslv3 alert certificate revoked</title><link href="https://visibilityspots.github.io/blog/puppet-revoked-certificate.rst.html" rel="alternate"></link><published>2012-10-08T11:22:00+02:00</published><updated>2015-10-08T00:00:00+02:00</updated><author><name>Jan</name></author><id>tag:visibilityspots.github.io,2012-10-08:/blog/puppet-revoked-certificate.rst.html</id><summary type="html">&lt;p&gt;I started the day with ssl issues using puppet. Last week I cleaned 2 hosts in our tree using the puppet command&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# puppet node clean [hostname]
&lt;/pre&gt;
&lt;p&gt;on the puppetmaster. I did this to clean out the stored configs for those nodes.&lt;/p&gt;
&lt;p&gt;But I didn't realized this also cleaned out the ssl certificates for those clients. So I started the new week with this uncomfortable issue:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;agent ~]# puppet agent --test err: Could not retrieve catalog from remote server: SSL_connect returned=1 errno=0 state=SSLv3 read server session ticket A: sslv3 alert certificate revoked warning: Not using cache on failed â€¦&lt;/pre&gt;</summary><content type="html">&lt;p&gt;I started the day with ssl issues using puppet. Last week I cleaned 2 hosts in our tree using the puppet command&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# puppet node clean [hostname]
&lt;/pre&gt;
&lt;p&gt;on the puppetmaster. I did this to clean out the stored configs for those nodes.&lt;/p&gt;
&lt;p&gt;But I didn't realized this also cleaned out the ssl certificates for those clients. So I started the new week with this uncomfortable issue:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;agent ~]# puppet agent --test err: Could not retrieve catalog from remote server: SSL_connect returned=1 errno=0 state=SSLv3 read server session ticket A: sslv3 alert certificate revoked warning: Not using cache on failed catalog err: Could not retrieve catalog; skipping run err: Could not send report: SSL_connect returned=1 errno=0 state=SSLv3 read server session ticket A: sslv3 alert certificate revoked
&lt;/pre&gt;
&lt;p&gt;After some digging on the internet I achieved to solve this issue.
Here under I described the steps to breath again:&lt;/p&gt;
&lt;p&gt;To be sure the certificates are completely removed on the puppetmaster I explicitly cleaned them again&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;master ~]#puppet cert -c hostname
&lt;/pre&gt;
&lt;p&gt;Now we are sure those certificates are cleaned up on the master we have to do this also on the agent&lt;/p&gt;
&lt;p&gt;Looking for the directory where those certificates are stored&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;agent ~]# puppet --genconfig | grep certdir
# The default value is '$certdir/$certname.pem'.
# The default value is '$certdir/ca.pem'. certdir = /var/lib/puppet/ssl/certs
&lt;/pre&gt;
&lt;p&gt;For older versions of puppet&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;agent ~]# puppet config print | grep certdir
&lt;/pre&gt;
&lt;p&gt;Removing the existing certificates on the client:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;agent ~]# rm /var/lib/puppet/ssl -rf
&lt;/pre&gt;
&lt;p&gt;Once the certificates are completely removed on the master and the client we have to regenerate them from the agent&amp;nbsp;using the puppet daemon&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;agent ~]#&amp;nbsp;puppet agent --test
&lt;/pre&gt;
&lt;p&gt;or by manually regenerating them&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;agent ~]# puppet certificate generate hostname.domain --ca-location remote true
&lt;/pre&gt;
&lt;p&gt;As soon as new certificates are generated and we got the true back from the agent we can sign the fresh certificate on the master&lt;/p&gt;
&lt;p&gt;List the certificates which are waiting to get signed and sign them&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[root&amp;#64;master ~]# puppet cert -l &amp;quot;hostname.domain&amp;quot; (XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX)
[root&amp;#64;master ~]# puppet cert sign hostname.domain
notice: Signed certificate request for hostname.domain
notice: Removing file Puppet::SSL::CertificateRequest hostname.domain at '/var/lib/puppetmaster/ssl/ca/requests/hostname.domain.pem'
&lt;/pre&gt;
&lt;p&gt;If everything went well you should be able to run puppet again on the client&lt;/p&gt;
&lt;pre class="literal-block"&gt;
puppet agent --test --noop
&lt;/pre&gt;
&lt;p&gt;and relax again!&lt;/p&gt;
&lt;p&gt;Digging the internet I crossed &lt;a class="reference external" href="http://honglus.blogspot.be/2012/01/force-puppet-agent-to-regenerate.html"&gt;honglus blog&lt;/a&gt; and an issue on &lt;a class="reference external" href="http://projects.puppetlabs.com/issues/11854"&gt;puppetlabs projects&lt;/a&gt;&amp;nbsp;which made my day.&lt;/p&gt;
</content><category term="puppet"></category><category term="certificate"></category><category term="continuous integration"></category><category term="Linux"></category><category term="puppet"></category><category term="revoke"></category><category term="sign"></category><category term="ssl"></category></entry><entry><title>Puppet module mumble-server</title><link href="https://visibilityspots.github.io/blog/puppet-mumble.html" rel="alternate"></link><published>2012-04-04T15:31:00+02:00</published><updated>2012-04-04T00:00:00+02:00</updated><author><name>Jan</name></author><id>tag:visibilityspots.github.io,2012-04-04:/blog/puppet-mumble.html</id><summary type="html">&lt;p&gt;&lt;a class="reference external" href="http://mumble.sourceforge.net/"&gt;Mumble&lt;/a&gt; is an open source, low-latency, high quality voice chat software primarily intended for use while gaming.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://puppetlabs.com/"&gt;Puppet&lt;/a&gt; is a tool designed to manage the configuration of Unix-like and Microsoft Windows systems decoratively.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://github.com/visibilityspots/puppet-mumble"&gt;puppet-mumble&lt;/a&gt; module installs a mumble server (version 1.2.3) automatically on a CentOS 6.x machine using the puppet software based on &lt;a class="reference external" href="http://mumble.sourceforge.net/Install_CentOS5"&gt;mumble-documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The module needs a repository which contains the &lt;a class="reference external" href="http://www.visibilityspots.com/repos/repoview/mumble-server.html"&gt;mumble-server&lt;/a&gt; package. I distribute this package on my own &lt;a class="reference external" href="http://www.visibilityspots.com/repos/repoview/"&gt;visibilityspots&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;Using puppet this will create the necessary mumble user and group and will configure the mumble-server using your desired settings, like username, password â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a class="reference external" href="http://mumble.sourceforge.net/"&gt;Mumble&lt;/a&gt; is an open source, low-latency, high quality voice chat software primarily intended for use while gaming.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://puppetlabs.com/"&gt;Puppet&lt;/a&gt; is a tool designed to manage the configuration of Unix-like and Microsoft Windows systems decoratively.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://github.com/visibilityspots/puppet-mumble"&gt;puppet-mumble&lt;/a&gt; module installs a mumble server (version 1.2.3) automatically on a CentOS 6.x machine using the puppet software based on &lt;a class="reference external" href="http://mumble.sourceforge.net/Install_CentOS5"&gt;mumble-documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The module needs a repository which contains the &lt;a class="reference external" href="http://www.visibilityspots.com/repos/repoview/mumble-server.html"&gt;mumble-server&lt;/a&gt; package. I distribute this package on my own &lt;a class="reference external" href="http://www.visibilityspots.com/repos/repoview/"&gt;visibilityspots&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;Using puppet this will create the necessary mumble user and group and will configure the mumble-server using your desired settings, like username, password, and tcp port the daemon will listen on.&lt;/p&gt;
</content><category term="puppet"></category><category term="centOS"></category><category term="Linux"></category><category term="module"></category><category term="mumble"></category><category term="mumble-server"></category><category term="open-source"></category><category term="puppet"></category></entry></feed>