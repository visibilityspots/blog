var tipuesearch = {"pages":[{"title":"Contact","text":"","tags":"pages","url":"https://visibilityspots.github.io/blog/pages/contact.html","loc":"https://visibilityspots.github.io/blog/pages/contact.html"},{"title":"Links","text":"Personal inuits.eu , the company I work for The scouting group I participated in Feeds List of all the RSS feeds I follow using newsbeuter Colleagues A list of the blogs from my colleagues Friends Blog of Bart Vermijlen Photo blog of Sander Van Loo Photo - Graphical portfolio of Gilbert Vandenbloeck","tags":"pages","url":"https://visibilityspots.github.io/blog/pages/links.html","loc":"https://visibilityspots.github.io/blog/pages/links.html"},{"title":"Profile","text":"Experience May 2012 - present | Linux & Open-Source consultant at Inuits - ANTWERP Advisement of the clients on the use of or the realization of Open Source software and Open Standards. Combining own knowledge with the shared knowledge of a group of highly experienced colleagues to realize technical high standard projects. November 2017 - present | Operations Engineer - Trendminer - HASSELT Setup and automation of the product's deployment options: cloud-based or on premise. Setup and automation of the internal development tooling and infrastructure. ansible based nomad / docker setup prometheus implementation azure been used as a public cloud July 2017 - October 2017 | Continuous Integration Engineer - Zappware - HASSELT Automation of the CI/CD chains for the components developed by the different teams in a dockerized container world. From IOS to Android to C++ and some backends. Designed and implemented the dockerized container world using a docker swarm cluster, from the jenkins master and the different slaves to the actual artifact which is stored in a nexus repository and deployed on an internal node for QA and development against latest pushed code base for a maven project. June 2015 - June 2017 | System engineer - Newtec Cy - SINT-NIKLAAS Automating the validation environment for the dialog infrastructure using different types of open-source tools like puppet for configuration management, looking at packer for building reproducible image artefacts and a private cloud for the provisioning part. In such a way a validation engineer can easily spin up a specific test environment to perform the necessary test banks to validate the dialog hub against many different setups. Puppetizing of many different tools for testing purposes Implementing the torrent protocol for benchmarking against other protocols Configuring packer templates for the cloud images Setting up an openstack private cloud in line with the current validation environment Next to the use case of the validation tests on the openstack private cloud also using it to have virtual development environments for the dialog platforms so developers have a flexible way of developing/bugfixing the product. Using the many benefits of this setup in a way so automated builds of the dialog product are deployed immediately so bugs are discovered as fast as possible within the development cycle. Last but not least using this virtual environment to bootstrap hardware components within the dialog platform so they match as much as possible the production environments for validation tests. March 2015 - June 2015 | System engineer - International Post Corporation - EVERE Helping the IT Team a hand during a datacenter migration of their production, test and uat environments. Troubleshooting various issues through the migrations of the different tomcat application servers. Provisioning new systems through puppet Implementing puppet roles & profiles Introducing LVM August 2012 - December 2014 | Automation of devops workflow - UGent Boekentoren - GENT Implementation of puppet, maintaining those server configurations. CentOS/RedHat based. Providing support to the dev team by using a bunch of open-source tools. Configuration management for RHEL based servers using puppet Setup fo Icinga monitoring and bacula backup system Migration of puppet 2.x to puppet 3.x Using jenkins for package deployment of internally developed software Implementing foreman and cloudstack for provisioning development machines Optimizing puppet setup in combination with jenkins August 2012 | Monitoring development environment - Belnet - BRUSSELS Analysing the zenoss monitoring software for belnet, a Belgium Telco company. Testing out the zenoss core 4 against the specifications of the belnet network. Deploying zenoss 3 and customizing to the requirements of belnet. April 2010 - May 2012 | Support Engineer at Dataflow Consultancy - HEVERLEE November 2010 - March 2012 | Service Engineer - Belgacom - BRUSSELS Configuration of business customers for both single-site and multi-sites within the explore network (Cisco, Alcatel, Thomson ..) with the specific options as shared netfirewall configuration (FortiGate), mobile data / workers / backup (3G), backup silver and gold, teleworking. Troubleshooting on CPE or backbone to quickly solve problems. April 2010 - October 2010 | Inside Network Operations - Belgacom - BRUSSELS Callback operation for business customers fast internet. Contacting the client after installation, respond to any problems. Appropriate solutions tailored to the customer to increase the customer satisfaction. Education Higher education 2009 - 2010 | Katholieke Universiteit LEUVEN | Faculty of Science Linking program, Master in Applied Informatics 2005 - 2009 | Katholieke Hogeschool LEUVEN | Dept. Health & Technolog Graduation: Bachelor in Applied Informatics Specialization: System engineer Final project : Automated unix server park using puppet Skills Linux - Ubuntu - TCP/IP - Cisco - Unix - LaTeX - Scripting - PHP - Fortinet - Routing - WAN - BGP - VLAN - Ethernet - HSRP - Nagios - Cacti - Cisco Technologies - MySQL - Organizational Behavior - AJAX - Apache - Mobile Communications - Python - Puppet - Eucalyptus - Optical Fiber - Copper Cabling - CentOS - KVM - Xen - Zenoss - Icinga - RedHat - Jenkins - Bash - Bacula - Pelican - Tracks - Hubot - Bacula-web - LVM - Software RAID - ArchLinux - Arduino - Digital Signage - Dashing - Xibo - Concerto - Raspberry pi - Raspbian - ArchLinx arm - Spacewalk - Cobbler - Packer - OpenStack - Packstack - AWS - InfluxDB - Grafana - Telegraph - Icinga2 - Docker - LXC - cAdvisor - Prometheus - Azure - Nomad - Terraform - Consul Certificates CCNA Certificate ( 409464168482JNAN ) Leisure time Scouts & Gidsen Vlaanderen - Commissaris Groepsleiding From December 2011 - December 2012 Support group leaders within the national operation of Scouts en Gidsen Vlaanderen through various projects. Contribute to and promote the educational and structural policies from Scouts en Gidsen Vlaanderen _. Scouts, De Vlasbloem - NIEUWENRODE (B1705G) Actively done for about 14 years, 5 years as a member, 7 years as a chief from which 4 as group leader. Snow- and longboarding","tags":"pages","url":"https://visibilityspots.github.io/blog/pages/profile.html","loc":"https://visibilityspots.github.io/blog/pages/profile.html"},{"title":"Projects","text":"","tags":"pages","url":"https://visibilityspots.github.io/blog/pages/projects.html","loc":"https://visibilityspots.github.io/blog/pages/projects.html"},{"title":"Tools","text":"some packages I created and used along my way in the linux/open-source world hosted by packagecloud hubot chat bot command line chat client irssi IRC to other chat networks bitlbee gateway command line mail client mutt and my config terminal recorder script conky dashboard static blog generator pelican been used to build this blog organize your todo's using taskwarrior and vit synchronized through inthe import your issues from 3th party services like redmine,github,.. to taskwarrior with bugwarrior vdirsyncer , which synchronizes my calendars and contacts command line disk space overview ncdu a presenter console with multi-monitor support for pdf files. woof to transfer a file from the command line i3lock-wrapper to have a fancy lock screen the open-source based browser chromium your caldav based calendar from within the command line khal pass a password store manage your laptops battery through powertop a file browser ranger khard , a console carddav client which integrates with mutt too a lightweight window manager ratpoison creating backups using rsnapshot taking control of your amazon services s3cmd sakis3g is a great tool to enable your 3G connectivity a semantic launcher called synapse thefuck , the magnificent app which corrects your previous console command vagrant used for setting local development environments based on lxc with perhaps one of those boxes I maintain an interactive tool for HTTP inspection called wuzz ctop , top-like interface for container metrics curl statistics made simple by httpstat http-prompt an interactive cli based HTTP client letsencrypt certificates for your cloudfront or S3 instance managed by cerbot-s3front POC minikube to test kubernetes from your local machine","tags":"pages","url":"https://visibilityspots.github.io/blog/pages/tools.html","loc":"https://visibilityspots.github.io/blog/pages/tools.html"},{"title":"Traefik nomad route53 setup","text":"I have this nomad cluster running on some spare devices for a while now. Serving my plane spotting setup, dns setup, mqtt bridge and some other services I experiment with throughout the years. Until today I've always relied on the ip addresses to point my browser and other services towards the different services. For my DNS setup I even had to pin the jobs towards specific hardware using meta data. But I've always wanted to implement a proxy in between so I could rely on DNS names instead. This would also increase the flexibility of my DNS setup since for a couple of months now I figured the proxy I was looking into implemented UDP services too. This proxy is traefik , a few years ago Emile Vauge talked about it on a meetup we organized with Inuits in Prague. Ever since it stood on my todo list to get it implemented on my home lab. But to be honest over the years traefik gained some interest and grew a lot. Which made it less attractive due to it's increased complexity compared to fabio which works very well in combination with consul. However by the time I had it up and running it lacked some letsencrypt integration as well as UDP ports and there where some doubts about it's continuity . Due the lack of time I never got it in the state I wanted it to be and the whole proxy plan got a bit dusted away. This year I started to teach a group of students into linux using nomad consul prometheus as a back bone. So my whole cluster gained some love again and I decided to upgrade it to the next level by implementing traefik! So I started with a simple setup, a nomad traefik job which uses consul as service discovery and added some tags to the traefik job itself to gather the configuration to add itself to the proxy. job \"traefik\" { region = \"global\" datacenters = [ \"DC1\" ] type = \"service\" group \"traefik\" { service { name = \"traefik\" tags = [ \"traefik.enable=true\", \"traefik.http.routers.dashboard.rule=Host(`traefik.example.org`)\", \"traefik.http.routers.dashboard.service=api@internal\", \"traefik.http.routers.dashboard.entrypoints=http\", ] } task \"traefik\" { driver = \"docker\" config { image = \"traefik:v2.3.6\" force_pull = true network_mode = \"host\" logging { type = \"journald\" config { tag = \"TRAEFIK\" } } volumes = [ \"local/traefik.toml:/etc/traefik/traefik.toml\" ] } template { destination = \"local/traefik.toml\" data = << EOF [ globals ] sendAnonymousUsage = false checkNewVersion = false [ entryPoints ] [ entryPoints.http ] address = \":80\" [ api ] dashboard = true insecure = true [ providers.file ] filename = \"/etc/traefik/traefik.toml\" [ providers.consulCatalog ] prefix = \"traefik\" exposedByDefault = false [ providers.consulCatalog.endpoint ] address = \"http://CONSUL-SERVER:8500\" datacenter = \"DC1\" EOF } } } } by starting this nomad job I got a first working dashboard after I configured traefik.example.org pointing towards the host where my traefik job was running upon in /etc/hosts. As a second step I configured some middlewares. BasicAuth to enable authentication for several services so I could maybe expose them to others in the future. And some redirection for http towards https. To achieve this I added an extra https entrypoint to the template and added the middleware as a tag to the traefik job; [entryPoints] [entryPoints.http] address = \":80\" [entryPoints.https] address = \":443\" group \"traefik\" { service { name = \"traefik\" tags = [ \"traefik.enable=true\", \"traefik.http.middlewares.http2https.redirectscheme.scheme=https\", As a start I configured every service separately to redirect its http traffic towards https. But I found out by reading through some issues in the community from an answer by Kugel that this could also achieved by setting a global redirect; group \"traefik\" { service { name = \"traefik\" tags = [ ... \"traefik.http.routers.catchall.rule=HostRegexp(`{host:(www\\\\.)?.+}`)\" , \"traefik.http.routers.catchall.entrypoints=http\" , \"traefik.http.routers.catchall.middlewares=http2https\" , by issuing this catchall rule all the http incoming traffic will be redirected to their https service done in one place instead of a separate configuration for every service. I gained some traction and configured letsencrypt, by default the letsencrypt certificateResolver uses the httpChallenge, but I didn't want to forward all http/https traffic from the internet towards my setup. Also when you want to use a wildcard certificate you'll have to use the DNS-01 challenge. I looked into this wildcard domain since I hit the rate limitations quite fast. I asked certificates for every sub domain using my routers and forgot to mount the acme.json file. So I went to the internet, found a solution for using a wildcard certificate for different services and fixed the acme.json mount. Since I already got my domains configured in AWS I added a CNAME record for all my sub domains to point to the internal ip of the traefik instance which I pinned to a specific nomad client using metadata. Luckily there is a route53 provider, which only needs some configuration in the AWS backend such as the IAM policy and a user from which you grab the ID's and keys to configure as environment variables towards the traefik container; task \"traefik\" { driver = \"docker\" env { AWS_ACCESS_KEY_ID = \"\" AWS_SECRET_ACCESS_KEY = \"\" AWS_HOSTED_ZONE_ID = \"\" AWS_REGION = \"eu-west-1\" } Once the AWS part is done you can configure the certificateResolver part in the traefik.toml static configuration mark the commented caServer to use the staging environment of letsencrypt first before putting it into production to not hit the rate limit too soon ;); [certificatesResolvers.letsencrypt.acme] email = \"letsencrypt@example.org\" storage = \"/etc/traefik/acme/acme.json\" # caServer = \"https://acme-staging-v02.api.letsencrypt.org/directory\" [certificatesResolvers.letsencrypt.acme.dnsChallenge] provider = \"route53\" delaybeforecheck = \"0\" and last but not least you'll have to reconfigure the traefik consul tags so the tls will be used; group \"traefik\" { service { name = \"traefik\" tags = [ \"traefik.http.routers.dashboard.rule=Host(`traefik.example.org`)\", \"traefik.http.routers.dashboard.service=api@internal\", \"traefik.http.routers.dashboard.entrypoints=https\", \"traefik.http.routers.dashboard.tls=true\", \"traefik.http.routers.dashboard.tls.certResolver=letsencrypt\", \"traefik.http.routers.dashboard.tls.domains[0 ] . main = example . org \", \" traefik . http . routers . dashboard . tls . domains [ 0 ] . sans =* . example . org \" ] Additional services on their turn can be configured in way they only have to enable tls and therefore falling back towards the wildcard certificate; group \"helloworld\" { service { name = \"helloworld\" tags = [ \"traefik.enable=true\", \"traefik.http.routers.helloworld-secure.rule=Host(`helloworld.example.org`)\", \"traefik.http.routers.helloworld-secure.entrypoints=https\", \"traefik.http.routers.helloworld-secure.tls=true\", \"traefik.http.routers.helloworld-secure.middlewares=default-auth\" ] So now letsencrypt only requests a wildcard for my domain with *.example.org as an alternative and all my services are using that wildcard. That way I could keep the letsencrypt communication to a bare minimum for my setup. Next up was the long waiting UDP feature, therefor I had to configure a separate UDP entrypoint ; [entryPoints.dns] address = \":53/udp\" and a TCP router using tags in the pihole nomad job; group \"pihole\" { service { name = \"pihole\" tags = [ \"traefik.enable=true\", \"traefik.udp.routers.pihole-dns.entrypoints=dns\" ] That way I don't have to pin the pihole containers anymore to a specific node, traefik will proxy the DNS traffic towards the pihole service dynamically which is again a little step up in my humble home lab :) And as finishing touch I also have configured the nomad service on every node with some consul tags by doing so I'm able to configure a router using tags in the traefik job; \"traefik.http.routers.nomad-ui.rule=Host(`nomad.example.org`)\" , \"traefik.http.routers.nomad-ui.service=nomad-client@consulcatalog\" , \"traefik.http.routers.nomad-ui.entrypoints=https\" , \"traefik.http.routers.nomad-ui.tls=true\" nomad.example.org will now be redirected to the nomad dashboard through one of the different clients. Since it's a cluster I will always get the same state and don't have to bother which client to access :) So I have now a nomad cluster running with containers I have absolutely no clue where they are running upon neither on which port they are listening. Traefik is the piece of software in between me using sub domains to access my services and the actual container running the software. What I still need to look into is to use this proxy also to configure redirections towards static services like my synology and it's services as well as my router's interface etc. And I need to figure out how I could dynamically redirect consul.example.org towards the consul ui!","tags":"containers","url":"https://visibilityspots.github.io/blog/traefik-nomad-route53.html","loc":"https://visibilityspots.github.io/blog/traefik-nomad-route53.html"},{"title":"Plane spotting on a nomad cluster","text":"Some weeks ago I upgraded my plane spotting setup by moving my antenna to the roof . It was worth every single effort I've made into it. My stats are rocking ever since. Until the corona crisis halted almost every airline to standstill.. It gave me some time to thinker about my setup, and for some weird coincidence Mike did create a series of docker containers like I was thinking about to implement. I have one pi connected to the USB device which captures the ADB radio signals. But in the current situation it also feeds the flightaware service since I used the docker container for piaware. So I wanted to split those up. And lucky for me Mike did some great work by creating a readsb container which only captures the signals and can be provides them to a port which can be easily consumed by services which feeds the positions towards a specific service. Mike did a great effort on that part too and crafted several docker images which are able to consume an external beast host to provide the messages towards the services; flightaware flightradar24 adsbexchange radarbox planefinder By doing so I could have the pi attached to the USB device dedicated for the readsb container and the other services are consuming other nodes in the nomad cluster to provide the leverage towards upstream. That way I have my setup spread over my whole nomad cluster. Which makes it totally future proof and made my day during this lockdown.","tags":"other","url":"https://visibilityspots.github.io/blog/planespotting.html","loc":"https://visibilityspots.github.io/blog/planespotting.html"},{"title":"Httpd semaphores","text":"Recently we encountered some strange issues with httpd on some of our CentOS 7 machines during my current project. Through our pipeline we restart httpd several times which sometimes leads to this error: Apache : [ error ] ( 28 ) No space left on device After some research we found out the semaphores were all being used blocking httpd daemon to restart. The list of semaphores can be fetched by issuing # ipcs -st ------ Semaphore Operation/Change Times -------- semid owner last-op last-changed 753664 apache Not set Mon Feb 17 20:20:47 2020 786433 apache Not set Mon Feb 17 20:20:47 2020 720898 apache Not set Mon Feb 17 20:20:47 2020 819203 apache Not set Mon Feb 17 20:20:47 2020 851972 apache Tue Feb 18 10:04:36 2020 Mon Feb 17 20:20:47 2020 884741 apache Tue Feb 18 10:04:36 2020 Mon Feb 17 20:20:47 2020 1540102 apache Not set Wed Feb 19 22:57:02 2020 1572871 apache Not set Wed Feb 19 22:57:02 2020 1507336 apache Not set Wed Feb 19 22:57:02 2020 1605641 apache Not set Wed Feb 19 22:57:02 2020 1638410 apache Thu Feb 20 11:16:48 2020 Wed Feb 19 22:57:02 2020 1671179 apache Thu Feb 20 11:16:48 2020 Wed Feb 19 22:57:02 2020 3276812 apache Not set Sun Feb 23 20:18:54 2020 3309581 apache Not set Sun Feb 23 20:18:54 2020 3244046 apache Not set Sun Feb 23 20:18:54 2020 3342351 apache Not set Sun Feb 23 20:18:54 2020 3375120 apache Mon Feb 24 11:30:21 2020 Sun Feb 23 20:18:54 2020 3407889 apache Mon Feb 24 11:30:21 2020 Sun Feb 23 20:18:54 2020 3538962 apache Not set Mon Feb 24 11:30:21 2020 Open connections are not cleared while restarting the httpd daemon in our case unfortunately which leads after some time into the error. We did found out that clearing those semaphores fixed the issue. Initially we did this manually by executing a for loop; # ipcrm sem $(ipcs -s | grep apache | awk '{print$2}') Obviously we didn't wanted to wait for our alerting or colleagues to shout when the httpd daemon is stuck. So we configured the ExecStopPost parameter of the httpd systemd unit. This is done by simply adding a drop in configuration file for that unit # cat / etc / systemd / system / httpd . service . d / clean - semaphores . conf [ Service ] ExecStopPost =/ usr / bin / ipcs - s | awk '$3 == \"apache\" {system(\"ipcrm sem \" $2)}' This will clean out the left over semaphores when the httpd daemon has been stopped right before it starts again. references; https://access.redhat.com/solutions/78873 https://www.crybit.com/semaphores-linux/","tags":"linux","url":"https://visibilityspots.github.io/blog/httpd-semaphores.html","loc":"https://visibilityspots.github.io/blog/httpd-semaphores.html"},{"title":"Piaware on a nomad cluster","text":"A couple of years ago I stumbled on the flightaware website and figured out you could capture radio signals from an airplane using a DVB-T dongle which can be bought for about EUR 15. As this really triggered me to start monitoring planes above our head I bought myself such a device, hooked it up to a raspberry pi zero, installed the piaware software and started grabbing the messages and pushing the compiled positions through flightaware. Through time I managed to upgrade my home lab towards a nomad cluster . So I migrated my piaware setup to a nomad node and wrote a nomad job file to run the docker image. This was a great achievement in my cluster setup regarding the software. For the hardware I initially used a cheap DVB-T dongle but I wanted to get more planes and positions so I invested into better gear. After saving enough by skipping the weekly wok days at work I ordered myself a 1090MHz ADS-B N-Type antenna in combination with the Flightaware Pro Stick Plus . To attach the two together I also got myself a 15m antenna RF coaxial cable . As a first test setup I used a large bamboo pole which I hooked up into the garden as you can see; The pi was placed inside of the garage and connected through the wireless network. Unfortunately I had a bad reception and installed a powerline based AP to have a stronger signal in the garage. But this setup suffered from outages from time to time. Causing the pi to lose connection and therefore many messages got lost :( Some auto reboots every now and then solved the connectivity issues as a quick 'ducktaped' fix but I wasn't quite happy with it. The temporary solution of the bamboo stick stood for about a year. Since I didn't wanted (neither was allowed by my girlfriend) to drill a hole through the roof. But I figured out our central heating unit has a fresh air outlet I could use to go to the roof. So once equipped with a fancy new pole made by my brother I went for it and installed the antenna on the roof and the pi indoors attached with a network cable instead of the wifi; I then made some time to update the docker container images I used from Mike Nye and create some pull requests. Once he merged those I upgraded my nomad job file and running on the latest available stable version of piaware in a container! The move of the antenna together with a stable connection was a big success seen my stats increased significantly! Still the 'Klitsberg' nearby our home is blocking some signals from that direction unfortunately.. Maybe someday I could set up an antenna on top of that hill who knows. A few days after I upgraded I figured Mike released a docker-fr24feed container image which works in tandem with the docker-piaware one. So I went for it adjusted the nomad job file and now I'm not only pushing messages towards flightaware but also towards flightradar24 Another thing I still need to figure out is the adjustment of the gain which could me bring even better statistics as today :) Also since becoming a business member on both platforms because being a feeder could open up a way to an API I could start using to setup my wemos based plane spotter in the future!","tags":"other","url":"https://visibilityspots.github.io/blog/piaware.html","loc":"https://visibilityspots.github.io/blog/piaware.html"},{"title":"Nomad ARM cluster","text":"as I already mentioned a few times before I have some hands on experiences with nomad . A couple of weeks ago I wrote about a local development setup based on nomad. Since quite some time I do have a thinclient which is running some docker containers through a docker-compose file I use for personal stuff at home. But this thinclient is suffering from all the containers I'm trying to spin up. While over thinking this issue I did realize I have quite some raspberry pi's laying around and figured I could maybe set up a cluster for those containers. Since my previous contact with nomad I thought I gave it a try and spin up a nomad home lab cluster. Priorities first I bought myself a cluster rack tied it together with a small switch and a power supply. Next I created a custom ArchLinux ARM image based on the work of disconnected systems which I very appreciated since it saved me quite some time! Once the image was created I pushed the SD cards in the pi's and booted them all. I did decide to use consul to automate the clustering part as described in the nomad operations guides . That way new nomad clients and servers are automatically recognized and added to the cluster when registered in consul. The side effect of this is a chicken and egg issue, while it would have been cool to run consul as a docker container in the nomad cluster it couldn't be used to auto join the cluster since consul needs to be running before nomad. That's the reason I opted to run consul as a system service instead. docker Since we will use the docker ecosystem to run our containers using nomad the docker daemon needs to be installed configured and ran on the system as explained on the archwiki consul Consul is a service networking solution to connect and secure services across any runtime platform and public or private cloud. installation first things first, installing consul using pacman $ sudo pacman -S consul configuration after that some configuration is required for all the consul nodes by creating a data directory and give the ownership to the consul user $ sudo mkdir /opt/consul $ sudo chown -R consul: /opt/consul next step on the list is the actual configuration of the consul service for a server functionality /etc/consul.d/config.json { \"acl_default_policy\": \"allow\", \"addresses\": { \"dns\": \"0.0.0.0\", \"grpc\": \"0.0.0.0\", \"http\": \"0.0.0.0\", \"https\": \"0.0.0.0\" }, \"advertise_addr\": \"IP.ADDRESS.OF.THIS.NODE\", \"advertise_addr_wan\": \"IP.ADDRESS.OF.THIS.NODE\", \"bind_addr\": \"0.0.0.0\", \"bootstrap\": true, \"bootstrap_expect\": \"3\", \"client_addr\": \"0.0.0.0\", \"data_dir\": \"/opt/consul\", \"datacenter\": \"NAME_DC\", \"disable_update_check\": false, \"domain\": \"consul\", \"enable_script_checks\": false, \"enable_syslog\": true, \"encrypt\": \"ENCRYPTION_KEY\", \"log_level\": \"INFO\", \"node_name\": \"NODE NAME\", \"performance\": { \"leave_drain_time\": \"5s\", \"raft_multiplier\": 1, \"rpc_hold_timeout\": \"7s\" }, \"ports\": { \"dns\": 8600, \"http\": 8500, \"server\": 8300 }, \"raft_protocol\": 3, \"retry_interval\": \"30s\", \"retry_interval_wan\": \"30s\", \"retry_join\": [ \"CONSUL_SERVER_IPS\", \"CONSUL_SERVER_IPS\", \"CONSUL_SERVER_IPS\" ], \"retry_max\": 0, \"retry_max_wan\": 0, \"server\": true, \"syslog_facility\": \"local0\", \"ui\": true } This configuration is based on a 3 server node setup, when adding extra clients without the server role you can reuse this config but remove the line of bootstrap_expect and change server to false when configuring a consul client only node. Most of the parameters to configure should be self explaining if not you can always check the consul documentation for more detailed explanations. ! The encryption key needs to be generated as explained in the agent encryption section. service # systemctl start consul.service # systemctl enable consul.service gui If everything went well you should be able to access the consul web interface on port 8500 at one of the configured ip addresses. nomad Finally the last and coolest part in the whole setup, nomad ; the actual cluster orchestrater piece of software. configuration There are 3 configuration parts; base server client All of them based in /etc/nomad.d/ I began with a 3 server/client node setup but added extra client only nodes afterwards. /etc/nomad.d/base.hcl name = \"NODENAME\" region = \"global\" datacenter = \"DATACENTERNAME\" advertise { http = \"IP.ADDRESS\" rpc = \"IP.ADDRESS\" serf = \"IP.ADDRESS\" } consul { # The address to the Consul agent . address = \"localhost:8500\" # The service name to register the server and client with Consul . server_service_name = \"nomad-servers\" client_service_name = \"nomad-clients\" # Enables automatically registering the services . auto_advertise = true # Enabling the server and client to bootstrap using Consul . server_auto_join = true client_auto_join = true } data_dir = \"/opt/nomad\" log_level = \"INFO\" enable_syslog = true /etc/nomad.d/server.hcl server { enabled = true raft_protocol = 3 bootstrap_expect = 3 } /etc/nomad.d/client.hcl client { enabled = true meta { } } plugin \"docker\" { config { allow_caps = [ \"ALL\" ] } } service a systemd unit file is written which will combine all the previous configuration files to start the nomad services; /usr/lib/systemd/system/nomad.service [Unit] Description = nomad agent Wants = basic.target After = basic.target network.target consul.service [Service] User = root Group = bin ExecStart = /usr/local/bin/nomad agent -config=/etc/nomad.d ExecReload = /bin/kill -HUP $MAINPID KillMode = process Restart = on-failure RestartSec = 42s [Install] WantedBy = multi-user.target now start and enable the nomad service so we can access the gui to see if everything works as expected. # systemctl start nomad.service # systemctl enable nomad.service gui future Since I'm migrating my current stacks I will add new post for each of those services I tackled afterwards. Starting with the DNS topic and a vault integration. Also for the different services and how I achieved to run them on my containerized cluster I will try to keep you in the loop through my blog. For now I use an NFS share from my synology station configured on all the nodes using autofs as shared storage. I do hope to get time once to setup a ceph cluster on those nodes too :D Another topic I would like to tackle are the job files. Right now those are stored in an NFS share but somehow I would like to have something more advanced to run them somehow..","tags":"containers","url":"https://visibilityspots.github.io/blog/nomad-arm-cluster.html","loc":"https://visibilityspots.github.io/blog/nomad-arm-cluster.html"},{"title":"ArchLinux on intel compute stick","text":"A few months ago we moved into a brand new office which was furnished with a dozen of samsung displays. Unfortunately the basic player included in those displays isn't capable to add a webpage/url as content. Since we've setted up a smashing instance to create dashboards for each team this was a huge bummer. While looking for a stable solution many teams brought their own raspberry pi's, chromecasts, airtame devices to at least be able to show something on the displays in the meanwhile. Since we already had good experiences with an intel compute stick and an intel NUC we decided to get and configure about 8 compute sticks model STK1A32SC with archlinux running to be able to display our dashboards. For our stand up corners we went for 3 intel NUC's with some peripherals like a web-cam/keyboard and a jabra device to provide proper communication during the stand-ups. But back to the compute sticks. Initial setup since we had to install and configure about 8 sticks we decided to configure a base archlinux setup on one stick and using dd afterwards to get the others up and running with a basic archlinux stack. BIOS before we could boot a live usb archlinux distro we had to change the operating system setting in the bios to Android boot device press F2 -> Select Operating System -> Android once that's done and you've rebooted press F10 to boot from the live USB distro basic setup following guide use wifi-menu command to connect to your wireless network to get some network connectivity first following the basic installation guide we went for an UEFI setup with a GPT partitioned disk. and opted for this layout where 20G is reserved for the root partition, 7.6G as var and 1G for swap. partition layout Device Start End Sectors Size Type / dev / mmcblk0p1 2048 1050623 1048576 512 M EFI System / dev / mmcblk0p2 1050624 3147775 2097152 1 G Linux swap / dev / mmcblk0p3 3147776 45090815 41943040 20 G Linux filesystem / dev / mmcblk0p4 45090816 61071326 15980511 7 . 6 G Linux filesystem We configured grub assuming the EFI partition being mounted as boot and chrooted using arch-chroot as being explained in the installation guide. # grub-install --target=x86_64-efi --efi-directory=boot --bootloader-id=GRUB tools some tools we preinstalled where the SSH daemon along with an authorized key for a specific user and python to be able to run ansible afterwards. also we configured the wireless network already using systemd-network This besides the preferred stuff which is described in the installation guide. when everything is installed through the installation guide using arch-chroot you can go ahead and reboot reboot when you have rebooted you should now enter the Grub to the installed archlinux distribution based on the disk of the compute stick. Once that's working fine you can go ahead. create an image with dd on separate stick as soon as you got a working compute stick you can reboot and now use the live usb distro again # wifi-menu # systemctl start sshd.service # passwd now try to ssh into the live distro from another machine so you could unplug the keyboard and use that second USB port to connect an empty USB drive to store the image on. # lsblk NAME MAJ : MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 : 0 0 500 . 8 M 1 loop / run / archiso / sfs / airootfs sda 8 : 0 1 14 . 5 G 0 disk ├─ sda1 8 : 1 1 614 M 0 part / run / archiso / bootmnt └─ sda2 8 : 2 1 64 M 0 part sdb 8 : 16 1 28 . 9 G 0 disk └─ sdb1 8 : 17 1 28 . 9 G 0 part mmcblk0 179 : 0 0 29 . 1 G 0 disk mmcblk0boot0 179 : 8 0 4 M 1 disk mmcblk0boot1 179 : 16 0 4 M 1 disk mount the additional USB drive and start creating an image through a screen session # mount / dev / sdb1 / mnt # screen - S image - creation # dd if =/ dev / mmcblk0 conv = sync , noerror bs = 64 K status = progress | gzip - c > / mnt / base - image - dashboards . img . gz restore image to restore the image on a new compute stick you have to boot again in live distro mode and enable wifi + ssh to be able to unplug the keyboard USB port once again # wifi-menu # systemctl start sshd.service # passwd # lsblk NAME MAJ : MIN RM SIZE RO TYPE MOUNTPOINT loop0 7 : 0 0 500 . 8 M 1 loop / run / archiso / sfs / airootfs sda 8 : 0 1 14 . 5 G 0 disk ├─ sda1 8 : 1 1 614 M 0 part / run / archiso / bootmnt └─ sda2 8 : 2 1 64 M 0 part sdb 8 : 16 1 28 . 9 G 0 disk └─ sdb1 8 : 17 1 28 . 9 G 0 part mmcblk0 179 : 0 0 29 . 1 G 0 disk mmcblk0boot0 179 : 8 0 4 M 1 disk mmcblk0boot1 179 : 16 0 4 M 1 disk next mount the USB drive with the base image on # mount /dev/sdb1 /mnt # ls /mnt base-image-dashboards.img.gz lost+found then start a screen session to unpack the image to the disk of the compute stick # screen -S restore # gunzip -c /mnt/base-image-dashboards.img.gz | dd of=/dev/mmcblk0 status=progress # umount -R /mnt and last but not least initiate grub to install the UEFI partition # mount /dev/mmcblk0p3 /mnt :( # mount /dev/mmcblk0p1 /mnt/boot # mount /dev/mmcblk0p4 /mnt/var # arch-chroot /mnt # grub-install --target=x86_64-efi --efi-directory=boot --bootloader-id=GRUB Installing for x86_64 - efi platform . Installation finished . No error reported . # exit # umount -R /mnt # reboot the compute stick should now boot into the new ArchLinux distribution installed on it's disk and can be configured using ansible. kiosk mode the main goal of our use case was to show an url. First idea was to use luakit as a browser. But luakit isn't available in the official repositories and isn't able to rotate different tabs. So we went for chromium which is started without a window-manager and nodm to automatically start an x session at boot. Some quirks we had to resolve where the disabling of the auto restore of chromium by altering the Default/Preference file and setting the values of exit_type to none and exited_cleanly to true after which we made the file read only by making it immutable with the chattr command.","tags":"linux","url":"https://visibilityspots.github.io/blog/compute-sticks.html","loc":"https://visibilityspots.github.io/blog/compute-sticks.html"},{"title":"Nomad local development","text":"Been using nomad for a few years now at the customer I got the question couple of weeks ago from some colleagues if nomad could be used to replace docker-compose. The reason behind the question is mainly to not vendor lock yourself for local development with the whole docker eco system. Since I like a certain level of challenge and I do believe it's a valid use case I gave it a try. This resulted in a nomad-local-development repository. The 2 major hurdles to take where to use one file like docker-compose.yml and dns resolving between the containers. The first one was a rather easy one to achieve. By grouping multiple tasks into a group you can use one job file to start multiple containers. This however doesn't seems to be best practice in the field but for our use case this works quite well. Resolving DNS between the different containers was a bit harder to tackle in regard to docker-compose where it's working out of the box. To tackle this in nomad one could use consul as a service discovery system. Consul comes by default with a DNS interface which can be used to resolve the registered tasks by nomad. By combing this interface with DNS forwarding using dnsmasq this can be solved in a rather proper manner in this matter. When using a default nomad docker task the container will be registered with his nomad client IP in consul. To get around this you could use the parameter address_mode = \"driver\" in the service. That way the docker internal ip will be registered and used to resolve when using the consul DNS. I used vagrant to create a sandbox for this test case to fiddle around the nomad ecosystem. How to use the setup is described in the README Feel free to share your thoughts idea's and suggestions!","tags":"containers","url":"https://visibilityspots.github.io/blog/nomad-local-development.html","loc":"https://visibilityspots.github.io/blog/nomad-local-development.html"},{"title":"BIOS upgrade lenovo archlinux","text":"I got some issues with my wired connection lately that the speed wasn't negotiated correctly and it felt back to 10Mb/s as default. Did some troubleshooting by eliminating various network devices, restarting them but the results didn't satisfy. Being completely random when and when not auto negotiated. Before becoming insane I decided to update the bios of my machine (being a lenovo T460s). I did this already in the past and talked about it even on one of our monthly last Friday's at work . So I was quite sure I had something written about it for future reference but I couldn't find it anymore. So I decided to write a post how I did it this time so I could refer to it for myself in the future since I don't do this regularly.. First of all you have to find out your serial number so we can download the latest bios from lenovo. By using dmidecode this can be done through your terminal $ sudo dmidecode -s system-serial-number This serial number can be used on the lenovo support site, where you should find the BIOS Update (Bootable CD) iso in the list of Drivers & Software. Once downloaded we will use geteltorito script as being described on the ArchWiki to extract the iso into an image file which we can copy onto an USB stick $ geteltorito.pl -o <image>.img <image>.iso $ lsblk -l $ sudo dd if = <image>.img of = <destination> bs = 512K This USB device can now be used to boot from (during boot process press ENTER followed by F12 where you can select the USB device) It's advised to connect your power supply and your battery has been charged about 80-100% before upgrading your BIOS. After I had upgraded my BIOS my speed is negotiated correctly. I'm not sure if upgrading did the trick since I rebooted all the network devices where I had this issue but it works now and I have an upgraded BIOS so I'm a happy surfer again. :) Some references: https://www.cyberciti.biz/faq/update-lenovo-bios-from-linux-usb-stick-pen/ https://wiki.archlinux.org/index.php/Flashing_BIOS_from_Linux https://workaround.org/article/updating-the-bios-on-lenovo-laptops-from-linux-using-a-usb-flash-stick/","tags":"linux","url":"https://visibilityspots.github.io/blog/bios-upgrade-lenovo.html","loc":"https://visibilityspots.github.io/blog/bios-upgrade-lenovo.html"},{"title":"Archlinux ARM pi zero cups network print server","text":"Printing Probably like many amongst us the time of the Christmas holidays is perfect to get some IT related stuff back on track. I used to have a print server setup which got broken over time and I didn't found the energy to invest time into fixing it. But the pressure became higher and higher. 111 From both my wife and daughter, especially during the holidays where the wife want to use it to print out tickets and the daughter want to print out color plates.. So during one of the evenings I pulled myself together and installed ArchLinux ARM on a pi zero w and went through the following configuration. First off all install and configure cups on the pi zero following the Arch wiki Do not configure any printer yet, only install cups. When following the guide you can also set cups to be available on our local network through the browser on port 631; # vim / etc / cups / cupsd . conf # Only listen for connections from the local machine . # Listen localhost : 631 Port 631 Listen / run / cups / cups . sock # Restrict access to the server... <Location /> Order allow,deny Allow from @LOCAL </Location> # Restrict access to the admin pages... <Location /admin > Order allow,deny Allow from @LOCAL </Location> # Restrict access to configuration files... <Location /admin/conf > AuthType Default Require user @SYSTEM Order allow,deny Allow from @LOCAL </Location> Once that's done you can start the cups service and enable it to start at boot; # systemctl start cups.service # systemctl enable cups.service Next we will configure the USB connected printers as raw on the pi. first get your printer USB handle # lpinfo -v | grep usb: direct usb://Samsung/CLP-310%20Series?serial=################ add a new printer with the found USB handle # lpadmin -p Visibilityspots-LaserJet -v usb://Samsung/CLP-310%20Series?serial=################## # lpstat -p Visibilityspots-LaserJet -l printer Visibilityspots-LaserJet disabled since Mon Dec 24 20:51:24 2018 - reason unknown enable the printer # cupsenable Visibilityspots-LaserJet # lpstat -p Visibilityspots-LaserJet -l printer Visibilityspots-LaserJet is idle. enabled since Mon Dec 24 20:51:54 2018 make the printer accept jobs # cupsaccept Visibilityspots-LaserJet We do have a server now which can accept print jobs. But before sending print request we need to configure our clients too. Since we configured a raw printer we need to install the printer drivers on the clients. Depending on you distribution you need to install some specific packages for the specific print drivers for your printer. I on my ArchLinux I had to install the packages samsung-drivers for my samsung printer and hplib for the hp one. To get the printers configured I just used the cups web interface on my machine and used the URI http://IP-ADDRESS-PRINT-SERVER-ON-A-PI:631/printers/Visibilityspots-DeskJet in combination with the driver for the specific model. After that I could easily print the common know print test page which magically printer the tux!! Scan Besides printing I also wanted to enable the scan option on one of the dives over the network. This was rather easy as being described on the ArchWiki after I installed both sane and hp-lib on the print-server too. Do not forget to enable the saned.socket systemd service on your print-server! $ sudo systemctl enable saned.socket And now I'm able to put something in the scanner bed and execute a custom scan function I wrote in my zsh config $ scan test-image zsh config: scan () { scanimage --device \"net:IP-ADDRESS-PRINT-SERVER-ON-A-PI:hpaio:/usb/Deskjet_F4100_series?serial=#################\" --resolution 600 -p | pnmtops | ps2pdf - \"$1.pdf\" } which creates a pdf file in the current directory based on the scanned file. So yipeeyayee I do now have again a working network based print/scan setup without too much effort and still using our old offline printer setups!! references: https://stackoverflow.com/questions/26329186/creating-a-raw-printer-queue-in-cups-host-and-adding-them-through-cups-client https://forum.manjaro.org/t/how-to-set-up-a-remote-printer-which-is-attached-to-a-raspberry-pi-or-any-other-arm-computer/57056y","tags":"linux","url":"https://visibilityspots.github.io/blog/print-server.html","loc":"https://visibilityspots.github.io/blog/print-server.html"},{"title":"Ansible-playbook archlinux upgrade","text":"Since a few years now I'm a happy Archlinux user. I like their philosophy which was one of the major points why I made the switch back in the days. I'm not only using it on my laptop, but do have some devices running at home which are configured with it. From a thin client which I use as a docker node through some raspberry pies running ArchlinuxARM . Since Arch is a rolling update distro there are several updates available throughout the day. To keep on top of them I had to log in on all those devices at least once a day to perform the updates. Experience learned me that let them drifting could lead to some major troubles when only updating after a few weeks. But it became a time consuming task to keep them all in line. Since ansible is used at the project I'm currently working at it seemed a good idea to write a playbook to update all those devices with only one command. And without having to configure some additional software on all the devices but based on good old SSH. Ansible already has a default pacman module which can be used for the official repositories. But since a lot of packages I installed are coming from the AUR I first went with a command execution for aurman . After some research I found out about ansible-aur a bit later so I installed the module and rewrote my playbook so it used the aurman helper. But only after a few weeks I found out that the developer wasn't really born with an open-source mind as can be seen by his commits dcb50aa & c409fee so I went for the yay implementation instead. In the initial phase I used to push my passwords as hashes into the playbook. But when I was about to push the playbook in github I figured it wouldn't be a good idea to share that with the public. So I stumbled on ansible-vault . That way I could refer to passwords in an encrypted file in the playbook so I could safely push the playbook to the public. In combination with the parameter --vault-password-file I can now run the playbook without interaction for passwords. And it works great, keeping them all up to date and having a clear output about which packages are updated on which machine. Yet another step closer to that ultimate dream of drinking cocktails on the beach while everything is running automatically in the back!","tags":"linux","url":"https://visibilityspots.github.io/blog/ansible-archlinux-upgrade.html","loc":"https://visibilityspots.github.io/blog/ansible-archlinux-upgrade.html"},{"title":"Prometheus export/import","text":"bumping into the case where once deployed a full stack application we don't have any direct connection due to no uplink for security reasons. So we (you too @Tom ) looked into a way to export the prometheus data into a tar.gz which could be transferred and imported into an instance on our local machine. After the initial blog post where we created a tar.gz file from the prometheus storage.tsdb.path on the filesystem @roidelapluie pointed me out about the snapshot feature. So we did a bit of research and came up with this new procedure. First of all make sure the prometheus service is running with the parameter --web.enable-admin-api which is disabled by default. For nomad I created a job file which enables this parameter through the args for you. Once you have the prometheus instance running with the admin api enabled you can use this api to create a snapshot: $ curl -XPOST http://localhost:9090/api/v1/admin/tsdb/snapshot { \"status\" : \"success\" , \"data\" : { \"name\" : \"20180611T130634Z-69ffcdcc60b89e54\" }} Next up is to collect the snapshot directory on your local machine and mount it into a fresh prometheus docker container for example. ( $ docker container list ) ( CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ) ( e19269f0c82c cc866859f8df \"/bin/prometheus --c…\" 45 minutes ago Up 45 minutes prometheus-eccbde40-c402-60cf-bee7-04a2e7e77883 ) ( $ docker cp e19269f0c82c:/prometheus/snapshots /tmp/ ) $ docker run --rm -p 9090 :9090 -uroot -v /tmp/snapshots/20180611T130634Z-69ffcdcc60b89e54/:/prometheus prom/prometheus --config.file = /etc/prometheus/prometheus.yml --storage.tsdb.path = /prometheus When you now go to http://localhost:9090 you have the data available from the snapshot and you could start troubleshooting. For example by starting a grafana container next to this prometheus container and configuring the prometheus one as data source. That way you could create some dashboards for readability. references: - https://www.nomadproject.io/guides/nomad-metrics.html - https://www.robustperception.io/taking-snapshots-of-prometheus-data/","tags":"containers","url":"https://visibilityspots.github.io/blog/prometheus-export-import.html","loc":"https://visibilityspots.github.io/blog/prometheus-export-import.html"},{"title":"Prometheus consul service discovery","text":"as published a few months ago I worked out a dockerized a jenkins farm where both master as slaves are docker containers working together with services like nexus and such. Next to that setup I've dockerized my home setup where services like pi-hole, home-assistant and others are running as docker containers on a thin client I promoted to my home lab. To have an overview about all those containers and the resources they are consuming I pulled in the git repo of Brian Christner which spins up a whole prometheus stack with some exporters and a grafana instance to visualize the different aspects of those containers. The prometheus has been configured manually with hard coded endpoints to scrape in both situations. It works fine but somehow I would have liked prometheus to automatically recognizing the endpoints himself. Luckily I bumped into nomad at my current project. Nomad is a tool for managing a cluster of machines and running applications on them which uses consul as a key value store. And guess what, prometheus has an integration with consul! Isn't that just great! By configuring prometheus to find it's endpoints in consul the only line of code is the one to point prometheus where he can find consul. - job_name: 'self' consul_sd_configs: - server: 'localhost:8500' services: [] But we went a bit further and used tags for that auto discovery. Prometheus will only fetch endpoints which are registered in consul with a certain tag. That way we hold some control in the configuration after all. relabel_configs : - source_labels : [ __meta_consul_tags ] regex : . * , metrics ,. * action : keep Another line is used to rename the endpoints in prometheus by a more human readable one instead of the auto generated one - source_labels : [ __meta_consul_service ] target_label : job And that's about it. With this prometheus.yml configuration file services started through nomad with the proper 'metrics' tag are auto discovered by prometheus as target. To demonstrate this behavior I created a github repository based on vagrant inspired by the getting started guide of nomad. Following the README a prometheus consul stack is configured and running with 2 exporters you can automatically add to prometheus by starting them through nomad. It's a pretty cool feeling when the appear and disappear without any manual configuration! A great reference which cleared my mind during my quest on this topic came from robust perception","tags":"containers","url":"https://visibilityspots.github.io/blog/prometheus-consul.html","loc":"https://visibilityspots.github.io/blog/prometheus-consul.html"},{"title":"dockerized DNS over HTTPS using pi-hole through cloudflared proxy-dns","text":"a few months ago I configured a thin client as my home server to replace the previous raspberry pi setup. During that migration I moved over all native services within docker containers. One of those services being a pi-hole setup to block ad serving domains on dns level and to have a dns cache within our LAN to gain a bit of speed. It has been running ever since without any issue and worked pretty well. When cloudflare announced their fast and privacy based DNS resolver I got a bit intrigued by their DNS over HTTPS feature. Especially since our ISP telenet is using our web history for their advertisements too. So I stumbled on some articles from Oliver Hough and Scott Helme that describe how you can combine a cloudflared proxy-dns with pi-hole to get your dns requests encrypted through HTTPS and still be able to filter out the advertisements. Since I got everything in docker I configured a cloudflared container automated through travis with dgoss tests. I got some inspiration from maartje who used a matrix to build multiple docker images for different architectures using travis. The main reason behind this was that after I got this setup up and running using this docker-compose file on my x86_64 machine I wanted to run it on a raspberry pi zero w. For the pihole container I figured out you can easily pass by the custom DNS servers through docker environment variables so no need anymore for a custom pihole docker container to maintain! $ cat docker-compose.yml version: \"3\" services: cloudflared: container_name: cloudflared image: visibilityspots/cloudflared:amd64 restart: unless-stopped networks: pihole_net: ipv4_address: 10 .0.0.2 pi-hole: container_name: pi-hole image: pihole/pihole:v4.2.1_amd64 restart: unless-stopped ports: - \"80:80/tcp\" - \"53:53/tcp\" - \"53:53/udp\" environment: - ServerIP = 10 .0.0.3 - DNS1 = '10.0.0.2#5054' - DNS2 = '' - IPv6 = false - TZ = CEST-2 - DNSMASQ_LISTENING = all - WEBPASSWORD = admin networks: pihole_net: ipv4_address: 10 .0.0.3 dns: - 127 .0.0.1 - 1 .1.1.1 cap_add \" - NET_ADMIN networks: pihole_net: driver: bridge ipam: config: - subnet: 10.0.0.0/29 I remembered this project where a raspberry pi zero W was used together with a tiny display. In the meanwhile I have the DoH cloudflared/pi-hole combination running on such a tiny device using ArchLinux ARM and ordered the display :D You can use the same dockerfile on a raspberry pi zero but with other tags for the container images: image : visibilityspots / cloudflared : arm image : pihole / pihole : v4 . 0 _armhf As you can see unfortunately I had to configure static ip's since the dnsmasq config needs the ip address of the cloudflared service. If someone has a better solution to implement it let me know! I also opted to not store the data. Meaning that when the docker containers are restarted the data is gone. So when you now bring up those 2 containers: $ docker-compose up -d Creating network \"###_pihole_net\" with driver \"bridge\" Creating pi-hole ... Creating cloudflared ... Creating pi-hole Creating cloudflared ... done $ docker-compose logs cloudflared Attaching to cloudflared cloudflared | time = \"2018-04-16T20:01:14Z\" level = info msg = \"Adding DNS upstream\" url = \"https://1.1.1.1/.well-known/dns-query\" cloudflared | time = \"2018-04-16T20:01:14Z\" level = info msg = \"Adding DNS upstream\" url = \"https://1.0.0.1/.well-known/dns-query\" cloudflared | time = \"2018-04-16T20:01:14Z\" level = info msg = \"Starting DNS over HTTPS proxy server\" addr = \"dns://0.0.0.0:5054\" cloudflared | time = \"2018-04-16T20:01:14Z\" level = info msg = \"Starting metrics server\" addr = \"127.0.0.1:35973\" $ docker - compose logs pi - hole Attaching to pi - hole ... pi - hole | [ services.d ] starting services pi - hole | Starting lighttpd pi - hole | Starting dnsmasq pi - hole | Starting crond pi - hole | Starting pihole - FTL ( no - daemon ) pi - hole | [ services.d ] done . pi - hole | dnsmasq : started , version 2.76 cachesize 10000 pi - hole | dnsmasq : compile time options : IPv6 GNU - getopt DBus i18n IDN DHCP DHCPv6 no - Lua TFTP conntrack ipset auth DNSSEC loop - detect inotify pi - hole | dnsmasq : using nameserver 10.0.0.2 #5054 pi - hole | dnsmasq : read / etc / hosts - 7 addresses pi - hole | dnsmasq : read / etc / pihole / local . list - 2 addresses pi - hole | dnsmasq : failed to load names from / etc / pihole / black . list : No such file or directory pi - hole | dnsmasq : read / etc / pihole / gravity . list - 121065 addresses pi - hole | dnsmasq : 1 127.0.0.1 / 48521 query [ A ] pi . hole from 127.0.0.1 pi - hole | dnsmasq : 1 127.0.0.1 / 48521 / etc / pihole / local . list pi . hole is 10.0.0.3 you should be able to query the containerized pi-hole DNS service from it's host or from within your netwerk using dig: $ dig @localhost -p 53 visibilityspots.org ( $ dig @IP-ADDRESS-OF-DOCKER-NODE -p 53 visibilityspots.org ) ; <<>> DiG 9 .12.1 <<>> @localhost -p 53 visibilityspots.org ; ( 2 servers found ) ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 51155 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 8 , AUTHORITY: 0 , ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 1536 ;; QUESTION SECTION: ; visibilityspots.org. IN A ;; ANSWER SECTION: visibilityspots.org. 37 IN A 54 .230.9.72 visibilityspots.org. 37 IN A 54 .230.9.109 visibilityspots.org. 37 IN A 54 .230.9.119 visibilityspots.org. 37 IN A 54 .230.9.143 visibilityspots.org. 37 IN A 54 .230.9.148 visibilityspots.org. 37 IN A 54 .230.9.182 visibilityspots.org. 37 IN A 54 .230.9.188 visibilityspots.org. 37 IN A 54 .230.9.203 ;; Query time: 223 msec ;; SERVER: ::1#53 ( ::1 ) ;; WHEN: Mon Apr 16 22 :05:37 CEST 2018 ;; MSG SIZE rcvd: 328 Obviously I wanted to see myself that when sniffing the network the DNS requests aren't readable so I used tcp dump to prove myself the data was sent through HTTPS pi-hole# tcpdump -i eth0 udp port 53 22:39:30.837594 IP 192.168.0.3.35765 > piholeContainerID.domain: 36972+ [1au] A? visibilityspots.com. (60) 22:39:31.009345 IP piholeContainerID.domain > 192.168.0.3.35765: 36972 8/0/1 A 54.230.228.38, A 54.230.228.42, A 54.230.228.54, A 54.230.228.68, A 54.230.228.69, A 54.230.228.84, A 54.230.228.92, A 54.230.228.104 (328) cloudflared # tcpdump - i eth0 udp port 5054 tcpdump : verbose output suppressed , use - v or - vv for full protocol decode listening on eth0 , link - type EN10MB ( Ethernet ) , capture size 262144 bytes 20 : 39 : 29 . 029132 IP piholeContainerID . 59189 > cloudflaredContainerID . 5054 : UDP , length 40 20 : 39 : 29 . 069864 IP cloudflaredContainerID . 5054 > piholeContainerID . 59189 : UDP , length 116 20 : 39 : 30 . 838803 IP piholeContainerID . 28892 > cloudflaredContainerID . 5054 : UDP , length 60 20 : 39 : 31 . 003756 IP cloudflaredContainerID . 5054 > piholeContainerID . 28892 : UDP , length 328 20 : 39 : 31 . 352487 IP piholeContainerID . 50291 > cloudflaredContainerID . 5054 : UDP , length 31 20 : 39 : 31 . 364073 IP piholeContainerID . 16365 > cloudflaredContainerID . 5054 : UDP , length 31 20 : 39 : 31 . 411227 IP cloudflaredContainerID . 5054 > piholeContainerID . 50291 : UDP , length 156 20 : 39 : 31 . 432364 IP cloudflaredContainerID . 5054 > piholeContainerID . 16365 : UDP , length 218 So by now you can configure this new DNS service on your router or dhcp daemon within your local network. Since the pi isn't running for a very long time I have no clue if it can cope with the load on our network but I'll keep you posted ;)","tags":"containers","url":"https://visibilityspots.github.io/blog/dockerized-cloudflared-pi-hole.html","loc":"https://visibilityspots.github.io/blog/dockerized-cloudflared-pi-hole.html"},{"title":"NRPE troubleshooting","text":"When refactoring a check_memory I wrote a few years ago I bumped into the feared NRPE : Unable to read output error message on our nagios instance. When looking for a solution I went through most possible debug steps I could think of and which are nicely described by nagios support but didn't found any solution. I almost grabbed to some anti depressants when I thought of the thing I always forget about. SELINUX When crawling through the audit log it became clear I forgot to configure the proper selinux context type for the new script. type = PATH msg = audit ( 1518702310 . 763 : 296695 ) : item = 0 name = \" /usr/lib64/nagios/plugins/check_mem \" inode = 1126947 dev = fd : 00 mode = 0100755 ouid = 0 ogid = 0 rdev = 00 : 00 obj = unconfined_u : object_r : admin_home_t : s0 objtype = NORMAL type = PROCTITLE msg = audit ( 1518702310 . 763 : 296695 ) : proctitle = 7368002 D63002F7573722F6C696236342F6E6167696F732F706C7567696E732F636865636B5F6D656D2E7368 type = AVC msg = audit ( 1518702310 . 763 : 296696 ) : avc : denied { getattr } for pid = 21213 comm = \" sh \" path = \" /usr/lib64/nagios/plugins/check_mem \" dev = \" dm-0 \" ino = 1126947 scontext = system_u : system_r : nrpe_t : s0 tcontext = unconfined_u : object_r : admin_home_t : s0 tclass = file type = SYSCALL msg = audit ( 1518702310 . 763 : 296696 ) : arch = c000003e syscall = 4 success = no exit =- 13 a0 = 268 ea10 a1 = 7 ffc5f708730 a2 = 7 ffc5f708730 a3 = 7 ffc5f708260 items = 1 ppid = 21212 pid = 21213 auid = 4294967295 uid = 997 gid = 994 euid = 997 suid = 997 fsuid = 997 egid = 994 sgid = 994 fsgid = 994 tty = ( none ) ses = 4294967295 comm = \" sh \" exe = \" /usr/bin/bash \" subj = system_u : system_r : nrpe_t : s0 key = ( null ) By refreshing my memory about selinux again following this gentoo tutorial I could quickly fix the issue by configuring the proper context type; # semanage fcontext -a --type nagios_unconfined_plugin_exec_t /usr/lib64/nagios/plugins/check_mem # restorecon /usr/lib64/nagios/plugins/check_mem and running the check_nrpe tool from the nagios instance finally worked again as I expected it to be: # /usr/lib64/nagios/plugins/check_nrpe -H host -c check_mem OK: 63% of memory used, 48% is available","tags":"linux","url":"https://visibilityspots.github.io/blog/nrpe-troubleshooting.html","loc":"https://visibilityspots.github.io/blog/nrpe-troubleshooting.html"},{"title":"Test ansible playbooks with docker","text":"recently I started working at a new project where the infra is maintained by ansible. When been asked to write some functionality in a playbook I missed my vagrant puppet setup where I could easily test my puppet code on my local machine. Due to my previous project I felt like maybe I could use docker for this purpose on the ansible part. So I looked a bit around and stumbled on the docker-ansible github repository of William Yeh. He already did a great job by creating a docker container with ansible preinstalled for a lot of linux distributions. I only figured the way he describes wasn't really what I am looking for. By using docker build the test of the playbook creates a docker image every time. I was looking for a solution a docker container will be brought up, the ansible playbook being tested, showing those results and bringing the container back down without too much hassle to set this environment up and running. So I digged around a bit further in his images and came up with the following command $ docker run -ti --rm -v \" $( pwd ) \" :/tmp --workdir = \"/tmp\" williamyeh/ansible:centos7-onbuild ansible-playbook-wrapper This will mount the current directory where a file named playbook.yml is placed to the /tmp directory of the docker container. By changing the container's workdir to this /tmp directory the ansible-playbook-wrapper he wrote will be executed using the mounted playbook.yml and spawning the results. $ docker run - ti -- rm - v \"$(pwd)\" :/ tmp -- workdir = \"/tmp\" williamyeh / ansible : centos7 - onbuild ansible - playbook - wrapper PLAY [ Remove upstream repositories when being an offline instance ] ************************************************************************************************************************************************************************************************************** TASK [ Gathering Facts ] ********************************************************************************************************************************************************************************************************************************************************** ok : [ localhost ] TASK [ Check if system is supposed to be offline ] ********************************************************************************************************************************************************************************************************************************* ok : [ localhost ] TASK [ find ] ********************************************************************************************************************************************************************************************************************************************************************* ok : [ localhost ] TASK [ file ] ********************************************************************************************************************************************************************************************************************************************************************* changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 560 , u ' isgid ' : False , u ' size ' : 1664 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Base . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 60 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 561 , u ' isgid ' : False , u ' size ' : 1309 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - CR . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 60 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 562 , u ' isgid ' : False , u ' size ' : 649 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Debuginfo . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 60 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 563 , u ' isgid ' : False , u ' size ' : 630 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Media . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 60 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 564 , u ' isgid ' : False , u ' size ' : 1331 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Sources . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 60 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 565 , u ' isgid ' : False , u ' size ' : 3830 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Vault . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 60 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) skipping : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 566 , u ' isgid ' : False , u ' size ' : 314 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - fasttrack . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 60 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) PLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************** localhost : ok = 4 changed = 1 unreachable = 0 failed = 0 the playbook will check for a certain file and depending on that file's existence it will remove all repositories except from a predefined list. --- - name : Remove upstream repositories when being an offline instance hosts : localhost become_user : root vars : repos_to_keep : - / etc / yum . repos . d / CentOS - fasttrack . repo tasks : - name : Check if system is supposed to be offline stat : path : / opt / offline register : offline - find : paths : / etc / yum . repos . d / patterns : \"*.repo\" register : repos - debug : var : repos . files - file : path : \"{{ item.path }}\" state : absent with_items : \"{{ repos.files }}\" when : - item . path not in repos_to_keep - offline . stat . exists during development you want to keep the container online to troubleshoot. This can be done by running the container and instead of executing the ansible-playbook-wrapper just launching bash. $ docker run - ti -- rm - v \"$(pwd)\" :/ tmp -- workdir = \"/tmp\" williamyeh / ansible : centos7 - onbuild / bin / bash [ root @5 cd5cfe2d7cf tmp ] # ls / etc / yum . repos . d / CentOS - Base . repo CentOS - CR . repo CentOS - Debuginfo . repo CentOS - fasttrack . repo CentOS - Media . repo CentOS - Sources . repo CentOS - Vault . repo [ root @5 cd5cfe2d7cf tmp ] # ansible - playbook - wrapper PLAY [ Remove upstream repositories when being an offline instance ] ************************************************************************************************************************************************************************************************************** TASK [ Gathering Facts ] ********************************************************************************************************************************************************************************************************************************************************** ok : [ localhost ] TASK [ Check if system is supposed to be offline ] ********************************************************************************************************************************************************************************************************************************* ok : [ localhost ] TASK [ find ] ********************************************************************************************************************************************************************************************************************************************************************* ok : [ localhost ] TASK [ file ] ********************************************************************************************************************************************************************************************************************************************************************* changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 560 , u ' isgid ' : False , u ' size ' : 1664 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Base . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 59 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 561 , u ' isgid ' : False , u ' size ' : 1309 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - CR . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 59 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 562 , u ' isgid ' : False , u ' size ' : 649 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Debuginfo . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 59 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 563 , u ' isgid ' : False , u ' size ' : 630 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Media . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 59 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 564 , u ' isgid ' : False , u ' size ' : 1331 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Sources . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 59 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) changed : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 565 , u ' isgid ' : False , u ' size ' : 3830 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - Vault . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 59 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) skipping : [ localhost ] => ( item = { u ' uid ' : 0 , u ' woth ' : False , u ' mtime ' : 1504108387.0 , u ' inode ' : 566 , u ' isgid ' : False , u ' size ' : 314 , u ' roth ' : True , u ' isuid ' : False , u ' isreg ' : True , u ' gid ' : 0 , u ' ischr ' : False , u ' wusr ' : True , u ' xoth ' : False , u ' rusr ' : True , u ' nlink ' : 1 , u ' issock ' : False , u ' rgrp ' : True , u ' path ' : u ' / etc / yum . repos . d / CentOS - fasttrack . repo ' , u ' xusr ' : False , u ' atime ' : 1504108387.0 , u ' isdir ' : False , u ' ctime ' : 1510218719.8578098 , u ' wgrp ' : False , u ' xgrp ' : False , u ' dev ' : 59 , u ' isblk ' : False , u ' isfifo ' : False , u ' mode ' : u ' 0644 ' , u ' islnk ' : False }) PLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************** localhost : ok = 4 changed = 1 unreachable = 0 failed = 0 [ root @5 cd5cfe2d7cf tmp ] # ls / etc / yum . repos . d / CentOS - fasttrack . repo [ root @5 cd5cfe2d7cf tmp ] # exit exit Since the command is rather long to memorize I created an alias for it in my zsh config (~/.zshrc) alias ansible-playbook-test='docker run -ti --rm -v \"$(pwd)\":/tmp --workdir=\"/tmp\" williamyeh/ansible:centos7-onbuild ansible-playbook-wrapper'","tags":"containers","url":"https://visibilityspots.github.io/blog/test-ansible-playbooks.html","loc":"https://visibilityspots.github.io/blog/test-ansible-playbooks.html"},{"title":"Jenkins docker-pipeline","text":"in a previous blog post I talked about setting up a dockerized jenkins master/slave setup and setting up a private docker registry using nexus . The next thing on the roadmap was to use this jenkins setup to actually build new docker images for specific software. Before going to the different teams and talking how they now build their software and how this could be done using this new containerized setup I setted up a new jenkins job. This jenkins job will build a generic jenkins slave docker container which will be used by the jenkins master to build some generic jobs. to be able to build docker images through jenkins there is the docker-pipeline plugin which can be used by seeding a repository with the Dockerfile and a Jenkinsfile as described by this tutorial . To get it configured I had to install the pipeline plugin, configure an SSH key into jenkins and github so jenkins was able to pull the repository together with the docker registry credentials from the private nexus which will be used in the jenkinsfile. To use the docker-plugin docker needs to be installed on the jenkinsmaster. We already covered that part in the dockerized jenkins master/slave post. Also the jenkins user needs to be added to the docker group too so it could try to communicate with the docker socket. Which is a weird combination because it's using the docker daemon of the docker node since the socket has been mounted on the jenkins master container. Because we mount the docker socket from the host to the docker daemon the GID's of the docker group on both host and container need to match to each other. This is explained on the github page which resides the config files for the jenkins-docker image. configuration GID The container docker GID is already configured statically to 900 so by changing the one on the host they match and no permission issues should arise concerning this topic. credentials jenkins To enable jenkins to read from github a jenkins public SSH key need to be added to repository and the private SSH key needs to be configured in jenkins Next to the repository credentials we also need to configure credentials of the docker repository hosted on our nexus instance and refer to the ID in the jenkinsfile so the pipeline plugin can act accordingly. Jenkinsfile The steps to be executed are defined in a Jenkinsfile this file can be added in a repository next to the Dockerfile and will perform some stages; clone repository perform a git checkout to the latest update trim the first 6 chars of the last commit to use as a tag version for the docker image to be builded build image build the image based on the Dockerfile test image use dgoss to perform a test based on the goss.yaml file push image push the created and tested image to the registry with the latest tag and with the short commit hash node ( 'generic' ) { def container ansiColor ( 'xterm' ) { stage ( 'Clone repository' ) { checkout scm shortCommit = sh ( returnStdout : true , script : 'git rev-parse --short HEAD' ) . trim () } stage ( 'Build image' ) { container = docker . build ( 'visibilityspots/jenkins-docker' ) } stage ( 'Test image' ) { sh 'export GOSS_FILES_STRATEGY=cp && /usr/local/bin/dgoss run --name jenkins-docker-dgoss-test --rm -ti visibilityspots/jenkins-docker' } stage ( 'Push image' ) { docker . withRegistry ( 'https://nexus.repository' , 'nexus-credentials-id' ) { container . push ( \"${shortCommit}\" ) container . push ( 'latest' ) } } } } testing the docker image there is this tool called goss which uses a simple yaml based approach to perform some tests against a server the same way serverspe wcorks. But there is a great wrapper script created called dgoss which uses the same yaml file to perform the tests against a docker container it spins up especially for the testing case. When one of the defined tests fails after the image has been build using the Dockerfile, the jenkins pipeline will abort and will not push the image to the registry. To be able to run the goss test suite the software is also preinstalled on our jenkins-slave docker images. [ jenkins-docker ] Running shell script export GOSS_FILES_STRATEGY = cp / usr / local / bin / dgoss run -- name jenkins - docker - dgoss - test -- rm - ti visibilityspots / jenkins - docker INFO : Starting docker container INFO : Container ID : e0849245 INFO : Sleeping for 0.2 INFO : Running Tests User : jenkins : exists : matches expectation : [ true ] User : jenkins : groups : matches expectation : [ [\"jenkins\",\"docker\" ] ] Group : docker : exists : matches expectation : [ true ] Group : docker : gid : matches expectation : [ 900 ] Package : docker - ce : installed : matches expectation : [ true ] Package : file : installed : matches expectation : [ true ] Command : / usr / bin / file / etc / localtime : exit - status : matches expectation : [ 0 ] Command : / usr / bin / file / etc / localtime : stdout : matches expectation : [ /etc/localtime: symbolic link to /usr/share/zoneinfo/Etc/UTC ] Total Duration : 0.076 s Count : 8 , Failed : 0 , Skipped : 0 INFO : Deleting container jenkins job Next up is the configuration of a pipeline defined jenkins job where the only config is the git repository and a pointer to the Jenkinsfile. If everything went well jenkins will execute the steps defined in the Jenkinsfile and store the docker image as a result in the nexus docker repository. configuration Name Value Poll SCM /1 * * * references https://getintodevops.com/blog/building-your-first-docker-image-with-jenkins-2-guide-for-developers","tags":"containers","url":"https://visibilityspots.github.io/blog/jenkins-docker-pipeline.html","loc":"https://visibilityspots.github.io/blog/jenkins-docker-pipeline.html"},{"title":"Jenkins gradle build succeeded with failure","text":"Today we bumped into an interesting issue in the jenkins builds of some android based applications. The gradle commands succeeded but then suddenly failed the build with this most cryptic message ever: BUILD SUCCESSFUL Total time : 1 mins 20.492 secs FAILURE : Build failed with an exception . * What went wrong : Already finished * Try : Run with -- stacktrace option to get the stack trace . Run with -- info or -- debug option to get more log output . [ Pipeline ] } Since this came out of nowhere without any modification on the build servers we where flabbergasted since the builds ran fine on our local machines. So we took the suggestion of the stacktrace option and ran the build again: 14 : 00 : 31.371 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] FAILURE : Build failed with an exception . 14 : 00 : 31.372 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] 14 : 00 : 31.372 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] * What went wrong : 14 : 00 : 31.372 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] Already finished 14 : 00 : 31.372 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] 14 : 00 : 31.372 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] * Exception is : 14 : 00 : 31.373 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] java . lang . IllegalStateException : Already finished 14 : 00 : 31.373 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at com . google . common . base . Preconditions . checkState ( Preconditions . java : 174 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at com . android . builder . profile . ProcessProfileWriter . finishAndMaybeWrite ( ProcessProfileWriter . java : 121 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at com . android . builder . profile . ProcessProfileWriterFactory . shutdownAndMaybeWrite ( ProcessProfileWriterFactory . java : 52 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at com . android . build . gradle . internal . profile . ProfilerInitializer $ ProfileShutdownListener . completed ( ProfilerInitializer . java : 110 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at sun . reflect . GeneratedMethodAccessor660 . invoke ( Unknown Source ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at sun . reflect . DelegatingMethodAccessorImpl . invoke ( DelegatingMethodAccessorImpl . java : 43 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at java . lang . reflect . Method . invoke ( Method . java : 498 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . dispatch . ReflectionDispatch . dispatch ( ReflectionDispatch . java : 35 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . dispatch . ReflectionDispatch . dispatch ( ReflectionDispatch . java : 24 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . event . DefaultListenerManager $ ListenerDetails . dispatch ( DefaultListenerManager . java : 249 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . event . DefaultListenerManager $ ListenerDetails . dispatch ( DefaultListenerManager . java : 229 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . event . AbstractBroadcastDispatch . dispatch ( AbstractBroadcastDispatch . java : 44 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . event . DefaultListenerManager $ EventBroadcast $ ListenerDispatch . dispatch ( DefaultListenerManager . java : 221 ) 14 : 00 : 31.374 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . event . DefaultListenerManager $ EventBroadcast $ ListenerDispatch . dispatch ( DefaultListenerManager . java : 209 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . dispatch . ProxyDispatchAdapter $ DispatchingInvocationHandler . invoke ( ProxyDispatchAdapter . java : 93 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at com . sun . proxy . $ Proxy17 . completed ( Unknown Source ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . initialization . DefaultGradleLauncher . stop ( DefaultGradleLauncher . java : 226 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . exec . InProcessBuildActionExecuter . execute ( InProcessBuildActionExecuter . java : 44 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . exec . InProcessBuildActionExecuter . execute ( InProcessBuildActionExecuter . java : 26 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . tooling . internal . provider . ContinuousBuildActionExecuter . execute ( ContinuousBuildActionExecuter . java : 75 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . tooling . internal . provider . ContinuousBuildActionExecuter . execute ( ContinuousBuildActionExecuter . java : 49 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . tooling . internal . provider . ServicesSetupBuildActionExecuter . execute ( ServicesSetupBuildActionExecuter . java : 49 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . tooling . internal . provider . ServicesSetupBuildActionExecuter . execute ( ServicesSetupBuildActionExecuter . java : 31 ) 14 : 00 : 31.375 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . ExecuteBuild . doBuild ( ExecuteBuild . java : 67 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . BuildCommandOnly . execute ( BuildCommandOnly . java : 36 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . api . DaemonCommandExecution . proceed ( DaemonCommandExecution . java : 120 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . WatchForDisconnection . execute ( WatchForDisconnection . java : 47 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . api . DaemonCommandExecution . proceed ( DaemonCommandExecution . java : 120 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . ResetDeprecationLogger . execute ( ResetDeprecationLogger . java : 26 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . api . DaemonCommandExecution . proceed ( DaemonCommandExecution . java : 120 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . RequestStopIfSingleUsedDaemon . execute ( RequestStopIfSingleUsedDaemon . java : 34 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . api . DaemonCommandExecution . proceed ( DaemonCommandExecution . java : 120 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . ForwardClientInput $ 2. call ( ForwardClientInput . java : 74 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . ForwardClientInput $ 2. call ( ForwardClientInput . java : 72 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . util . Swapper . swap ( Swapper . java : 38 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . ForwardClientInput . execute ( ForwardClientInput . java : 72 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . api . DaemonCommandExecution . proceed ( DaemonCommandExecution . java : 120 ) 14 : 00 : 31.376 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . LogAndCheckHealth . execute ( LogAndCheckHealth . java : 55 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . api . DaemonCommandExecution . proceed ( DaemonCommandExecution . java : 120 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . LogToClient . doBuild ( LogToClient . java : 60 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . BuildCommandOnly . execute ( BuildCommandOnly . java : 36 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . api . DaemonCommandExecution . proceed ( DaemonCommandExecution . java : 120 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . EstablishBuildEnvironment . doBuild ( EstablishBuildEnvironment . java : 72 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . BuildCommandOnly . execute ( BuildCommandOnly . java : 36 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . api . DaemonCommandExecution . proceed ( DaemonCommandExecution . java : 120 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . exec . StartBuildOrRespondWithBusy $ 1. run ( StartBuildOrRespondWithBusy . java : 50 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . launcher . daemon . server . DaemonStateCoordinator $ 1. run ( DaemonStateCoordinator . java : 297 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . concurrent . ExecutorPolicy $ CatchAndRecordFailures . onExecute ( ExecutorPolicy . java : 63 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at org . gradle . internal . concurrent . StoppableExecutorImpl $ 1. run ( StoppableExecutorImpl . java : 46 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at java . util . concurrent . ThreadPoolExecutor . runWorker ( ThreadPoolExecutor . java : 1142 ) 14 : 00 : 31.377 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at java . util . concurrent . ThreadPoolExecutor $ Worker . run ( ThreadPoolExecutor . java : 617 ) 14 : 00 : 31.378 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] at java . lang . Thread . run ( Thread . java : 745 ) 14 : 00 : 31.378 [ ERROR ] [ org.gradle.internal.buildevents.BuildExceptionReporter ] but this didn't make us giving any more clue. So decided to log on to the build node and trying to remove the gradle caches on suggestion of a colleague: $ rm /home/jenkins/.gradle/caches/* -rf but no solution neither. Checked out the repository to commit of the latest successful build and tried to run the command from there manually but still no success. After reading many other gradle issues I came to one indicating killing the gradle daemons could solve the mystery. So we went ahead and killed the daemons on the build node. # ps aux | grep gradle jenkins 6341 33.9 7.9 14954276 2625892 ? Ssl 18 : 11 58 : 59 / usr / lib / jvm / java - 8 - openjdk - amd64 / bin / java - Xmx1536m - Dfile . encoding = UTF - 8 - Duser . country = US - Duser . language = en - Duser . variant - cp / home / jenkins /. gradle / wrapper / dists / gradle - 3.5 - all / exhrs6ca08n232b14ue48lbye / gradle - 3.5 / lib / gradle - launcher - 3.5 . jar org . gradle . launcher . daemon . bootstrap . GradleDaemon 3.5 jenkins 10631 19.3 6.5 14449300 2164472 ? Ssl 18 : 48 26 : 25 / usr / lib / jvm / java - 8 - openjdk - amd64 / bin / java - Xmx1536m - Dfile . encoding = UTF - 8 - Duser . country = US - Duser . language = en - Duser . variant - cp / home / jenkins /. gradle / wrapper / dists / gradle - 3.4 . 1 - all / c3ib5obfnqr0no9szq6qc17do / gradle - 3.4 . 1 / lib / gradle - launcher - 3.4 . 1.j ar org . gradle . launcher . daemon . bootstrap . GradleDaemon 3.4 . 1 kill - 9 6341 10631 And reran the build manually which succeeded, when re-triggering the job it succeeded. Victory! We assumed those daemons where stopped right after the execution but as we noticed this wasn't the case. I wrote this post since I couldn't find anything related to the message and maybe I can help others gaining time resolving the issue since as always such things comes on days you haven't time for them..","tags":"automation","url":"https://visibilityspots.github.io/blog/gradle-build.html","loc":"https://visibilityspots.github.io/blog/gradle-build.html"},{"title":"Nexus OSS repository manager","text":"looking for a global repository store which could store maven projects, yum repositories, docker repositories, we bumped into Nexus repository manager . We used the official docker image to see how it can be implemented in the dockerized CI environment. docker repository as a first the docker repository feature could be enabled so we can start building and storing docker images for the different jenkins build slaves and the jenkins master so our work is reproducible and stored in a safe central place. We configured 3 repositories in nexus for our docker images seen as a recommended approach in the nexus documentation . Each of them are configured to their own separate blob store. Name Purpose private self hosted docker repository where all the internal images will be stored proxy cached proxy which will download every request from docker hub and caches to reduce download but also for having an offline backup of upstream images group group which combines the first 2 repositories behind one URL configuration by following a tutorial from Ivan Krizan a working example has been configured, but this in a way without https enabled which is not the recommended manner to set it up. Therefore we looked into enabling https instead. This can be done in different ways. SSL directly configured setting it up with https doesn't seemed straightforward with nexus directly connected. Also it has a disadvantage in the docker world that the certificates needs to be present on the container and some configuration changes needs to be made. This makes upgrading the nexus container on it's own a bit of a more complex task as without the direct SSL encryption enabled. reverse proxy setup we could instead spin up an nginx reverse proxy which will handle the encrypted requests towards the client. In the back it communicates to the nexus container service through plain HTTP on a dedicated docker network only for this kind of traffic. Based on the standard setup for one private repository by Stefan Prodan I came to the described nginx configuration. It will serve the web GUI when you access the proxy using a normal browser by a redirect to the nexus container on port 8081 where the management console is living. For the docker client it will act differently, when a docker pull command is executed it will get redirected to the docker-group repository which combines both images from upstream (cached) as well as images from the private docker repository. On the other hand when issuing a docker push command the image will get pushed towards the private repository only since group and proxy repositories aren't able to do so. worker_processes 2 ; events { worker_connections 1024 ; } http { error_log / var / log / nginx / error . log warn ; access_log / dev / null ; proxy_intercept_errors off ; proxy_send_timeout 120 ; proxy_read_timeout 300 ; upstream nexus - gui { server nexus : 8081 ; } upstream docker - private { server nexus : 5000 ; } upstream docker - group { server nexus : 5001 ; } map $ request_method $ redirection { default \"nexus-gui\" ; \"~GET\" \"docker-group\" ; \"~(HEAD|PATCH|PUT|POST|DELETE)\" \"docker-private\" ; } server { listen 80 ; listen 443 ssl ; server_name nexus . domain . org ; ssl_certificate / etc / nginx / nginx . crt ; ssl_certificate_key / etc / nginx / nginx . key ; keepalive_timeout 5 5 ; proxy_buffering off ; # allow large uploads client_max_body_size 1 G ; location / { if ( $ http_user_agent ~ docker ) { proxy_pass http : //$ redirection ; } proxy_pass http : // nexus - gui ; proxy_set_header Host $ host ; proxy_set_header X - Real - IP $ remote_addr ; proxy_set_header X - Forwarded - For $ proxy_add_x_forwarded_for ; } } } Unfortunately when using a self signed key pair the client will not connect properly to the repository. You'll need to use letsencrypt or another party to get a valid certificate from; [ root@dockernode ~ ] # docker login nexus . domain . org Username : test Password : Error response from daemon : Get https : // nexus . domain . org / v2 / : x509 : certificate signed by unknown authority letsencrypt setup when looking for solutions letsencrypt could be an option in combination with an nginx-proxy container. But for the moment we placed the auto nginx-proxy on a side track since only one service is needing the proxy and it can perfectly be combined with a docker-compose setup. self signed certificates in the meanwhile to gain some time we'll stick with the self generated scripts and use the insecure-registry workaround on the docker nodes, jenkins build servers and my local client. When heading for production we'll generate certificates with letsencrypt and I'll update this post. centos 7 docker node daemon configuration [ root @ dockernode ~ ] # vim /etc/systemd/system/docker.service.d/docker.conf [ Service ] ExecStart = ExecStart =/ usr / bin / dockerd - H tcp : // 0.0 . 0.0 : 2375 - H unix : // var / run / docker . sock -- insecure - registry = nexus . domain . org [ root @ dockernode ~ ] # systemctl daemon-reload [ root @ dockernode ~ ] # systemctl restart docker.service [ root@dockernode ~ ] # docker login nexus . domain . org Username : test Password : Login Succeeded archlinux client configuration $ sudo cat /etc/docker/daemon.json { \"insecure-registries\" : [ \"nexus.domain.org\" ] } $ sudo systemctl restart docker.service $ docker login nexus.domain.org Username: test Password: Login Succeeded tips and tricks blobstore we created a separate blob store for every repository, that way on the filesystem they are easily recognized for maintenance tasks in the future. removal when removing a blob store and a repository through the GUI I noticed the actual content on the file system is still available, when recreating the repository and blob store with the same name the content is available again. When removing the content on the filesystem too you have again a clean repository to start with. future improvements and features there are some features and configuration options that will need to be implemented in the future, when I got there I will update this blog post accordingly.","tags":"containers","url":"https://visibilityspots.github.io/blog/nexus-oss-repository-manager.html","loc":"https://visibilityspots.github.io/blog/nexus-oss-repository-manager.html"},{"title":"Dockerized jenkins master/slave setup","text":"started at a new customer we were looking for a more flexible way of having jenkins spinning up slaves on the fly. This in a way a slave is only started and consuming resources when a specific job is running. That way those resources could be used more efficient. Also the fact that developers could take control over their build servers by managing the Dockerfiles themselves is a great advantage too. But that's for a later phase. Let's start at the beginning. For the docker host a CentOS 7 server has been provisioned and prepared to run the docker daemon. Starting with updating the OS, removing unnecessary services and implementing NTP. # yum upgrade -y # systemctl stop postfix # systemctl disable postfix # yum remove postfix # systemctl stop chronyd # yum remove chrony -y # reboot # yum install ntp # systemctl start ntpd # systemctl enable ntpd # ntpdate # date Once the system has been prepared we can start with the installation of the docker daemon using the upstream docker community edition repository # yum install -y yum-utils device-mapper-persistent-data lvm2 # yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # yum clean all # yum makecache fast # yum install docker-ce # systemctl start docker # systemctl enable docker # docker run hello-world Hello from Docker ! This message shows that your installation appears to be working correctly . ... # docker info we do now have a basic docker daemon running on a CentOS 7 machine. The next thing to do is to start a jenkins master docker container. Before doing so some decisions need to be made. Docker containers are stateless, meaning when the container is gone, the data is gone too. With this taken into account and assuming it's a proof of concept I decided to mount a directory on the host to the jenkins master container. Since the pipeline plugin will be used, all job configurations are residing in Jenkinsfiles. The needed plugins can be passed through the docker run command later on so that's covered as well. When the setup is ready for production we could look which directories are needed to be persistent and decide to mount only those needed. But for starters we go for a full mount of the jenkins home directory. Therefore we need to create a jenkins user on the docker host which is the owner of the local directory which will be used afterwards to mount onto the jenkins master docker container. # mkdir /opt/jenkins # chown -R 1000: /opt/jenkins before we can start using this very same docker host through our jenkins instance we need to open up the API port of our docker daemon. This can be configured in the systemd entity of the docker daemon: # mkdir -p /etc/systemd/system/docker.service.d/ # cat /etc/systemd/system/docker.service.d/docker.conf [ Service ] ExecStart = ExecStart =/ usr / bin / dockerd - H tcp : // 0.0 . 0.0 : 2375 - H unix : // var / run / docker . sock # systemctl daemon-reload # systemctl restart docker.service # systemctl status docker.service * docker . service - Docker Application Container Engine Loaded : loaded ( / usr / lib / systemd / system / docker . service ; enabled ; vendor preset : disabled ) Drop - In : / etc / systemd / system / docker . service . d - docker . conf Active : active ( running ) since Thu 2017 - 07 - 20 13 : 43 : 22 CEST ; 2 weeks 5 days ago Docs : https : // docs . docker . com Main PID : 5858 ( dockerd ) Memory : 4.2 G CGroup : / system . slice / docker . service |- 5858 / usr / bin / dockerd - H tcp : // 0.0 . 0.0 : 2375 - H unix : // var / run / docker . sock |- 5866 docker - containerd - l unix : /// var / run / docker / libcontainerd / docker - containerd . sock -- metrics - interval = 0 -- start - timeout 2 m -- state - dir / var / run / docker / libcontainerd / containerd -- shim docker - containerd - shim -- runtime docker - runc ... we now have the API listening to tcp port 2375 but the port is still filtered by our firewall. To configure the firewall add the port to the appropriate zone and interface. Also the range 32000-34000 which is used by jenkins to access the slaves through ssh is needed to be accessible: # firewall-cmd --permanent --add-port=2375/tcp # firewall-cmd --permanent --zone=trusted --change-interface=docker0 # firewall-cmd --permanent --zone=trusted --add-port=2375/tcp # firewall-cmd --permanent --zone=public --add-port=32000-34000/tcp # firewall-cmd --reload # yum install -y nmap # nmap -p 2375 ip.of.docker.host Starting Nmap 6.40 ( http : // nmap . org ) at 2017 - 08 - 09 10 : 29 CEST Nmap scan report for 10.11 . 1.17 Host is up ( 1500 s latency ) . PORT STATE SERVICE 2375 / tcp open unknown Nmap done : 1 IP address ( 1 host up ) scanned in 0.05 seconds # curl -X GET http://localhost:2375/images/json and we are off to go, a docker container can be started using the official upstream image from jenkins. They also did a great job on documentation about those docker images. We went for the lts release because this setup will be the production one in the future. # docker run -d -p 8080:8080 -v /opt/jenkins/:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock jenkinsci/jenkins:lts as you can see we passed through the docker unix socket from the host to the jenkins container by doing so we are now able to instruct actions on the docker host from within our jenkins instance. on your docker host a jenkins container is running and accessible through the docker host's ip address on port 8080 which is forwarded to the jenkins master docker container. To have access to the container himself or follow the logs: # docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d28838216442 jenkinsci / jenkins : lts \"/bin/tini -- /usr...\" 19 hours ago Up 19 hours 0.0.0.0 : 8080 -> 8080 / tcp , 50000 / tcp determined_bartik # docker exec -i -t --user root d28838216442 /bin/bash # docker logs -f d28838216442 the plugin needed to communicate with our docker host is the docker plugin following the configuration as described in their documentation we created a docker cloud which will be used to execute builds with a specified label only. This is very handy because we can now create different docker images for every piece of software with other dependencies. For the initial testing setup we used the upstream docker image evarga/jenkins-slave . docker cloud configuration Manage Jenkins -> Configure System: Cloud - Add new cloud: Docker Cloud Name Value Name testdocker Docker URL tcp://ip.address:2375 Connection Timeout 15 Read Timeout 5 Container Cap 100 -> Add Docker Template Name Value Docker Image evarga/jenkins-slave:latest (the tag can be pinpointed for production) Container Settings Volumes /var/run/docker.sock:/var/run/docker.sock Remote Filing System Root /home/jenkins Labels generic Usage Only build jobs with label expressions matching this node Launch method Docker SSH computer launcher Credentials jenkins (A dedicated SSH key pair for jenkins use cases) Host Key Verification Strategy Manually provided key Verification Strategy SSH-KEY the private SSH key for the jenkins slave user Remote FS Root Mapping /home/jenkins Remove volumes true Pull strategy Pull once and update latest Depending on the label specific docker images will be used to execute the job and store the result in a repository. Along the way I struggled with setting up the docker cloud. In the first place because we are abusing the same docker host as our jenkins container is running on. By passing through the docker socket and opening up a range of ports used by jenkins to SSH into the slaves I finally achieved a running job which spawns a docker container and destroys him as soon as the job is done. custom jenkins master image to get the docker plugin working nicely the docker daemon needs to be installed on the jenkins instance too. Therefor I created a new jenkins-docker docker image which is based on the upstream jenkins:lts image but adds the docker daemon and staticly configure the docker GID to prevent mismatches between the docker socket mounted on the container from the host. to have the daemon mounted from the host in the jenkins containers, both master as slave should be able to execute docker commands over the socket the docker group need to have the same GID on all containers and 'physical' machines. Since we statically changed this GID already on the containers we also need to map this GID on the physical machine: # groupmod -g 900 docker # systemctl restart docker.service The container is started with some extra parameters like the java_opts one for the allocation of RAM memory for the jenkins daemon. # docker run -d -p 8080:8080 -v /opt/jenkins/:/var/jenkins_home -e JAVA_OPTS=\"-Xmx6144m\" -v /var/run/docker.sock:/var/run/docker.sock visibilityspots/jenkins-docker:latest","tags":"containers","url":"https://visibilityspots.github.io/blog/dockerized-jenkins.html","loc":"https://visibilityspots.github.io/blog/dockerized-jenkins.html"},{"title":"Docker openstackclient-kilo container","text":"A couple of years ago I deployed an openstack cluster based on RDO . Back in the days we implemented the kilo release. Until today we didn't updated yet due to various reasons being no need for the new features, no resources, no time no.. Upgrading would be a better option but we'll have to live with it and since it's running rather well so far we are quite happy with it. To manage that cloud I use the clients I installed on my local machine, from nova to cinder they all have different packages available for many different platforms. Only a couple of weeks ago I noticed the new shiny versions shipping with Archlinux aren't working anymore with our setup. So I looked for alternatives, in the short term I logged in on one of the compute nodes or our jenkins machine to perform the actions I needed to do. But that's bad practice. So I went a bit further and decided to create a docker container for this. Looking on the docker hub there are already some containers available but they don't specify a specific version. Also this seemed to me like a great exercise to experience the different stages how a docker container could be built up completely by myself. So I went for it and created a github repository to share my work with the world. docker build The whole setup relies on one Dockerfile . Since I rely on centos in almost every server environment I'm working on I decided to use the centos:7 official docker image. After some try and error I came to this setup: FROM centos:7 RUN set -x \\ && yum upgrade -y \\ && yum install -y bash-completion \\ && yum install -y https://repos.fedorapeople.org/repos/openstack/EOL/openstack-kilo/rdo-release-kilo-2.noarch.rpm \\ && yum install -y python-novaclient \\ && yum install -y python-ceilometerclient \\ && yum install -y python-cinderclient \\ && yum install -y python-glanceclient \\ && yum install -y python-heatclient \\ && yum install -y python-ironicclient \\ && yum install -y python-keystoneclient \\ && yum install -y python-manilaclient \\ && yum install -y python-novaclient \\ && yum install -y python-openstackclient \\ && yum install -y python-saharaclient \\ && yum install -y python-troveclient \\ && yum install -y python-zaqarclient \\ && yum clean all \\ && useradd client USER client ENTRYPOINT [\"bash\", \"--rcfile\", \"/home/client/.keystonerc\"] as you could see I opted to install every package using a separate yum install command. That way when a package can't be installed the others aren't infected. Also a yum clean all is performed to cleanup a bit the filesystem to keep the image size a bit under control. By adding a 'client' user I could prevent doing everything as root user which isn't really necessary in our case. To use the openstack clients, openstack provides a keystonerc file you'll have to source before you could connect through the different API endpoints. I went for the ENTRYPOINT solution which will source a file which can be mounted at run time of the docker container afterwards. I have to credit my colleague roidelapluie here since he guided me to this Dockerfile. To be honest about every option :), but hey I learned a lot about the docker world this way! So right now everyone should be able to build this container himself by checking out the repository and run the docker build command: $ git clone git@github.com:visibilityspots/dockerfile-openstackclient-kilo.git $ docker build -t openstackclient-kilo . If everything went well the new created docker image should be available on your machine: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE openstackclient-kilo latest e89297f75fa8 About a minute ago 331MB docker run So you could now use the image to spin up an openstack kilo client container: $ docker run --name openstack-client --rm -ti visibilityspots/openstackclient-kilo bash-4.2$ As already described before the docker container will automatically source a keystonerc file at the /home/client/.keystonerc path. So you could mount a file from your local machine into the docker container at runtime: $ docker run --name openstack-client --rm -ti -v $( pwd ) /keystonerc_admin:/home/client/.keystonerc visibilityspots/openstackclient-kilo [ client@a1c38f7635e3 / ( keystone_admin )] $ openstack hypervisor list +----+---------------------+ | ID | Hypervisor Hostname | +----+---------------------+ | 1 | node-01 | | 3 | node-02 | | 4 | node-03 | +----+---------------------+ dgoss (testing) Some weeks ago I discovered the docker image testing tool dgoss which is comparable with serverspec to write tests in an easy yaml format to be performed against your docker image. So I decided to give it a try and wrote a goss.yaml file; package : rdo - release - kilo - 2 : installed : true python - novaclient : installed : true python - ceilometerclient : installed : true python - cinderclient : installed : true python - glanceclient : installed : true python - heatclient : installed : true python - ironicclient : installed : true python - keystoneclient : installed : true python - manilaclient : installed : true python - novaclient : installed : true python - openstackclient : installed : true python - saharaclient : installed : true python - troveclient : installed : true python - zaqarclient : installed : true file : /home/client/ : exists : true command : openstack -- version : exit - status : 0 which will check if all packages are installed correctly and if the commands are working as they should; $ dgoss run -- name openstack - client -- rm - ti - v $ ( pwd ) / keystonerc_admin : / home / client / . keystonerc visibilityspots / openstackclient - kilo INFO : Starting docker container INFO : Container ID : 539 f6896 INFO : Sleeping for 0.2 INFO : Running Tests File : / home / client / . keystonerc : exists : matches expectation : [ true ] Package : python - cinderclient : installed : matches expectation : [ true ] Package : python - ironicclient : installed : matches expectation : [ true ] Package : python - manilaclient : installed : matches expectation : [ true ] Package : rdo - release - kilo - 2 : installed : matches expectation : [ true ] Package : python - novaclient : installed : matches expectation : [ true ] Package : python - saharaclient : installed : matches expectation : [ true ] Package : python - zaqarclient : installed : matches expectation : [ true ] Package : python - glanceclient : installed : matches expectation : [ true ] Package : python - troveclient : installed : matches expectation : [ true ] Package : python - heatclient : installed : matches expectation : [ true ] Package : python - keystoneclient : installed : matches expectation : [ true ] Package : python - ceilometerclient : installed : matches expectation : [ true ] Package : python - openstackclient : installed : matches expectation : [ true ] Command : openstack -- version : exit - status : matches expectation : [ 0 ] Total Duration : 0.786 s Count : 15 , Failed : 0 , Skipped : 0 INFO : Deleting container docker hub So now we have a working container on our local machine, but since sharing is caring I wanted to upload it to the docker hub . They have a feature to build your docker container based on a github repository. They also have documented the whole setup of automated builds pretty good. So for every git push a new docker image will be released on the docker hub. But they don't test the image before releasing it. travis So I looked around and found out this can be achieved by travis . I started writing a travis.yml file which will perform following steps; sudo : required services : - docker before_install : - curl - L https :// goss . rocks / install | sudo sh - docker build - t visibilityspots / openstackclient - kilo . script : - dgoss run -- name openstack - client - ti visibilityspots / openstackclient - kilo after_success : - if [ \"$TRAVIS_BRANCH\" == \"master\" ]; then docker login - u = visibilityspots - p = \"$DOCKER_PASSWORD\" ; docker push visibilityspots / openstackclient - kilo ; fi env : global : secure : QA24IWGelQ6nd3Xuwzif8L + 2 AeWC6M8V1jkZNJzJxaBLcxpucWcjkmowzaWGPmVfKeFAgfPsgVPfy8gK03JfY3J4H2AcCa8TWhVOoLvvY6ytFLobCWtRI8nIZoYeV8 /BNlKzguoE4OEfrloJUU52tQ/NDWVleH5KCBCj3H93eE52USwZ4JUyoZtngd+Zqma0GUomMvOZ0mg2e87UOYQNnCZehh5okAIU34sKGTGwEwWeqxA9xUvBd3l7pRj+5bfeQ07Fn0n3/tmrzFkOKRfCL2HC63Aq0T0LFjKsYza2QykiI5z4enNVjH8d+/05dCCHaj+/ZqKVQWbMi/RIheXk1XvsxPttBlHC03EXdQBfZmiNUUaxtVQ7f9Df0sRPvIrZsYzbHkeqVSHTNIGZLzY2cizLzIedYLOFGKNRM3WsOokIlsn+f/XciZse0D3YPBPzkRlI6sXMLtduxkZLzC1tRgyZhTl1A1kMbXzj94SJzftQ2NW7g0lbSO38DxsZcy4g/VlTDLmDzHvnlcdiU2KPSXgQcPZXQk/AZuZdl/AaYb9FIhP06GYRVRJfxls5qlxIAxxJsBTyBR4UE7SGq86UtZdTqzr/ LjQXQr8SU + KfxvKbOvfBiIrqzMcBaRqQgPfTeKhI + jEhNyAlgnvSQNf8Eg9UgHv6aW51TqyI + RzVqIs = It will install dgoss and build the container locally on the travis infrastructure. Next it will run the dgoss tests based on the goss.yaml file. When those tests are finished it will upload the created and tested docker container to the docker hub. To have travis authorized to push to the docker hub we have to provide him our docker credentials in a secure way. Luckily travis thought that through and provided us with the encrypt feature. $ travis encrypt DOCKER_PASSWORD = YOUR-DOCKER-HUB-PASSWORD --add That way the password is stored into the travis environment variable $DOCKER_PASSWORD through an encrypted way as can be seen in your .travis.yaml file. A detailled log will be provided on travis To get to this travis.yaml file I combined the official travis docker documentation with a tutorial I found on the net. Last but not least I had to disable the automatic build feature of the docker hub repository so only the travis builds will be pushed. That can be done on the \"Build Settings\" of the docker hub repository.","tags":"containers","url":"https://visibilityspots.github.io/blog/docker-openstackclient-kilo.html","loc":"https://visibilityspots.github.io/blog/docker-openstackclient-kilo.html"},{"title":"Auto deploy webpage using pelican and travis","text":"many years ago I created my own webpage, it all started with pure, HTML evolved to a wordpress and finally became a pelican based setup. It got served on many different hosting providers but since a few years it's running on S3 storage and hosted through cloudfront all over the world. It's a very fast setup, and once the site has been deployed and every little service has been configured and implemented the only thing I need to do is writing content in markdown without having to consider how to deploy or how it will look. In this post I'll try to describe how I configured every service, connected them to each other and automated them through travis-ci . pelican it all starts by initiliazing your pelican framework following the quickstart guide. Before you proceed you can configure and write some initial content for your webpage locally and see how it will look like without having it published to the world. You can choose a theme of your choose, adding plugins for various use cases or even import an existing webpage. Once you have something you want to publish we can proceed to publish it to the world. github versioning is something very important in my opinion, by doing so you can easily track changes and collaborate with a team on one web project. Also other people can easily propose changes on your website this way through pull requests. Another purpose of using a github repository is the way we could trigger automation which could deploy our project to different hosting providers. A nice side effect is that you have a backup in the \"cloud\". For the different pelican plugins and themes I use git submodules so I can easily update them with upstream changes. AWS as I already mentioned I opted for AWS to host my blog and some other websites I manage. It's easy to deploy, it's fast and rather cheap compared to other providers, I pay about 30 EUR a year for everything, including domain registration, traffic all over the world and storage. IAM I learned that using dedicated users for every single use case isn't a bad idea. So for this setup we need a dedicated user with pro-grammatic access, which have only full access to S3 and cloudfront only for the distributions we configure. The generated access and secret keys will be used by travis to upload new content to our S3 bucket and invalidate cache. They can be created by following the documentation Access for letsencrypt policy needs to be granted to the user which will be used to update the blog. route53 Creating your own domain or migrating to the DNS service route53 is a very easy way to manage your domain also on amazon. It's easy in the end after all by having one bill for everything. The only thing I struggled with was the way to update your nameservers after you migrated the domain and made an error in them when migrating. In the route53 configuration pane it can be found in the \"Registered domains\" tab and not in the hosted zones! Took me some time to figure out the difference between those 2. Also don't forget to hide your personal data for the different contacts you configured for every registered domain. S3 Amazon S3 object storage service can be used to serve static files and therefore a static webpage we will be using this feature to host our pelican based website on. I found a great how to on stackoverflow which explains perfectly how you have to create 2 buckets to redirect between www and the naked domain and how to enable https once you have that feature enabled. cloudfront cloudfront is amazon's own CDN serving your website around the world on different edge locations. It's easy to implement with your static S3 based setup too. Cloudfront caches your site on the different edge locations, by using cache invalidation we can trigger the different locations to update their cache according to the new files when being pushed through travis later on. Letsencrypt letsencrypt is a free, automated and open Certificate Authority which can be used in combination with S3 using the certbot-s3front tool to get your site served through https. I automated this process with a script based in /usr/local/bin/: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash # # renew certificates for X export AWS_ACCESS_KEY_ID = \"\" export AWS_SECRET_ACCESS_KEY = \"\" certbot --agree-tos -a certbot-s3front:auth \\ --certbot-s3front:auth-s3-bucket BUCKET-NAME \\ --certbot-s3front:auth-s3-region REGION \\ -i certbot-s3front:installer \\ --certbot-s3front:installer-cf-distribution-id CLOUDFRONT-ID \\ -d DOMAIN --renew-by-default --text if [[ $? -ne 0 ]] ; then /usr/bin/ntfy -b telegram send \"ERROR | Certificate renewal for DOMAIN has failed on $( date ) !\" exit 1 ; fi /usr/bin/ntfy -b telegram send \"SUCCESS | Certificates for DOMAIN have been renewed till $( date -d \"3 months\" ) \" It will source the IAM credentials you created for this specific use case, use certbot to renew your certificate on the specified cloudfront distribution and use ntfy to inform you about it through telegram in this case. When the renewal fails it will also sent a notification, I didn't had this feature in the past which lead to expiration of the certificate.. It's triggered by cron: 0 0 1 */ 2 * / usr / local / bin / let sencrypt -* it will be triggered the first day every 2 months , I used this sequence so in case of issues I have time enough to solve it before it expires. Since we now have everything in place and your website should already be available hosted on AWS we can now automate the whole setup. Meaning the only thing you'll have to perform afterwards is writing content and pushing to git. Travis travis is a tool which enables you to easily write automation tasks every time a new commit has been pushed to your repository. Once you've created your account and linked it to github you'll have to enable travis through their GUI on the repositories you want to monitor for automation. Once you've done that for your repository you'll have to configure some credentials and deploy keys. First you'll need the git deploy key, the process is nicely explained by Steve Klabnik . That way you'll have a Github token we'll configure in a bit in our .travis.yaml file. Besides the github token you'll also need to configure the previously created AWS user access and secret keys in travis so travis will be able to update your S3 bucket and invalidates your caches on cloudfront. You'll need to configure those through the GUI of travis on the particular repository as explained by (renzo)[https://renzo.lucioni.xyz/s3-deployment-with-travis/]. Now the most of the administrative part is done a .travis.yaml file is needed in your repository which contains a list of steps to be performed by travis every time a new commit will be pushed. dependencies The travis file I'm referring to is divided in four parts, the first one being the installation of the different python dependencies needed to build and deploy our website. pip install - r requirements . txt make Secondly we rely on the Makefile to first build and deploy the website to github pages and secondly build it for AWS; $ make clean $ make github-travis $ make clean $ make aws-create deployment to AWS When the website is prepared for AWS it can be deployed using the built in deploy of travis. cache invalidation Last but not least is the invalidation of the cache on the different cloudfront edge locations so the updated website is also renewed on those servers. $ aws configure set preview.cloudfront true $ aws cloudfront create-invalidation --distribution-id $CLOUDFRONT_DISTRIBUTION_ID --paths \"/*\" The result of your build can be followed on the travis webpage as for example the build of this page","tags":"linux","url":"https://visibilityspots.github.io/blog/pelican-travis.html","loc":"https://visibilityspots.github.io/blog/pelican-travis.html"},{"title":"Openstack Kilo change MTU till the VM it's tap interface","text":"Recently I was been asked to increase the MTU on the deployed openstack cluster at one of our customers. Since the beginning of my touch on openstack networking has been the hardest part to get my head around. In the first place because openstack does some nifty things on the networking path. But also cause for the use case at the customer a lot of customization has been done to get it implemented in their infrastructure. Hence the shiver when the MTU question was been made.. Nevertheless together with a colleague who likes a challenge and has a profound knowledge in this area we dived into it. Starting at the external device over all the hardware network switches we came to the openstack cluster, until now nothing got in our way of increasing the MTU size. On the most of the network gear (combination of HP and Cisco) the MTU was already high enough. But now we came to the compute nodes of our openstack cluster. We have an RDO based kilo release running with one all-in-one controller and a dozen compute nodes. We isolated the compute node where the test instance was running on and went for our dearest friend Mr google for some advise and found a very informational pdf document about this topic. After some try and error we got to the current situation where the ovs bridge has been configured with this increased MTU size together with the NIC interface of the compute node itself. This has been achieved by changing following parameters. To change the MTU size on the ovs bridge we need veth interfaces as described in the configuration file of the plugin. # /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini use_veth_interconnection = True veth_mtu = 8000 By restarting the neutron component on the compute node the ovs bridge got reconfigured with veth interfaces and an increased MTU size within seconds and no noticeable down time which was very convenient. So there is only one step to achieve our goal of sending big packets over the whole chain, the tap interface of the VM on that OVS bridge. We manually adjusted the tap interfaces by executing an ovs-vsctl command. ip link set mtu 8000 dev PORTID Unfortunately until today we didn't find a fix where a new VM gets a network connection on the OVS bridge with this increased MTU size automatically. We tried several configurations but no luck. As a temporary workaround we found a loop which reconfigures the MTU sizes for all available ports on an OVS bridge. for i in $( ovs-vsctl list ports <BRIDGENAME> ) ; do ip link set mtu 9000 dev $i ; done ; ip a show <BRIDGENAME> The unsuccesfull changes; # /etc/neutron/neutron.conf advertise_mtu = True # /etc/nova/nova.conf network_device_mtu = 8000 So if you did found a solution on this part, please en light us!! :)","tags":"cloud","url":"https://visibilityspots.github.io/blog/openstack-mtu.html","loc":"https://visibilityspots.github.io/blog/openstack-mtu.html"},{"title":"S3stat","text":"Some weeks ago an article on hacker news got my interest. From time to time I really get an healthy dose of jealousy when people found an idea which could make them buy a tesla. My terms of someone who make a lot of money ;) This one is so brilliant in it's simplicity that I really was flabbergasted and made me wonder why I never came up with the idea. It generates nice reports of the usage of your site which is hosted by aws. Based on the logs of the S3 bucket or the cloudfront domain you setted up. As I blogged about a few months ago I migrated my blog as static content onto an S3 bucket and serve it through the CDN of amazon to the world for a really cheap price. I manage my blog with pelican which makes a beautiful static website based on markdown files. One of the features is the google analytics component which sends data through the browser of the visitor. Which can be blocked off course through some inventive add blocking features of the used browser. So I was trilled when logging creating an account on s3stat to see what my data is all about in their visual reports. I started by adding my S3 buckets which obviously didn't have logging enabled. I disabled them back in the days when migrating since I didn't saw a use case for them at that moment. This feature can easily be enabled using the separate aws account I created by following their how to guide. It took a while before the first data got through their website but after a day or two I had some nice and simple reports about the usage of my S3 buckets. One of my blog and one of my custom vagrant boxes . Besides the delay of about one day I could see what I needed to see. In comparison with google analytics they offer some more details especially the referral pages are quite interesting to me which is like the only feature I miss in S3stat, they do show your referral pages, but every single page of the website itself is also seen as a referral page. Which kinda creates a lot of pages and it's hard to find the relevant information of external pages.. Google analytics isn't live data neither and since s3stat can't be blocked by some browser plugins they offer more accurate data about your content usage. Another nice feature are the costs, they give you an idea which of the requests is costing you money. Which could be interesting to see if you could adopt your website so it can be cheaper to host it at AWS.. It took some time to get the cloudfront instances coupled, the interface did found the instances but it froze when I selected on of the cloudfront distributions. After a week or two I finally managed to select them and get them coupled through s3stat. My best guess is that the success of the default setting took down some services. I created a support ticket for it but didn't got an answer so far. Guess they are very busy to keep the service up and running. Since it's working fine now I don't bother about it :) I do have now nice statistics of my blog which is really great and I love it. The only reason I still connect to google analytics are the detailed information about referral websites.. One of the sad parts is the pricing. For my blog it would take about $ 10 each month. Since I only pay $ 2 dollar on average to host it that isn't something I'm willing to pay for those statistics. But I stumbled on their cheap bastard plan which is the mean reason I wrote a blog post about it. And because of my empathy with their simplicity :) I only had trouble the first time I wrote a new article after I setted up s3stat. I'm using s3cmd to upload new pages to the S3 bucket. And the sync command was deleting the log directory which includes all log entries.. So I had to add the exclude parameter to my s3cmd sync command; --exclude 'log/*' This was a huge mistake from my end. Luckily the files aren't that critical I only lost a few days of them since so nothing to bother about in the end..","tags":"cloud","url":"https://visibilityspots.github.io/blog/s3stat.html","loc":"https://visibilityspots.github.io/blog/s3stat.html"},{"title":"wireless bond archlinux arm","text":"for one of my projects, the sms-twitter wall setup, I configured a raspberry pi with 2 wireless network interfaces to connect through a hotspot enabled on an android device. I discovered on previous events that the wireless adapter failed on me from time to time. So I went to the internet to look if I could add a second interface and bond them together. I found a lot of documentation on how to bond an active-backup strategy with a wired and a wireless interfaces but didn't found a setup with 2 wireless interfaces. After a while I figured out how it can be accomplished, even with a static ip. This static ip was necessary because the phone is receiving text messages converting them and sending them to the pi's ip address so the smswall could handle them. Another side note was the wired connection, when at home I wanted to plug the ethernet cable so I could connect to my NAS for backups and to by pass the slow mobile connection for debugging purposes. A lot of usable documentation I found on the archwiki obviously, like the netctl article A range of tools needs to be installed on the system, which I did using yaourt $ yaourt -S netctl wpa_supplicant ifenslave The bonding kernel module needs to be installed and configured vim /etc/modules-load.d/bonding.conf bonding /etc/modprobe.d/bonding.conf options bonding mode=active-backup miimon=100 primary=wlan0 max_bonds=0 the netctl wired connection profile /etc/netcltl/wired Description='A basic static ethernet connection' Interface=eth0 Connection=ethernet IP=static Address=('192.168.0.106/24') Routes=('default metric 1 via 192.168.0.1') DNS=('192.168.0.1') ExcludeAuto=no Priority=2 the netctl profile failover needs to be configured also with a static ip /etc/netctl/failover Description=\"bond interface\" Interface=bond0 Connection=bond BindsToInterfaces=(wlan0 wlan1) IP=static Address=('192.168.43.150/24') Routes=('default metric 100 via 192.168.43.1') DNS=('192.168.43.1') notice the Router parameter instead of the Gateway, by assigning different metrics both profiles can be used together. Else the will be fighting with each other to configure the default route. And in my experience obviously the one you need (wired) will loose over and over again.. Enable the profiles $ sudo systemctl start netctl-ifplugd@eth0.service $ sudo systemctl status netctl-ifplugd@eth0.service $ sudo systemctl enable netctl-ifplugd@eth0.service $ sudo netctl enable failover If you like me already had the wireless interface configured in the netctl-auto modus be sure to disable and stop that service! $ sudo systemctl status netctl-ifplugd@eth0.service $ sudo systemctl stop netctl-ifplugd@eth0.service $ sudo systemctl disable netctl-ifplugd@eth0.service We should now configure the wireless authentication for both wireless adapters. First calculate the wpa_passphrase $ wpa_passphrase SSID passphrase and copy over the output into the following configuration files /etc/wpa_supplicant/wpa_supplicant-wlan0.conf ctrl_interface=/run/wpa_supplicant-wlan0 update_config=1 network={ ssid=\"SSID\" #psk=\"passphrase\" psk=XX } /etc/wpa_supplicant/wpa_supplicant-wlan1.conf ctrl_interface=/run/wpa_supplicant-wlan1 update_config=1 network={ ssid=\"SSID\" #psk=\"passphrase\" psk=XX } and their dependency configuration files /etc/systemd/system/wpa_supplicant@wlan0.service.d/customdependency.conf [Unit] After = netctl@failover.service /etc/systemd/system/wpa_supplicant@wlan1.service.d/customdependency.conf [Unit] After = wpa_supplicant@wlan0.service I had to configure them in this order since they didn't came up properly if I didn't only one could achieve to authenticate instead of both. By having the wlan1 unit depend on the wlan0 I had tackled it. When rebooting the device you should see them both connected using 'iwconfig' and only the bond0 and eth0 interfaces having an ip address using 'ifconfig' or 'ip -4 a' to see the bond status # cat /proc/net/bonding/bond0 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: fault-tolerance (active-backup) Primary Slave: wlan0 (primary_reselect always) Currently Active Slave: wlan0 MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 Slave Interface: wlan0 MII Status: up Speed: Unknown Duplex: Unknown Link Failure Count: 0 Permanent HW addr: ##:## Slave queue ID: 0 Slave Interface: wlan1 MII Status: up Speed: Unknown Duplex: Unknown Link Failure Count: 1 Permanent HW addr: ##:## Slave queue ID: 0 I did some testing by unplugging interfaces, bringing them down through the cli and my system kept online. Even without the ethernet cable plugged ;) So after some debugging I once again won a fight over the network!!","tags":"linux","url":"https://visibilityspots.github.io/blog/wireless-bond-archlinux.html","loc":"https://visibilityspots.github.io/blog/wireless-bond-archlinux.html"},{"title":"AWS migration","text":"About a year ago I attended the AWSome Day at Mechelen . Back then I wrote a first draft article about it, but it got out of my sight unfortunately. I reviewed it and decided to publish it anyway. The event was based on their essentials course and took use through the different AWS core services (compute, storage, database and network). I do know it has nothing to see with open-source. But it is a part of that ultimate cloud based setup I believe in which exists in one central place from where you can manage all your virtual machines independent of which stack/service it uses. In that ultimate setup the public cloud is important to me too. And when you look at public clouds, amazon can't just be ignored in my opinion. That should give you the flexibility to extend your infrastructure when needed, add the ability to benchmark applications between different virtualization / storage resources and make managing them easier without having to open up way too many management consoles. The course the course itself was intended for both technical as management background profiles. The goal of it was merely to highlight the different products and what you can technically achieve with them as well as how they could be combined. introduction Starting with an introduction on the different AWS services and the console going over the many different options, unfortunately the live demos where postponed since the speaker forgot his charger. storage After the introduction the different storage services provided by AWS where enlighted, focusing on the S3 and EBS instances. Where clearly told the first one is an object storage system and the second one can be used to deploy filesystems on it. console demo The speaker also pulled my attention by mentioning you could serve a static website on an S3 instance. Since you only buy for what you use this has been the trigger for me to start looking into migrating my current blog to an amazon hosted one. Which I'll describe further on in this post. Once he got his charger back the speaker showed us the aws management console and the different options and features you could use. During the demo he also pointed to the security best practices like having MFA enabled, not using your root account and such.. So I went buy myself an ezio display card to enhance my geeky nerd state. compute services and networking Next topic of the day concerned the different services of computing instances and networking. Starting with the EC2 by explaining their different tastes and flavors depending on what you want to achieve. Also a tip of the speaker was to benchmark the application you want to provision on different types of instances. It only costs you the amount of time running the benchmark tests. But in the end you should have found the right instance for what you are trying to achieve at a reasonable price in the long term! databases two types of databases are briefly touched first the RDS one which is the more classic alternative amazon provides. And the dynamodb which is like the nosql database managed cloud service. And when this all is what you were looking for you could start perhaps your own amazon based Virtual Private Cloud open guides I stumbled onto open guides for aws which is a collection of very useful information and references for every aws service combined with how-to guides and such. Very useful when playing around with the AWS services and need information about one of them! dynamic DNS It is even possible to get rid of all the free dyndns services you are using and use the route53 API to update your DNS names for certain appliances. By following the guide of Will Warren I now have my own domain name used for dynamic DNS names. Static blog During the presentation the speaker mentioned you could host a static website on an amazon S3 instance. A couple of years ago I migrated my blog to a pelican based one. This tool allows you to write your articles in markdown and convert those into a static html based instance. Since I don't need interactivity for my blog it's the perfect solution to me. The only aspect I need to take into account is to write the actual content. So I don't have to focus about the other stuff like layout and such. I used to host my website on a traditional hosting service called one.com . Costs me about 30 EUR a year for some web space and a domain name. Since it felt like a great exercise to get to know the different AWS services I decided to host my blog on those technologies as a proof of concept to start with. I followed the tutorial from the aws documentation to get it up and running. Using the cloudfront functionality the content of my static blog from the s3 instance will be populated through the different edge locations of the CloudFront Global Edge Network . By linking my new domain name visibilityspots.org which is a Route 53 instance to the cloudfront instance, the end users requests are automatically routed to the nearest edge location for high performance delivery of your content. This makes my blog super fast on any continent for a rather cheap price! I once read an article about the https encryption on the internet. The author believed in a world where only encrypted web traffic should exist so nobody has to care about anymore if their data is encrypted on the web. My blog doesn't has any use case where I really need this encryption. But once again it's a great exercise to set it up to get a feeling how this stuff actually works. So I went for a start ssl domain ssl certificate which costs me nothing but a monthly reactivation mail. This certificate I uploaded to my aws account, as described by Bryce Fisher so I could start using it to serve my blog with the world through an encrypted line. In the meantime letsencrypt was founded and I switched my start ssl certificate to a letsencrypt one. Using the letsencrypt-s3front tool from Diego Lapiduz this got really easy, and I even got it automated through my pi so every x months the certificate is renewed and I get a notification through ntfy on telegram about it as soon as it's done. Price I have my blog served by AWS about more then a year now and it costs me about $ 1.5 every month. With the annual fee of $ 12 for the domain name it costs me about the same as before at one.com. Only now my blog is supersonic fast and available through amazons cloudfront service. Benchmarks I did a test using the siege software on 4 different platforms where I do host my blog. The one.com hosting which is serving the visibilityspots.com domain, the github pages one, the s3 instance directly and the cloudfront cached visibilityspots.org domain. I did expected the one.com domain would be ending at the bottom of the performance tests. Which did indeed turns out as I thought. Rather unexpected was the platform of github scoring the highest on the test. I could have directed my DNS to the github pages but over the time I experienced some down time of github from time to time. It's not that I could save a lot of money by using github so I decided to keep AWS cloudfront as my primary hosting partner. I decided to execute the benchmarks once again after more than one year and as you can see the results are a lot better compared to a year ago. And my decision to stick with AWS has payed of as you can see they are a lot faster than github those days. # siege -b -t5M http://visibilityspots.github.io/blog/ Date & Time, Trans, Elap Time, Data Trans, Resp T, Trans R, Thrghput, Concur, OKAY, Fail 2015 -06-03 22 :35:02, 2794 , 299 .46, 47 , 0 .11, 9 .33, 0 .16, 1 .00, 2794 , 0 2016 -09-18 19 :31:23, 14293 , 299 .94, 24 , 0 .41, 47 .65, 0 .08, 19 .39, 14293 , 1 # siege -b -t5M http://visibilityspots.org 2015 -06-03 22 :40:45, 2642 , 299 .14, 44 , 0 .11, 8 .83, 0 .15, 1 .00, 2642 , 0 2016 -09-18 19 :38:46, 16613 , 299 .90, 79 , 0 .44, 55 .40, 0 .26, 24 .44, 16614 , 0 # siege -b -t5M http://visibilityspots.org.s3-website-eu-west-1.amazonaws.com/ 2015 -06-03 22 :52:32, 1686 , 299 .03, 28 , 0 .18, 5 .64, 0 .09, 1 .00, 1686 , 0 2016 -09-18 19 :45:07, 14063 , 299 .40, 73 , 0 .53, 46 .97, 0 .24, 24 .66, 14063 , 0 # siege -b -t5M http://visibilityspots.com 2015 -06-03 22 :27:47, 1617 , 299 .56, 27 , 0 .19, 5 .40, 0 .09, 1 .00, 1617 , 0 Since the results of all those actions where rather satisfying I decided to migrate all the services I had at one.com to AWS. By using a second domain I could play around without interrupting my existing services. Once each one of them was running I redirected the DNS records to the .org ones. A static blog on S3, a mail service, OWA instance, my little roomba project and an owncloud S3 storage were running on amazon. I still had a VPS running in the field serving an owncloud instance which I used for my calendars (caldav) and address books (carddav) syncing with my laptop vdirsyncer and my android phone davdroid and sharing pictures with the family. Since the performance of the photo page was rather unsatisfying, and storage became an issue I bought myself a Synology DS214play NAS so I migrated all my services to my own little cloud storage running at home. Right now only my blog is served on AWS and all other services are running on my NAS. I don't rely on the public cloud anymore for any of my services. Only an offsite backup of some of my encrypted data containers is synced once in a while through glacier . Which is only used in case of geological disaster happens and both my parents and parents-in-law computes devices are destroyed together with my own (which are in sync using syncthing and my off site backup disk at work got destroyed. Which really sounds paranoia now I write about it :) I moved all this because of some privacy matters I have regarding all the public cloud services provides. And until today that combination really worked out very well. I don't loose a lot of time in maintaining the different platforms but I can focus on using and configuring new services like home automation using home-assistant, a wemos sensor framework, and many more. Which I will blog about in the future.","tags":"cloud","url":"https://visibilityspots.github.io/blog/aws-migration.html","loc":"https://visibilityspots.github.io/blog/aws-migration.html"},{"title":"Openstack static ip","text":"last couple of days I have been fighting with the way an static ip is configured on an openstack virtual centos 6 instance. In our specific use case we ditched as many network openstack services as possible as I previously described. We want to have the instances running in our current network spaces of the R&D department. In this department until some days ago we didn't had any DHCP server running. But a few weeks back we added an extra remote network space into our platform where we configured a remote compute-node. This is where the issues started popping up. In openstack when you spin up an instance with a fixed ip, it will basicly create a neutron port for it, attach it to the vm's NIC interface who will get the ip through DHCP. This makes sense since you want to have your basic images as abstract as possible. But since we disabled a lot of the neutron/openstack network logic our vm got an ip address served by an external dhcp service. Which obviously wasn't what we where looking for. So we digged in the documentation of cloud-init, openstack and network interfaces. Not much has been documented, or at least we couldn't find it easily about the metadata served by a so called configuration drive. I figured the metadata is attached through either a /dev/vdb disk or a /dev/sr0 cd-rom drive. In the ec2 metadata the local-ip is indicating the static ip address assigned to the created port. After some looking around the possibilities we decided to write a little script which will fetch this information, rewrite the interface configuration and restart the network service to get the static up and running. The script is located in /usr/local/bin/reconfigure-static-ip-eth0 #!/bin/bash if $( lsblk -l | grep -q sr0 ) ; then mount /dev/sr0 /mnt elif $( lsblk -l | grep -q vdb ) ; then mount /dev/vdb /mnt else echo \"Mountpoint of metadata not found\" exit 1 ; fi splitip () { local IFS IFS = . set -- $* GATEWAY = $GATEWAY \".\" $@ } STATIC_IP = $( grep -ri local-ipv4 /mnt/ | tr ',' '\\n' | head | grep local-ip | awk -F ' ' '{print $2}' | tr -d '\"' ) GATEWAY = ` echo $STATIC_IP | cut -d \".\" -f1-3 ` \".1\" echo \"VLAN=no\" > /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"NOZEROCONF=yes\" >> /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"NETMASK=255.255.255.0\" >> /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"BOOTPROTO=static\" >> /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"USERCTL=no\" >> /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"IPADDR= $STATIC_IP \" >> /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"ONBOOT=yes\" >> /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"GATEWAY= $GATEWAY \" >> /etc/sysconfig/network-scripts/ifcfg-eth0 echo \"DEVICE=eth0\" >> /etc/sysconfig/network-scripts/ifcfg-eth0 cat /etc/sysconfig/network-scripts/ifcfg-eth0 /etc/init.d/network restart umount /mnt By calling this script through rc.local (/etc/rc.local) it will reconfigure the network interface right after all services are started. In our use case the instance is only used as a hop through different separated network environments so no services are relying on the network interface. During our little search we couldn't believe we are the only ones hitting against this issue, I do hope others will read this post and comment with more clean ways to do so but this did solved it for us in a rather clean and fast way to go further with the development of the actual products behind those hop through nodes.","tags":"cloud","url":"https://visibilityspots.github.io/blog/openstack-static-ip.html","loc":"https://visibilityspots.github.io/blog/openstack-static-ip.html"},{"title":"Openstack live-migration","text":"Some of you may already have notices others just stumbled on this post through a search engine, I have set up an openstack private cloud at one of our projects: vlan flat-neutron provider network layer2 We have noticed that the benefits of having a private cloud is spreading through the different teams within the organization and therefore the interest into this flexibility is growing. Since this wasn't the original use case we are encountering some design issues right now. For the original instances the default overcommit ratios are fine. But the request for new machines with other goals are like interfering with those original instances running in the same default availability zone . So we are looking to configure some aggregate zones to keep this under control. As soon as we figure out a workable solution I will write about it in a new blog post. But in the discussions to come to a solution one remark was , couldn't openstack tackle the issues of having an hypervisor with a growing load and memory issues itself by migrating instances to another hypervisors? Which is like a valuable argument to me. So before even looking into such a solution the feature of live migration should work.. Since we aren't using shared storage for our cloud this could be tricky. So I went to the web to inform myself about the different options. I came across some very interesting reads, like the one of Thornelabs why you shouldn't use shared storage for example. Which has some valuable disadvantages of it besides the benefits. In our use case the benefits aren't outweighing against disadvantages. But as I have noticed in the whole openstack story there are options for almost every cloud use case and therefore the logical complexity of it. So for many amongst you shared storage could be a solution. Another rather interesting one about live migration as a perk not a panacea One of the options is non shared storage, the default of the RDO packstack installer, based on LVM. On our setup we are using this default. This has the consequence we can only use the live migration about with the kvm block storage migration which isn't really supported by the upstream developers and will probably phased out in the future for something more reliable. We configured config drives as the default to get the metadata served to cloud-init at boot time for an instance. The default drive format (iso9660) has a bug in libvirt of copying a read-only disk. To tackle this one we configured the vfat format on all hypervisors. Unfortunately this still doesn't solve our issue with it. Apparently when you use the live migrate option openstack doesn't take the overcommit ratio into account. Since our cloud is already overcommitted we don't have enough resources according to the live migration precheck to move instances around.. The proposed fix isn't released yet in the RDO kilo nova packages and patching a system isn't something I like to do in a semi-production environment. So until today live migration isn't something we have tackled yet on our cloud. If you have solved this on your kilo RDO release cloud already feel free to enlighten me about it!","tags":"cloud","url":"https://visibilityspots.github.io/blog/openstack-live-migration.html","loc":"https://visibilityspots.github.io/blog/openstack-live-migration.html"},{"title":"Openstack layer2","text":"A few months ago I implemented an RDO based openstack kilo release private cloud at one of our customers for their development platform. Through time we tackled a couple of issues so the cloud could be fitted into their work flows. We stumbled onto some minor issues and some major ones. Let's begin with the minor ones ;) When upgrading the all-in-one controller before we started using the cloud in 'production' a mean bug bit us in the ankle due to a new hiera package. After some digging around a patch came to the rescue together with the exclusion of the packages puppet and hiera from the epel repository vim /etc/yum.repos.d/epel.repo +9 exclude=hiera*,puppet Once launched some rather irritating behavior seemed to be the default timeout of horizon (openstack-dashboard) of 30 minutes. It's a development cloud after all and people didn't wanted to re login all the time. To change the default timeout we added a SESSION_TIMEOUT parameter to the local_settings file and restarted apache vim /etc/openstack-dashboard/local_settings SESSION_TIMEOUT = 28800 systemctl restart httpd After which we reconfigured the expiration time of the keystone token and restarted the keystone service vim /etc/keystone/keystone.conf expiration = 28800 systemctl restart openstack-keystone.service Another rather tiny issue was the access to the console through the web interface. It couldn't connect through the instance when running on another compute node. After some research it seemed to by DNS. In the development setup no DNS has been configured for the different compute nodes. We tackled it by the proxy client setting in nova.conf on every compute node vim /etc/nova/nova/conf vncserver_proxyclient_address=actual.ip.of.the.server During maintenance we had to reboot a compute-node, after this reboot the instances living on the compute node where not started and came up in shutdown state. It seemed to be the default behavior of openstack, but as always there is a configuration parameter for it to force the instances to start after a reboot. vim /etc/nova/nova.conf resume_guests_state_on_host_boot=true The default behavior of the kilo RDO deployed cloud to pass data through cloud-init to the instance is by serving the file through a separate network at boot time. We however found it more efficient to serve this file through the filesystem, the so called config_drive option. It can be forced through the nova config file vim /etc/nova/nova.conf force_config_drive=True Also be aware with the lock_passwd feature with passing users through cloud init in openstack due to a bug So now the minor issues are tackled let's switch to the major and more impacting ones. We digged into the behavior of the network during the proof of concept phase. And hell that's a big challenge! By default openstack neutron creates a linux bridge and an open vswitch bridge. The linux bridge is used to translate the security groups into iptables configured to the linux bridge interfaces. In our use case we wanted to keep the networking setup as simple as possible. Mainly cause a lot of the instance will be used for testing purposes including network traffic and performance. Since it's only used for internal usage into the R&D department on the we decided to disable the security groups and therefore to ditch the linux bridge out of the linux cluster. Another reason to disable the security groups was the fact that by using the nova interface-attach command of a network without a subnet configured (layer2) only the default security group was applied to this extra interface. I filled out a bug for this, but it seems we are abusing a bug (attaching networks without subnet configured to an instance) as a feature. Openstack really doesn't like you to take over control of the layer3 networking part after all. But in our use case we really needed to take over this control to keep our instances as abstract and dynamic as possible. We only need layer2 connectivity when adding an instance to a specific VLAN based network, an ip address is provided by another DHCP server on this VLAN network. So we don't want openstack to provide DHCP addresses. By using the nova interface-attach command and the bug/feature of having networks without subnets configured attached we achieved to meet this goal of layer2 connectivity. So to disable the linux bridge you'll need to disable the security groups vim /etc/nova/nova.conf security_group_api = nova firewall_driver = nova.virt.firewall.NoopFirewallDriver vim /etc/neutron/plugins/ml2/ml2_conf.ini # (only on controller node) enable_security_group = False vim /etc/neutron/plugins/openvswitch/ firewall_driver = neutron.agent.firewall.NoopFirewallDriver Last but not least some configuration parameters got changed through without we noticed probably by some misconfigured settings in packstack . To keep this under control we made the configuration files immutable so they could only be modified by manual changes. chattr +i /etc/neutron/plugins/ml2/ml2_conf.ini # (only on controller node) chattr +i /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini chattr +i /etc/nova/nova.conf chattr +i /etc/neutron/policy.json Those are about the main issues worth mentioning we went through. By sharing those I hope to help others in their quest to tame the openstack cluster. If any question arise feel free to comment or contact me about them!","tags":"cloud","url":"https://visibilityspots.github.io/blog/openstack-layer2.html","loc":"https://visibilityspots.github.io/blog/openstack-layer2.html"},{"title":"SMS twitter wall","text":"A long time ago I was active in the local scouting group. To earn some money to keep our group in a healthy financial position we organized a so called vedettweekend. It's an event where people can have a beer, vedett obviously spin a dance, enjoy a spaghetti and have a good chat with your friends. I always played with the idea of creating a social media wall on one of the projectors we rented for the event. But I never found/made time for it, until today.. My youngest brother is who's interests are overlapping with mine brought up the idea again. Since he's already managing the drupal in combination with the facebook page of the organization he was thinking of combining that knowledge to set up a social media page and serve it to one of the projectors using a raspberry pi. So I picked up back the idea and started looking around on the internet to a neat open-source solution which could help us achieve our goal. I found a solution by sms-wall an open-source piece of software which shows tweets and sms messages in real time. A disadvantage of this software is the language although the documentation is very detailed it's hard to translate it from time to time from french to english/dutch. Same does count for the text displayed through the web application, I created a pull request to enhance more languages for the front end of the application so this application could reach more people around the globe! But honestly, this is the only disadvantage, I succeeded setting this up in less then a day and spent more time in setting up the underlying components. hardware HTC One V running cyanogenmod Raspberry pi B+ running alarm xtorm XPD06 - smart hub bluetooth keyboard RT-MWK02 In a first reaction I was going to configure the sms-wall on a VPS or laptop and using the pi to show the stream to the public. But when thinking about the whole idea it just triggered my interest if I could configure it all together on a pi and therefore having a mobile setup which can be deployed everywhere. software operating system As an archlinux addict off course I will try to get it working on this lightweight distribution. Which seems like the perfect use case for a raspberry pi. Archlinux doesn't come with a lot of services running which you will never gonna need. Every single piece of software you need you have to install and configure yourself. That way only the needed services are occupying resources! Which is exactly what I want in like all my use cases :) So I started by installing archlinux to the pi as described in the installation guide , once this base setup was running on both the wired as the wireless network I could ssh into the raspberry to start the installation of the different services needed for the operationality of smswall. It's one of the topics on my todo list to configure a raspberry pi so I could use him as an emergency work station. That means my current window and display manager on a archlinux distribution on a pi. Since this project also needs an output I started to configure this pi with the slim display manager and ratpoison window manager. To get them working you also need xorg and the xf86-video-fbdev driver. I configured slim to auto login and used .xinitrc in the users home dir to start ratpoison exec ratpoison That way when booting up the pi my window manager is started up immediately. services Smswall does need a web service and a database, so I installed and configured apache , mariadb and glued them all together through the different configuration options. Don't forget to install and configure the different extensions for apache too, like php and the pdo database drivers. Once those dependencies are installed I got myself an headache by following the french documentation to install the actual service. I also followed their guidelines to use the python grabber instead of the php tweet grabber and configured an old android phone running cyanogenmod through tasker so it delivers sms messages immediately to the pi itself. wireless To accomplish this behavior I configured the phone as a hotspot and staticly assigned an ip address to the pi. Using this static pi the phone could deliver the incoming messages through the API on served on the static ip on the pi. Through this hotspot the raspberry also has connectivity to the outside world using mobile data. During previous events I experienced some malfunction on the wireless adapter, so I decided to introduce bonding which I spent an entire blog post about. reverse tunnel As port forwarding isn't an option on belgian mobile networks I configured a reverse ssh tunnel to an upstream VPS. A systemctl script will run in the background and try when not already up and having an internet connection available to bring up this tunnel. By doing so I can access the pi from everywhere I have internet connectivity and through some port forwardings I could even access the smswall and admin page myself. pagekite When the pi is also connected to a local LAN through it's ethernet port the reverse tunnel is working fine. But for some events that uplink isn't an option. And for some reason the reverse tunnel option didn't worked out fine. So I looked around on the net and stumbled onto pagekite . After setting up a custom frontend on a VPS I could connect the kite running on the raspberry to it. That way I didn't had to subscribe to the free service of pagekite itself and keeping control of the tunnel. It worked fine through the cli but took me a while to have it started on the pi at boottime. Added it to the startup script; startup script I also wrote a little startup script which waits till there is internet connectivity, start the pagekite service, starts the tweet grabber and the luakit web browser which points directly to the smswall. 1 2 3 4 5 6 7 8 #!/bin/bash while ! ping -c1 www.google.com & >/dev/null ; do : ; done /bin/bash -c \"pagekite &\" cd /srv/http/smswall/ /bin/bash -c \"source env/bin/activate; cd stream; python grabber.py &\" xset s off xset -dpms luakit http://localhost/smswall/smswall result at home I don't have a beamer to test this setup so I attached the pi to our television screen. And I have to say it really looked nice already on this rather little screen. My first idea was to hook up the pi to a powerbank and hook the powerbank to the electricity as a cheap UPS. Unfortunately all powerbanks I tried weren't able to handle both at the same time. So I directly connected the pi to the electricity instead. Found also my old carkit for the phone and attached him to the raspberry pi case. During the event the first evening from about 23 hrs till 4 in the morning about 900 messages were sent through SMS. Tweets where rather rare, about 10 tweets where sent using the hashtag #vedettweekend. As murphy came through the second day the wifi dongle gave up, since debugging on that big beamer screen wasn't really an option since the event was already started I decided to break up the wall that second evening. lessons learned I need some more robust way of communication between the pi and the raspberry or by bonding 2 wifi dongles or by using the USB tethering instead of wifi. I spent some money on a tiny tft lcd screen to attach directly to the pi. As soon as it arrives I will try to configure it in a way I could use it for debugging when no hdmi cable is connected to the pi. It was a real blast and worked perfectly the first night, still we have to warn you, no moderation or filtering has been configured on sms wall. Some people really will abuse this and send some nasty shit to your wall! On the other hand you do have their phone numbers right now up to you what to do with those ;) future I will try to enhance his setup for the next event, maybe with facebook integration and other social networks. Or with a special mobile number so for every message a part can be used to donate. I'll update this post with those enhancements and so keep you posted about the progress on this one.","tags":"linux","url":"https://visibilityspots.github.io/blog/social-media-wall.html","loc":"https://visibilityspots.github.io/blog/social-media-wall.html"},{"title":"Vagrant puppet setup","text":"We at Inuits are using vagrant for a lot of use cases, neither you are a developer or a sysadmin you for sure will walk into it. Me, myself I do use it merely to automate the many different use cases asked by various projects. It took some time to get myself organized with this pretty nifty piece of software. In the beginning I used it with the default virtualization provider virtualbox later on I switched to lxc containers instead. By using those containers I already gained on performance. Spinning up and down new containers to test if an application is deployed fully automatically got 2 times as fast as when using vm's. But what I struggled with the most where the many different projects. Each time a new piece of software needed to be automated I copied over the puppet base I used the previous time. Which lead to outdated setups for older projects, many duplicate code over and over again. When updating base modules for both the puppet agent as the puppetmaster previous projects got forgotten.. So I tried to figure out a way I could keep them all up to date with the same code base. We are using git for almost all our projects as our versioning platform. So I figured out I could use the features of git to achieve the goals I've setted for my setup. One code base I could update without interrupting the functionality of the different proof of concepts but with the availability of upgrading those in a easy way. So I started my vagrant-puppet project on github. In the master branch a base setup has been configured with a puppetmaster container and a client container. Both are running the latest stable release of puppet 3.x. The puppetmaster is setted up using puppetdb puppetserver or apache/passenger it can be used both with the centos6 or centos7 containers I crafted using the lxc-base-boxes repository. puppet to automate the different pieces of software we do use puppet, if upstream puppet-modules are available I pull those in through git submodules if not I write my own. By using the vagrant rsync functionality I could write my module and hiera data in my own preferred environment since they are synced to the running puppetmaster through rsync. This syncronisation can be achieved in two ways. Manually: $ vagrant rsync Or setting up a daemon: $ vagrant rsync-auto That way changes you made through your local environment are synced into the puppetmaster container. upgrade submodules - upstream puppet modules Every once in a while when a new version of puppet has been released I try to keep my container vagrant boxes up to date. Once those are upgraded I get myself into to master branch and update all the used git submodules with this one liner: $ git submodule foreach git pull origin master This way the latest released version of the different used upstream puppet module repositories are fetched into the master branch. And try to provision my puppetmaster from scratch, depending on the changes been done in the different puppet modules I need to adopt my hieradata . By looking into the hieradata you could see I'm using the puppet roles and profiles principle. With one simple trick in the site.pp pointed out by one of my colleagues I created a role based hierarchy in my hieradata. Based on the role fact given in the node hiera data the parameters needed to get the functionality of the role configured are fetched from the role's hieradata. By doing so the hiera data of a particular role can be easily reused without having to keep them in sync on every node who needs the same role. I still need to figure out a way I can achieve the same behavior based on profiles data. This way my master branch keeps staying in sync with the latest releases on the different puppet tools. different projects, different branches But I wanted to go a step further, by getting all my different projects in one place to ease the maintainability of them. So I started tinkering about it. The first idea consisted of having them all in one environment like it would be the case in the real world. But this has a big disadvantage. It would be a mess in the future when a lot of such proof of concepts are combined in one puppet environment. Also an unneeded level of complexity would been added if you want to show of one particular project to a customer or an interested fellow through the interweb. It looked for me this was the perfect use case for branches. Every branch created from the master branch already got a working puppetmaster client setup and can easily be upgraded by merging the master branch into it when upgrades are released. By checking out a branch a subset of different submodules can be loaded after the previous ones are cleaned up: $ git checkout feature_branch $ git clean -d -f -f $ git submodule update --init --recursive This way the specific puppet modules for a specific projects are loaded with a known working version. When merging from the upgraded master branch the submodules are updated to. merging master branch when the master branch has been upgraded I now can easily merge those updates into the different feature branches: $ git checkout feature_branch $ git clean -d -f -f $ git merge origin/master $ git submodule update --init --recursive $ vagrant up puppetmaster --provider = lxc $ vagrant up node --provider = lxc By configuring this setup I know have a flexible environment to test deploy and write new puppet code when some piece of software needs to be automated on my local machine with a puppetmaster almost simultaneous to a production one.","tags":"puppet","url":"https://visibilityspots.github.io/blog/vagrant-puppet-setup.html","loc":"https://visibilityspots.github.io/blog/vagrant-puppet-setup.html"},{"title":"Openstack vlan based flat neutron network provider","text":"at one of my projects I was been asked to set up a private cloud for a validation platform. The whole idea behind this proof of concept is based on the flexibility to spin up and down certain instances providing some specific functionality so tests banks can be ran against them. As soon as the tests are finished the machines could be terminated. Those instances should be configured using some configuration management software, like puppet . That way the instances are rebuildable and could be treated as cattle. On the other hand, it takes about 20 minutes to build up an instance from scratch, centos minimal with a puppet run to install and configure the whole needed stack. So we looked for a workable way to spin up instances really quick without the waiting time of 20 minutes every time. We found a workable solution with packer . By configuring a template which describes a series of steps needed to be executed to get a fully working instance based on a centos minimal cloud instance, we could provide an easy and reusable way to build our artifacts. When running the packer command an openstack instance is launched based on a centos cloud image. Packer will use rsync to upload some needed data directories, in our case a puppet environment. Once this step has been done a local puppet apply will be performed based on the previously uploaded puppet environment. As soon as this puppet run has been successfully executed an image will be created an immediately be uploaded to your openstack instance. By using vagrant you could easily write your puppet code first and test it against a local vm based on virtualbox or lxc containers. Once you know your puppet manifests are working on a local vm you could test it on an openstack instance using the vagrant-openstack provider. That way you could filter out some unforeseen issues without the need of running packer over and over again. When your vagrant-openstack based instance is deployed fine packer is used to build an image of your specific device. By spinning up an instance based on this crafted image you could gain like about 18 minutes every time you launch one since it takes about less than 2 minutes to get it up and running fully functional! Openstack We used the RDO all-in-one installer to get an openstack up and running on one physical machine rather quickly (15-30 minutes for the initial services). To set openstack up without the demo data: # packstack --allinone --provision-demo=n This openstack instance is based on CentOS 7 minimal since it's a requirement of the used openstack release kilo . networking in our case we wanted some different networking setup as from the default one with natting. Instead we wanted a flat network provider so our instances have an ip within the same range as our development network. That way the natting could be kicked out of the setup to exclude some possible networking performance. Beside this flat network we do use vlan's too, so the openstack instance should be able to route over those vlan's too. We found a similar setup on the blog of the University of Zurich. But it lacked an underlying physical network configuration example on the all-in-one node itself. On opencloudblog a clear article helped in trying to understand the network philosophy used to get it working. manual network configuration creating the vlan bridge which is used by openstack to communicate with the physical vlan based networking switch: ovs-vsctl add-br br-vlan ovs-vsctl add-port br-vlan eth0 vconfig add br-vlan 100 configuring an ip from the development range to an interface on the node so we have access to it: ip link set br-vlan up ip link set br-vlan.100 up ip address add dev br-vlan.100 192 .168.0.100 netmask 255 .255.255.0 ip route add default via 192 .168.0.1 configuring openstack ml2 plugin with our vlan setup: /etc/neutron/neutron.conf core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin Configuring the actual vlan's: /etc/neutron/plugins/ml2/ml2_conf.ini type_drivers = vxlan,gre,vlan network_vlan_ranges = vlan100:100:100 Creating the mapping between the vlan and the actual physical interface: /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini bridge_mappings = vlan100:br-vlan to get the metadata service usable on this flat network: /etc/neutron/dhcp-agent.ini enable_isolated_metadata = True restarting neutron-dhcp-agent # openstack-service status neutron-dhcp-agent Configuring the openstack networks on the all-in-one machine: # source keystonerc_admin # neutron subnet - list # neutron subnet - delete ID # neutron net - list # neutron net - delete ID # neutron net - create vlan100 -- shared -- provider : network_type vlan -- provider : segmentation_id 100 -- provider : physical_network vlan100 -- router : external # neutron subnet - create -- name vlan100 -- gateway 192 . 168 . 0 . 1 -- allocation - pool start = 192 . 168 . 0 . 150 , end = 192 . 168 . 0 . 200 -- enable - dhcp -- dns - nameserver 192 . 168 . 0 . 1 vlan100 192 . 168 . 0 . 0 / 24 # neutron subnet - update -- host - route destination = 169 . 254 . 169 . 254 / 32 , nexthop = 192 . 168 . 0 . 151 vlan100 We do have a working setup right now if everything went well and you should be able to launch an instance. To test ICMP traffic do not forget to enable a security group which allows this kind of traffic. Otherwise you couldn't use ping to test traffic. Some useful commands: ovs-vsctl show #shows the openvswitch configuration ovs-ofctl dump-flows br-int #shows the flows to map an internal project tag to an actual vlan id brctl show #shows the linux bridge persistent network configuration To keep your networking up and running after a reboot you should configure you bridges natively on the all-in-one instance: /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE = \"eth0\" ONBOOT = yes OVS_BRIDGE = br-vlan TYPE = OVSPort DEVICETYPE = \"ovs\" /etc/sysconfig/network-scripts/ifcfg-br-vlan DEVICE = br-vlan BOOTPROTO = none ONBOOT = yes TYPE = OVSBridge DEVICETYPE = \"ovs\" /etc/sysconfig/network-scripts/ifcfg-br-vlan.100 BOOTPROTO = \"none\" DEVICE = \"br-vlan.100\" ONBOOT = \"yes\" IPADDR = \"192.168.0.100\" PREFIX = \"24\" GATEWAY = \"192.168.0.1\" DNS1 = \"192.168.0.1\" VLAN = yes NOZEROCONF = yes USERCTL = no Be sure to use the OVSBridge type and ovs DEVICETYPES otherwise it will not work.. Something we have on our todo is the configuration drive setup. When using a configuration drive the metadata dhcp service could also be skipped and therefore possibly the whole openvswitch configuration could be passed by only using a provider network with a linux bridge serverspec were also written so the functionality of the puppet managed services are tested easily over and over to be sure the code is actually doing as it supposed to do.","tags":"cloud","url":"https://visibilityspots.github.io/blog/vlan-flat-neutron-provider.html","loc":"https://visibilityspots.github.io/blog/vlan-flat-neutron-provider.html"},{"title":"Btrfs mount issue","text":"I decided to bootstrap my new machine with btrfs as filesystem instead of ext4 LVM volumes. By following the excellent arch-wiki btrfs page I successfully crafted a base system with sub volumes, limited on size and snapshots enabled. Everything went fine, installed all the other stuff I needed, pulled in my data and was ready to go. Obviously on that very moment disaster happened.. Due to an unexpected interrupt the journal went corrupt. When trying to boot I got stuck right after decrypted the disk failing to mount my btrfs root volume. Uncool, unpleasant, .. I almost got insane.. So back to the live usb arch Linux, decrypted my disk: # cryptsetup luksOpen /dev/sda2 and tried to manually mount the root volume # mount /dev/mapper/root /mnt which resulted in some error like: parent transid verify failed on 109973766144 wanted 1823 found 1821 parent transid verify failed on 13891821568 wanted 540620 found 541176 parent transid verify failed on 13891821568 wanted 540620 found 541176 parent transid verify failed on 13891821568 wanted 540620 found 541176 parent transid verify failed on 13891821568 wanted 540620 found 541176 btrfs: open_ctree failed Crawling through so many forum posts, stack overflow, blogposts a lot of solutions were suggested but none of them resulted in successfully mounting my brand new system.. After some time I could mount the filesystem in recovery mode # mkdir -p /mnt/root # mount -o ro,recover /dev/mapper/root /mnt/root So I decided to copy over all this data to an USB disk using rsync: # mkdir -p /mnt/disk # mount /dev/sdb1 /mnt/disk # rsync -ah --progress /mnt/root /mnt/disk Off course this took some time.. Once the data was copied over I followed the steps described by kossboss : # btrfs-zero-log /dev/mapper/root # btrfsck --init-csum-tree /dev/mapper/root # btrfsck --fix-crc /dev/mapper/root # btrfsck --repair /dev/mapper/root But no luck at all unfortunately So I went online to the irc #btrfs channel and explained my issue. A user named darkling reached out a hand and really made my day by suggesting to mount the filesystem as recovery without the ro option. # mount -orecovery /dev/mapper/root /mnt/root And hell yeah I finally got it mounted, this recovery option did wrote a new working tree when I unmounted the filesystem so I could mount it in a normal way again! # umount /mnt/root # mount /dev/mapper/root /mnt/root So I rebooted my system from live cd to hard drive and got my system back up and running! Darkling really was my hero of the day!","tags":"linux","url":"https://visibilityspots.github.io/blog/btrfs-mount-issue.html","loc":"https://visibilityspots.github.io/blog/btrfs-mount-issue.html"},{"title":"Simple monitoring","text":"As I already wrote about in the past I have a raspberry pi running at home. I do also have a VPS running somewhere on the interweb for an owncloud instance. Being a sysadmin I wanted to know when my home devices become unreachable and when the owncloud instance is down. By mail in the first place if possible by sms message for free in the ideal world. And guess what, I managed to reach the ideal world to monitor my instances. Over here I'll describe how I managed to do so. msmtp First of all you need to install and configure msmtp $ sudo pacman -Syu msmtp The configuration is done in the /etc/msmtprc file, we use telenet as ISP at home so therefore I used their smtp server details: $ sudo vim /etc/msmtprc defaults # A providers service account telenet from yourmailaddress host out.telenet.be # Set a default account account default : telenet mail-alert Next I wrote a bash script which I could pass some parameters so I could use it in many different situations to sent out mails. $ sudo vim /usr/local/bin/mail-alert #!/bin/bash # # Script which sends out mail using the given params RECIPIENT = $1 SUBJECT = $2 MESSAGE = $3 echo \"To: $RECIPIENT \" > .mail echo \"From: youremailaddress\" >> .mail echo \"Subject: $SUBJECT \" >> .mail echo \"\" >> .mail echo \" $MESSAGE \" >> .mail echo \"\" >> .mail cat .mail | msmtp $RECIPIENT rm .mail -rf This way you can easily sent out mails through the command line and therefore use this command in any of your scripts: $ mail-alert recipient@domain.eu \"Subject\" \"Your actual message\" monitor-lan So now I could actually send out mails through the command line I wrote a little bash script which performs some tests and based on the output triggers the mail-alert command: $ sudo vim /usr/local/bin/monitor-lan #!/bin/bash HOSTS = \"8.8.4.4 gateway.ip.address\" COUNT = 4 echo \"===========================================================\" >> /tmp/monitor-lan.log for myHost in $HOSTS do count = $( ping -c $COUNT $myHost | grep 'received' | awk -F ',' '{ print $2 }' | awk '{ print $1 }' ) if [ $count -eq 0 ] ; then echo \" $myHost was down at $( date ) \" >> /tmp/monitor-lan.log mail-alert recipient@domain.eu \" $myHost is down\" \"This mail is to inform that host $myHost is down (ping failed) at $( date ) \" else echo \" $myHost was alright ok at $( date ) \" >> /tmp/monitor-lan.log fi done You can easily test this command by adding a non-reachable ip to the $HOSTS array and manually execute the monitor-lan command. $ monitor-lan You should know have received a mail confirming the ip is down. sms As I already proclaimed I wanted this a step further and receive text messages on my cellphone instead of mails. I achieved this functionality using the service ifttt . In general it works as follows. You create yourself an ifttt account and configure the sms-alerting recipe. Once that's done every mail you sent from the specified mail address using the #hashtag in the subject you configured in the mail channel to trigger@recipe.ifttt.com will trigger an sms to the mobile number you specified in the sms channel. I used the #alert hashtag and therefore needed to reconfigure the monitor-lan script: $ vim /usr/local/bin/monitor-lan #!/bin/bash PING_HOSTS = \"8.8.4.4 gateway.ip.address\" COUNT = 4 echo \"===========================================================\" >> /tmp/monitor-lan.log # Reachable for myHost in $PING_HOSTS do count = $( ping -c $COUNT $myHost | grep 'received' | awk -F ',' '{ print $2 }' | awk '{ print $1 }' ) if [ $count -eq 0 ] ; then echo \" $myHost went down at $( date ) \" >> /tmp/monitor-lan.log mail-alert trigger@recipe.ifttt.com \" $myHost is down #alert\" \" $myHost went down (ping failed) at $( date ) \" else echo \" $myHost was alright at $( date ) \" >> /tmp/monitor-lan.log fi done When you now add an unreachable ip and perform a manual test your should see after a while in the recipe log a trigger is been executed and the message should arrive on your mobile. remote checks Until now we only monitored the reachability of nodes through the icmp protocol. You could also perform more functional tests. Like for example login through ssh to a remote host and see if httpd is running. Before adding the if statement to your existing monitor-lan script you should configure key based authentication between the pi and your remote host. This can easily been done by using the ssh-copy-id command. if [[ $( su - username -c \"ssh remote-node 'ps aux | grep httpd | grep -v grep | wc -l'\" ) ! = \"0\" ]] ; then echo \"httpd was up at $( date ) \" >> /tmp/monitor-lan.log else echo \"httpd went down at $( date ) \" >> /tmp/monitor-lan.log mail-alert trigger@recipe.ifttt.com \"Owncloud is down #alert\" \"Owncloud down (no httpd process running) at $( date ) \" fi cron Now everything is functional and in place you can configure a scheduled cronjob for it as root. # crontab -e # Monitor LAN */15 * * * * /usr/local/bin/monitor-lan You have now a rather easy peasy monitoring setup up and running which provides your the most basic monitoring for your different systems totally for free!","tags":"linux","url":"https://visibilityspots.github.io/blog/simple-monitoring.html","loc":"https://visibilityspots.github.io/blog/simple-monitoring.html"},{"title":"vagrant-setup","text":"In this article I'll try to describe how I use vagrant in my daily tasks as an operations dude as well as I deployed it at one of our customers to help the developers focusing on the coding part rather than the operations part. Vagrant Since the beginning of my career at inuits I'm using vagrant almost everyday. If I got payed every time I spin up a box I could have bought that tesla already some years ago! But unfortunately I'm not :) For almost 99% of the use cases I use this nifty tool it's related to puppet. Writing modules, testing out some configuration changes on a virtual machine first before pushing the changes to development, fighting with selinux, .. it are all crucial but changes that requires a lot of destroyments of boxes. Virtualization Vagrant uses some virtualization techniques to start stop and halt the different development machines. I listed the ones I use on a daily base or from time to time :) Virtualbox Like most of us I started using vagrant together with virtualbox . In the beginning I had a lot of issues when updating virtualbox, every time it was upgraded my vagrant environment failed to stay up and running. Once I figured out most of my used vagrant boxes where reliant on the extension pack I never had any issue upgrading virtualbox anymore. The only thing I had to do was to upgrade the extension pack too. It works great, but it's slow as hell when you spin up boxes again and again to test stuff out since it has to boot a whole vm every time. So about a year ago I started looking at some alternatives. LXC Looking around I found out about lxc containers. My interest was triggered at both linuxcon and CloudCollab 2013 where some talks went about containers. But I struggled configuring the lxc part on my previous fedora machine. It's one of the reasons I switched about a year ago. So I installed ArchLinux on my machine and started reading the related wiki pages. Once I installed all necessary packages I went through the documentation of the vagrant-lxc plugin. There I got on a page describing the configuration steps for archlinux. After following those configuration steps for the networking and DNS part of the lxc functionality I succeeded by creating my very first lxc centos based container: $ sudo lxc-create -t centos -n centos Host CPE ID from /etc/os-release: This is not a CentOS or Redhat host and release is missing, defaulting to 6 use -R | --release to specify release Checking cache download in /var/cache/lxc/centos/x86_64/6/rootfs ... Cache found. Updating... Loaded plugins: fastestmirror Loading mirror speeds from cached hostfile * base: be.mirror.eurid.eu * extras: be.mirror.eurid.eu * updates: be.mirror.eurid.eu base | 3 .7 kB 00 :00 extra | 3 .3 kB 00 :00 update | 3 .4 kB 00 :00 Setting up Update Process No Packages marked for Update Loaded plugins: fastestmirror Cleaning repos: base extras updates 0 package files removed Update finished Copy /var/cache/lxc/centos/x86_64/6/rootfs to /var/lib/lxc/centos/rootfs ... Copying rootfs to /var/lib/lxc/centos/rootfs ... sed: can 't read /etc/init/tty.conf: No such file or directory Storing root password in ' /var/lib/lxc/centos/tmp_root_pass ' Expiring password for user root. passwd: Success Container rootfs and config have been created. Edit the config file to check/enable networking setup. The temporary root password is stored in: ' /var/lib/lxc/centos/tmp_root_pass ' The root password is set up as expired and will require it to be changed at first login, which you should do as soon as possible. If you lose the root password or wish to change it without starting the container, you can change it from the host by running the following command ( which will also reset the expired flag ) : chroot /var/lib/lxc/centos/rootfs passwd $ sudo lxc-start -d -n centos $ sudo lxc-ls -f NAME STATE IPV4 IPV6 AUTOSTART --------------------------------------------------------------------------- centos RUNNING 10 .0.3.94 - NO $ sudo lxc-console -n centos CentOS release 6 .5 ( Final ) Kernel 3 .17.4-1-ARCH on an x86_64 centos login: It was a great feeling finally having that box up and running with internet available so I could update the box and install software on it. Vagrant-lxc Next up was to get such an lxc container up and running using the vagrant framework for my local development purposes. Manual crafted box So I first needed to get a centos minimal container with some vagrant tweaks on it. Looking around for documentation about this part I found out a helpful blog post . But every time I tried to get that freshly build custom box up and running it failed. It was frustrating as hell! But when I figured out the logic behind the vagrant-lxc-base-boxes project I finally got to a working setup and could close the issue myself. So now I could configure a centos minimal lxc container for vagrant usage. But there are a lot of manual steps to perform if I want to keep that box up to date. Script crafted box When founding out about the vagrant-lxc-base-boxes project I tried to get up and running a centos box by following the documentation. But once again it failed big time. After some digging around I came to the conclusion the environment $PATH's of both archlinux the host and centos the guest are not the same and where the cause of this issue. Together with the missing ssh server package I got the script working. So I got a step further, I now could create a centos lxc vagrant box pretty easy myself. Script crafted box with puppet preinstalled Since I develop on puppet a lot, that minimal box needed at least the puppet software itself. The code of vagrant-lxc-base-boxes could do that but only for Debian based guests. So I refactored that code for the centos boxes too. And it worked out great! So great I'm sharing the box through vagrantcloud so everyone can benefit of the usage of vagrant-lxc. Cloudstack At the Cloudstack Collaboration Conference 2014 in Budapest I followed this tutorial of Sebastien Goasguen where I successfully got a vagrant project up and running as a vm through cloudstack. It felt great to get that vm up and running only through vagrant. It is like heaven, you can actually start coding in a vagrant-lxc box first, and test your code out on a so called real life production server only by using vagrant up! Production setup Since I got some great experiences with vagrant myself for developing puppet-modules the developers at the customer found out vagrant is really helpful tool in the actual developing part of a project. So we started with a tutorial of vagrant together with virtualbox. In the initial stage of the vagrant implementation we all used some internet provisioned vagrant box. In the never ending process of improving the infrastructure we came to setup where a custom base box is provisioned by the operations team. This base box is deployed using the same puppet code and therefore the same configuration as a production like server. Base box Every now and then that base box is updated by one of the operations people and placed on an accessible place for the developers. The developers have configured vagrant together with the vagrant-box-updater plugin. That way every time they bring up a vagrant project based on this base box a check will be performed if they are using the latest provisioned base box. I do know it looks very like the golden images era. But I do believe also both team should have their focus on the right topic. Writing code for developers and managing infrastructure for operations. Together with a well-thought deployment process (blog post coming soon) this works out really great. Virtualbox custom base box The base box is crafted based on vStone boxes and provisioned through puppet. $ vagrant init vStone/centos-6.x-puppet.3.x $ vagrant up Once the box is up and running some manual tasks needs to be done $ vagrant ssh $ sudo -s # yum upgrade # yum groupinstall \"Development Tools\" -y # setenforce 1 # vim /etc/sysconfig/selinux Some tweaks for the virtualbox guest additions on centos 6.5 # cd /usr/src/kernels/<kernel_release>/include/drm/ # ln -s /usr/include/drm/drm.h drm.h # ln -s /usr/include/drm/drm_sarea.h drm_sarea.h # ln -s /usr/include/drm/drm_mode.h drm_mode.h # ln -s /usr/include/drm/drm_fourcc.h drm_fourcc.h # exit $ exit Check virtualbox guest additions using the vagrant-vbguest , that way the virtualbox guest additions are updated automatically every time you bring up the box $ vagrant plugin install vagrant-vbguest $ vagrant suspend $ vagrant up Package the existing box with some default vagrant configuration $ vim '''Vagrantfile.pkg''' Vagrant.configure ( VAGRANTFILE_API_VERSION ) do | config | config.vm.box_url = \"http://path/to/custom.box\" config.vm.hostname = \"CUSTOMHOSTNAME\" config.box_updater.autoupdate = true end Creation of the box $ vagrant package --output custom.box --vagrantfile Vagrantfile.pkg The custom.box is the actual box you need to provision on an accessible place for the development team. LXC custom base box For the lxc part a custom base box can also be created. To get all the processes done automatically I extended the vagrant-lxc-base-boxes project with an own_box feature. That way you can easily create a vagrant box from an actively running lxc container you configured for your own needs. $ git clone git@github.com:visibilityspots/vagrant-lxc-base-boxes.git $ cd vagrant-lxc-base-boxes $ ACTIVE_CONTAINER = lxc-container-name \\ make own_box To get the name of the running lxc-container you can use the lxc-ls command. Vagrantfile When starting to use both virtualization platforms next to each other I had created 2 identical Vagrantfile with the only difference the box. But when reading through the docs I found out about you could also override some settings. So I combined those 2 vagrantfiles into one. Depending of which platform you choose (virtualbox by default) the right box and settings are configured. That way you only have to manage one Vagrantfile! # -*- mode: ruby -*- # vi: set ft=ruby : VAGRANTFILE_API_VERSION = \"2\" Vagrant . configure ( VAGRANTFILE_API_VERSION ) do | config | config . vm . define \"default\" do | default | default . vm . hostname = \"vagrant0\" default . vm . provider :virtualbox do | virtualbox , override | override . vm . box = \"vStone/centos-6.x-puppet.3.x\" override . vm . network :forwarded_port , guest : 80 , host : 8080 end default . vm . provider :lxc do | lxc , override | override . vm . box = \"visibilityspots/centos-6.x-puppet-3.x\" lxc . container_name = 'vagrant-container' end end end Example project I abuse the vagrant-yum-repo-server project to showcase the usage of vagrant in my world. By using puppet, hiera, directory environments thanks to a colleague kenny all with the vagrant puppet provisioner you actually get up and running a yum-repo-server yourself without big efforts. As you can imagine that open up gates for developers AND operations cause they are all using the same puppet-tree so you should have control over the configuration of the box in all stages of the project. Vagrant-plugins I use vagrant-cloudstack vagrant-global-status vagrant-hostmanager vagrant-lxc vagrant-vbguest Improvements Some improvements where I need more time for are automating the actual box creation by for example jenkins auto update the base boxes processed like a development box using the ansible orchestration flow .. And of course the suggestions made by who know you? Cause this setup can always be improved, it will never been done..","tags":"linux","url":"https://visibilityspots.github.io/blog/vagrant-setup.html","loc":"https://visibilityspots.github.io/blog/vagrant-setup.html"},{"title":"Github mirroring","text":"As an enthusiastic open-source addict I use github on a regularly base to share my knowledge with the world, to explore new software tools, to enhance software with new features, to fix bugs, to collaborate with others, and above all to live the open source way! But I also have to admit that their are some disadvantages too, from time to time the availability, well lacks availability.., you have to pay for private repositories used for testing purposes and github enterprise can't be used publically anymore.. Self-hosted git Using your own git instance makes your software less accessible, since like the majority of the open-source software is available through github. But on the other hand, you can use as many private repositories as you want, you can protect your git server from the interweb by running it on your private network, you have control over the availability yourself, ... You can set up your own git server quite easy, together with some frontend it's all under your own control. Their are some all in one systems available too, like redmine , gitlab , chilliproject and many others which combines the git repo functionality with some user management system in combination with or without project management and such. Mirroring So I looked for some setup where I could benefit from the github.com awareness and its features as well as the advantages of having your own git instance running. After digging around I found a mirroring solution which can be automated through for example a jenkins instance. $ git clone --mirror git@private.gitinstance.org:localOrganisation/localRepo.git mirroring $ cd mirroring $ git remote set-url --push origin git@github.com:remoteOrg/remoteRepo.git $ git fetch -p origin $ git push --mirror Setting up a mirror automated or not isn't really the biggest issue to solve. One of the things I was wondering about where the upstream pull requests. Merge upstream pull requests I wanted to have a way to pull those upstream pull requests into my private git repository. And get them synced afterwards with upstream. That way I could benefit of both github.com and a one. And yes I found a way described by wincent , to do so you have to go through following steps. First of all go to the directory where your local git repository is located at. Next up is to add a remote to your private git repo based on the user's upstream repo who made the pull request and fetch his repo into a local branch named to the user for example. $ cd localRepo $ git remote add USER git@github.com:USER/REPONAME.git $ git fetch USER Once that's done you point your local repo to the master branch of the users branch you just fetched and test if the code is suiting your taste and everything is still up and running. $ git checkout USER/master When you are satisfied the code matches your standards and everything still works fine you can merge the pull request. $ git checkout master $ git merge USER/master $ git push origin master Once you merged the pull request successfully and you mirrored your local repo to your upstream one, you should see that the upstream pull request also has been automatically closed by your commit. To keep your repos clean you should remove the freshly fetched branch from upstream out of your local repository $ git remote rm USER Owner This whole article only works on git repositories owned by yourself or a git organization you are member of. What I'm still trying to solve is some automated way to keep some upstream github.com repository in sync with a local private git repository through a fork you made on github.com so you could create pull requests yourself to that software. I am an open-source addict and I really want to make efforts to share features I wrote to existing upstream software by creating pull requests, but time has learned I do forget about creating them. So I should find myself a way how to keep my private local repository in sync with the forked one on github.com by mirroring. And some way I get notified about pieces of software ready to create a pull request. Once those pull requests are merged by the upstream author they should be synchronized automatically with the fork and the private repo to keep them all in sync. The reason why I want to automate this process is to not forget about sharing the code with the world cause I lack time, memory and resources to do so.. It's an excuse not worth an excuse and I really should make time for it! #blameingmyself","tags":"linux","url":"https://visibilityspots.github.io/blog/github-mirroring.html","loc":"https://visibilityspots.github.io/blog/github-mirroring.html"},{"title":"Ansible orchestration","text":"I do use puppet as our main configuration management tool. Together with puppetdb all our services are automatically configured from bottom to top. And it rocks, getting automated as much as possible it is like easy as hell to get a server up and running. The only feature it lacked in my opinion is orchestration. I do know about collective which is made for this purpose. Only it's yet again using an agent which fails from time to time and eating resources which can be avoided. It's the same reason I don't use the puppet agent daemon but trigger puppet every time. orchestration We have puppet running every 15 minutes through cron, main reason is to pick up and install the latest software which has been deployed. The other reason puppet runs after installation is to make sure the configuration files were not manually manipulated and making sure necessary services are still running. Using puppet for making sure services are running and configuration files are not being changed an hourly puppet run would be enough. Thing is for those deployment flows it's merely like polling. And I strongly hate polling jobs, 99% of the time they don't have to do anything. So to me it's just useless, a waste of time, energy and resources. It meant that developers had to wait in worst case scenario 15 minutes before their changes where deployed on the development environment. Their changes were already processed by jenkins, packages are been made, deployed on the repository only waiting for puppet to install the latest version of them. Nobody complained, but in my opinion it was waaay too long! By running puppet immediately after the package is been deployed to the repository the right order of installing, configuring and restarting the necessary services can be executed. This will gain time for deployments next to some hourly puppet cron jobs which are running just to be sure no configuration has changed manually and the services are still running. ansible So I started looking at some solution where I could trigger a puppet run on the hosts configured the software through puppet in the right environment as soon as the package is deployed to the proper repository through jenkins. At first I looked into the ssh jenkins plugin, it works but has one big disadvantage. You have to configure ssh credentials for every host in jenkins and therefore you can't use abstract jenkins flows cause you need to configure in each job the specific ssh credentials. I looked further and came across ansible . You don't have to configure a client on every host, neither you have to configure a per server based jenkins configuration to get it working. It was a blessing, the only things you have to do is creating a user, his public ssh key and grant him sudo rights on every server. This can easily be done through puppet! static inventory At first I crawled through our foreman instance and copied over the nodes into 2 groups, development and production, the puppet environments. I also configured some stuff like ssh port and user. I refused to configure the root pw in some plain text file on the jenkins node. That's not safe at all in my opinion, instead I created an ssh key pair and distributed the public key on all servers. In my fight to automate as much as possible this wasn't the most efficient way of using the inventory. Every time you removed or added a node you had to reconfigure it yourself manually in the first place. Beside the manual intervention you also have to take note how you are gonna perform that manual action? Manipulating configuration data on the production machine is not done, using a git repository which you package or adding them to puppet, which both sounds wrong. The first because it's overkill the second because it's rather data over configuration. dynamic inventory In my quest I got pointed to a python script by a colleague. Unfortunately the script isn't straight forward and the 'maintainers' hides themselves behind their footer: Notice : The puppetdb inventory plugin is not quite generic for the moment . Use more as an example . Once I found out about the inventory part of ansible I knew what I was looking for and saw the light by an article on cedecentric.de. Their was only one issue, my jenkins host which needs ansible to run isn't my puppetmaster and therefore can't list the signed certificates as used in his script. But I am using puppetdb , and puppetdb has a great API . So I could take advantage of it by using this great API, melting it down into an inventory script and using the json generated output through ansible. So I started modifying the code example I found on codecentrec and got it working by writing a puppetdb.sh dynamic inventory script. Together with the puppet-ansible module it even got automated too! adding it to ansible-core I went to the Cloudstack Collaboration Conference in Budapest where I followed a tutorial by Sebastien Goasguen . It turned out he wrote an ansible apache-libcloud inventory script and tried to pushing it in to the ansible core. This inspired my to rewrite my bash script in python so it could be added to ansible-core too. After fooling around a bit in python I used the pypuppetdb library so I don't have to make all the API calls natively myself through urllib request. And it turned out quite fine and I got it up and running in my setup. So those days I'm waiting on feedback from the ansible community to my pull request so everyone can benefit of the joy between ansible and puppet. still need some attention I need some time to look which processes it takes to run a command through ansible so I could specify more clear the sudoers file. Also the environments should be more abstract in my puppetdb.sh script without having to manually adapt the necessary puppetdb query files. drinking cocktails From now on it only takes less than 5 minutes to push your code, get it through jenkins tests into a package on an apt or yum repository got pulled into a repository and deploy it through puppet using ansible on the development servers. All without any manual action, without any cron job all automated, glued the pieces together. I'll dig deeper into the whole deployment process later on, when I found time between drinking cocktails, looking at my daughter and living the dream.","tags":"puppet","url":"https://visibilityspots.github.io/blog/ansible-orchestration.html","loc":"https://visibilityspots.github.io/blog/ansible-orchestration.html"},{"title":"Wifi QR code","text":"To make the process of connecting to our local wifi at home a bit less complex I decided to create a qr code for it. That way people can easily use their camera of their smartphone to connect to our network without typing in the WPA key. So I looked on the net for a qr generator and started by typing in our SSID, when realizing it can't be secure to fill in our wpa key too.. It may be a bit paranoia but well I don't trust anything on the interweb most of the time. Doing some further research I found out about qrencode a command line tool which can be used to generate many different QR codes. For installation in archlinux you can use pacman: $ pkgfile qrencode extra/qrencode $ sudo pacman -Syu qrencode Once installed you can generate a qr code consisting of your wifi data base on some standardized barcode contents . When figured out what that content had to be I generated an svg file of our QR code: $ qrencode -t SVG -o wifi-code.svg \"WIFI:S:SSID-OF-YOUR-WLAN;T:WPA2;P:YOUR-WPA2-KEY;;\" You now should be able to print that svg file with your SSID connection information you could use to connect to your network by simple scanning the code from your smartphone using a barcode scanner of your choice. And by not using the internet but your local machine there is no chance anyone is storing that data in some dark database ;)","tags":"linux","url":"https://visibilityspots.github.io/blog/wifi-qr-code.html","loc":"https://visibilityspots.github.io/blog/wifi-qr-code.html"},{"title":"Rsnapshot backup","text":"Some weeks ago I removed some files on my system I wish I didn't cause I still needed them. Since I use some encrypted containers in dropbox I figured I could recover them from this nifty service. But in the logs of dropbox those files are obviously also encrypted. So that was a no go. Therefore I started looking for backup solutions. The first one I tried was backintime , very easy to use, clean GUI interface but failed multiple times in restoring some files. Looking for a more decent piece of software I found rsnapshot And hell I like it. It's not a GUI interface which is a plus for me, it don't use a huge amount of disk space for all the backups, instead it uses hard links between non changed files and it's completely indepented and automizable by cron. So I bought myself a lacie slim P9223 SSD external USB 3.0 disk from about 120GB only used for backing up my system. Encryption Since I want my data to be back upped in a secure way I chose to encrypt the entire disk. I will only use it for this backup purpose so it doesn't matter. To do so I followed a tutorial I found on the net which describes very clearly every step in the process to achieve an encrypted disk using cryptsetup . The data I backup are the decrypted encfs containers from within my dropbox instance. That way dropbox contains my data in an encrypted way so I can synchronize them between multiple machines and I still have a backup on my external hard disk in case of disaster happens. UDEV rule Every time an usb device is connected it is attached to another random /dev/sdX pointer. Since I wrote a little script to mount/umount this encrypted disk I needed a persistent pointer. Using some nifty udev rule based on the serial of the device this issue is solved by always attaching this disk to the /dev/backup pointer.. Get your devices short serial: $ udevadm info -q all -n /dev/sdX1 | grep SERIAL_SHORT E: ID_SERIAL_SHORT = 00000000000000000000 Write the udev rule itself to attach the device to the /dev/backup pointer: $ sudo vim /etc/udev/rules.d/66-persistent-backup-disk.rules KERNEL == \"sd?1\" , SUBSYSTEMS == \"block\" , ENV { ID_SERIAL_SHORT }== \"00000000000000000000\" , SYMLINK += \"backup\" That way the device is always attached to /dev/backup and could therefore be used to decrypt and mount: sudo cryptsetup open /dev/backup backup sudo mount /dev/mapper/backup /mnt/backup/ or umounted and encrypted: sudo umount /mnt/backup sudo cryptsetup luksClose backup Rsnapshot I read the wiki page of rsnapshot from archlinux and followed their configuration instructions and adapted those to my own preferences. After this configuration has been saved and tested I added some cron magic to automate those backups during my lunch break. # Backup schedule (rsnapshot) 0 */3 * * * /usr/bin/rsnapshot hourly 30 12 * * * /usr/bin/rsnapshot daily 45 12 * * 1 /usr/bin/rsnapshot weekly 0 13 1 * 1 /usr/bin/rsnapshot monthly Fsck I ran into some ext4 troubles on my encrypted filesystem, it took me a while to figure out how I could resolve them. You had to open the encrypted container without actually mounting the device. sudo cryptsetup luksOpen /dev/sdX# SOMENAME Once that's done you could use fsck to manually fix the filesystem. sudo fsck.ext4 /dev/mapper/SOMENAME or let fsck automatically fix the filesystem. sudo fsck.ext4 -y /dev/mapper/SOMENAME If everything got fixed you could mount the partition and use the fixed filesystem again. sudo mount /mnt/backup So from now on my data is back upped on an encrypted USB hard disk without using any fancy pansy GUI interface :) Hence feel free to comment if there are some quirks you tackled another way around!","tags":"linux","url":"https://visibilityspots.github.io/blog/rsnapshot-backup.html","loc":"https://visibilityspots.github.io/blog/rsnapshot-backup.html"},{"title":"Nest","text":"A few years ago I discovered the nest thermostat. It looked nice, is connected to the internet, self learning but a bit expensive and not so much documentation if it will work in Belgium nor with the boiler we have installed (Vaillant TurboTec). Doing some research on the internet I figured that they have updated their system software and are supporting European countries including Belgium. A solution for the power has also been found by an ICY converter so it could easily be implemented in setups where no power is transferred over those wires. So I had only to tackle the price, you could order one using the official nest store of the UK but then you would pay about € 300 euro's for only the device. And that was still too pricy for me. Luckily you can find them for about € 190 on amazon , you have to take into account the shipping and handling fee, € 8 and the import fees € 42. Together with the ICY converter which is about € 50 you have the whole setup for about € 290. So you could save some money by ordering with amazon! Convincing myself it's worth the money, and it could save me some on the very long term I took the plunge and ordered one so I could enjoy those geeky features. (Doing so my girlfriend is convinced I'm a nerd, especially because I'm also blogging about it right now) It only took a couple of days to have those 2 packages delivered and I could start playing around with it. Unboxing The nest comes into an hipster sleek box. It's almost the same feeling as unboxing the ipod I bought about 10 years ago. Opening the box the first thing which you discover is the nifty looking product itself. Underneath the nest display a whole bunch of stuff is packed, a paper guide, cool looking screw driver, a couple of mount plates, some screws and the base plate to connect the wires too. Initialization First thing I did was charging the display using a micro USB cable, so I could start configuring it afterwards without being disrupted due to a low battery. After a night of charging I started configuring the thermostat by first connecting it to our wireless network (using a WPA2 encryption) without any hassle it got connected and started downloading the latest updates After the updated had been downloaded and got installed the device rebooted itself and was ready for the real work. Old thermostat Before I could get started I had to remove our current thermostat, a Honeywell CM907. I went ahead after I shut down power from our boiler which is connected to the thermostat. This can be done easily once I figured out how, you have to remove the cover, battery lid and batteries. In the battery compartment I marked two little arrows which indicates being clips you can unlock using a screw driver. Once I got ejected that control panel I got to the wiring underneath. I wrote down the current wired setup on a sticker and putted it on that plastic base mount. That way I could easily roll back if anything will go wrong in the near or far away future. Wiring ICY connection Doing my research on the net I came to the issue of powering your nest device. In Europe most devices are powered by batteries, unfortunately the nest device doesn't. To solve this issue you could use a so called ICY converter. I had to cut the wires coming from my boiler to the thermostat itself. Next step was to connect the wires from the boiler to the CV clips on the converter and the wires going to the thermostat on the Th clips. After connecting all the wires I did not yet plugged the converter in. I first installed the mount plate for the base of the nest before powering on all the electrical devices. Mountplate After disassembling my old thermostat I discovered that the wall has some holes and missing paint behind that old mount plate. Luckily you could use the enclosed base cover to hide those imperfections. Once I drilled the holes, screwed the base plate onto the wall I could connect the wires. As you can see on the picture you have to connect on on the W1 port and one on the Rh port. When I then attached the display, powered on the boiler and plugged in the converter the nest asked configuration of the heating equipment I first got the E24 error which means no power is detected coming to the nest device. I switched the wires from W1 and Rh and repowered the boiler. Now I got the N23 looking at the support pages it's indicating you are using an unusual configuration but my boiler picked up signals coming from the nest thermostat. Configuration I went to all the configuration steps, connecting to the wifi, adding to my online nest account, setting the auto-away options, ... and many more. Once that's done you could see on the screen that I turned the nest to it's minimal, 9° C and that on the moment of taking the picture it was 26° C inside the nest device. Since I took the decision to install this thermostat in the summer when we don't need the CV system I could not yet fully test the device. I didn't wanted to take the risk not having a working CV when it was cold. So I had to turn it off for now and will come back with a review of the usage in most likely december when the winter has taken his start over here in Belgium. Statistics Doing some research about applications for the nest thermostat I discovered a tool called nest-graph . This tool collects data from the nest device using the unofficial nest-api through php. I forked the nest-graph repository to make some customizations like using Celsius instead of Fahrenheit, adding the nest-api-master as a git submodule, changing the database setup and some ignoration of configuration files. Cause I have the availability of a mysql database on my one.com account I decided to collect the data into that database and running the nestgraph service onto that website. Some modifications had to be made to the dbsetup file and adding the login parameters to inc/config.php file. After successfully running the command $ php test.php Once that test is succeeded you could use $ php insert.php to fetch the real time data from your nest device and import them into your mysql database. From my raspberry-pi I configured a cron job to log in into the my one.com hosting account using ssh and start the import job every 5 minutes. */5 * * * * ssh ONE.COM-URL \"/bin/rm -f /tmp/nest_php_* ; /usr/bin/php /path/to/nestgraph/insert.php > /dev/null\" Last but not least I added some security to this service using htaccess so all this information isn't publicly available. Now I only have to wait to winter so I can finally start monitoring our heating usage! Resources fousa.be promo-code.be (dutch) how-stuff-works nest-api nest-graph open-source nest alternative","tags":"home-automation","url":"https://visibilityspots.github.io/blog/nest.html","loc":"https://visibilityspots.github.io/blog/nest.html"},{"title":"Raspberry pi setup","text":"Since I discovered the joy of linux servers over desktop distributions a few years ago I revived an old portable and promoted him to be my home server. Connected him our router in the little storage room on a top shelf gathering dust I could test, configure, break (and pass sleep) a huge variety of open-source software. Many of those adventures I also used to provide my blog with content. After a while I figured this setup isn't really needed to be powered on 24h a day 7 days a week. So I bought myself a raspberry pi which would cover the basic functionalities I needed to be online as much as possible without the need of a subscription for a VPS or dedicated server in one fancy pansy data center. For the operating system I didn't choose for the default raspbian , mainly because I don't need a graphical interface. So I headed over to archlinux arm, also called alarm . The installation is quite straight forward. After that I installed and configured some other stuff on it to gather my ultimate personal little mini server: vim ssh mosh noip screen yaourt pkgfile bash-completion ncdu htop mlocate msmtp locale timedatectl fake-hwclock lsyncd Partitioning After a while I couldn't update any packages anymore cause my root partition was full. When looking at the partitions I noticed I had only 1.7G for my root partition available but I installed it on a 4G sd card. Looking for a solution I discovered the base image I copied over is by default configured for 2G cards. You have to expand the file system yourself if you want to benefit the full amount of storage on your sd card. Following this tutorial I found on the net I achieved to grow my root partition without extracting the sd card. RAM Using ssh I will only use terminal connections to the pi without any graphical interfaces though the HDMI socket. By default the memory of the pi is shared for those graphical stuff and normal os operations. I decided to decrease the amount of memory for the gpu by setting those amounts into the /boot/config.txt file: gpu_mem_512 = 48 gpu_mem_256 = 48 That way I could benefit of more memory for the stuff I will run on the pi. Communication To be online day and night I installed a chat client using bitlbee and irssi . When I am not connected to this chat terminal and I get some message on one of the connected channels a notification will be sent through my android phone using irssinotifier so I could decide if it's important enough to connect using my phone, juicessh or spin up my laptop. Be sure to check out all the nifty scripts which can increase the joy of using the irssi chat client. RAID For one of projects running at inuits I was asked to set up a software raid using mdadm . Through the years I have collected a bunch of USB disks. Combining those two facts I figured I could set up a raid using a little usb hub. Model B I am aware of the bottle neck this hub creates to the raid setup but since it's not for a production environment and I like to play around I doesn't care about it :) Model B+ An updated model of the raspberry pi was launched, the model b+ extended with to 4 USB ports in total. So finally I could gain benefit of my raid setup. I reconfigured my whole RAID setup by using those 4 individual USB sockets instead of the hub I used before. And man what a difference! It runs a lot faster and is a lot more use full and efficient nowadays. Since there are no case available yet for this model I looked on thingiverse and found a closed case which I printed out on the ultimaker2 robot we can use at the office. It's a neat case but the top isn't clicked in to the bottom part so I had to use tape to stick them together.. Configuration This whole setup is based on the well documented archlinux wiki page 4 sticks of 2 GB were formatted to ext4: # mkfs.ext4 /dev/sdXX -L NAME Next step was to identify the UUID's of those freshly created volumes so I could use those to initialize the actual raid setup. $ sudo blkid /dev/sde1: LABEL = \"three\" UUID = \"39564f64-18ed-4f0b-a2d8-9b2d7c62032a\" TYPE = \"ext4\" PARTUUID = \"91f72d24-01\" /dev/sdb1: LABEL = \"four\" UUID = \"9b37dc7f-3f0c-44ba-846b-e9ba9efaa03a\" TYPE = \"ext4\" PARTUUID = \"688434a6-01\" /dev/sdd1: LABEL = \"two\" UUID = \"321d1d03-eb87-4129-83c7-ee1ce232d1c1\" TYPE = \"ext4\" PARTUUID = \"9852d7fa-01\" /dev/sdc1: LABEL = \"one\" UUID = \"d61e46bd-9a28-427e-9a85-94dc292463ec\" TYPE = \"ext4\" PARTUUID = \"00099342-01\" Using the gathered UUID's I then created a raid5 using 3 active sticks and one as hot spare device: $ sudo mdadm --create --verbose --level = 5 --metadata = 1 .2 --chunk = 256 --raid-devices = 3 /dev/md0 /dev/disk/by-uuid/d61e46bd-9a28-427e-9a85-94dc292463ec /dev/disk/by-uuid/321d1d03-eb87-4129-83c7-ee1ce232d1c1 /dev/disk/by-uuid/39564f64-18ed-4f0b-a2d8-9b2d7c62032a --spare-devices = 1 /dev/disk/by-uuid/9b37dc7f-3f0c-44ba-846b-e9ba9efaa03a mdadm: layout defaults to left-symmetric mdadm: /dev/disk/by-uuid/d61e46bd-9a28-427e-9a85-94dc292463ec appears to contain an ext2fs file system size = 1982464K mtime = Thu Jan 1 01 :00:00 1970 mdadm: /dev/disk/by-uuid/321d1d03-eb87-4129-83c7-ee1ce232d1c1 appears to contain an ext2fs file system size = 2013184K mtime = Thu Jan 1 01 :00:00 1970 mdadm: /dev/disk/by-uuid/39564f64-18ed-4f0b-a2d8-9b2d7c62032a appears to contain an ext2fs file system size = 2013184K mtime = Thu Jan 1 01 :00:00 1970 mdadm: /dev/disk/by-uuid/9b37dc7f-3f0c-44ba-846b-e9ba9efaa03a appears to contain an ext2fs file system size = 1982464K mtime = Thu Jan 1 01 :00:00 1970 mdadm: size set to 1981440K mdadm: largest drive ( /dev/disk/by-uuid/321d1d03-eb87-4129-83c7-ee1ce232d1c1 ) exceeds size ( 1981440K ) by more than 1 % Continue creating array? y mdadm: array /dev/md0 started. You can see the progress of the creation by: cat /proc/mdstat Personalities : [ linear ] [ raid0 ] [ raid1 ] [ raid10 ] [ raid6 ] [ raid5 ] [ raid4 ] [ multipath ] [ faulty ] md0 : active raid5 sde1 [ 4 ] sdb1 [ 3 ]( S ) sdd1 [ 1 ] sdc1 [ 0 ] 3962880 blocks super 1 .2 level 5 , 256k chunk, algorithm 2 [ 3 /2 ] [ UU_ ] [ >.................... ] recovery = 1 .8% ( 36960 /1981440 ) finish = 96 .5min speed = 335K/sec unused devices: <none> Once the creation process has been done you can start by updating the configuration: # mdadm --detail --scan >> /etc/mdadm.conf # mdadm --assemble --scan And we finally can create a file system on the raid itself: # mkfs.ext4 -v -L NAME -m 0.5 -b 4096 -E stride=64,stripe-width=192 /dev/md0 Issues Since the raspberry is powered by the USB slot of my ISP's modem and they quite often restart their device for software updates the pi also rebooted from time to time. During such reboots I figured that the process of mounting the raid volume got stuck. This because the hardware came up to slowly and the mounting process didn't recognized the usb sticks. So I wrote this script (/usr/bin/start-communication) which does the magic (after many try and error attempts). #!/bin/bash # # Script which mounts the RAID volume storage before starting an irssi screen session # Mount the storage RAID volume while ! df | grep NAME ; do echo \"10 sec break...\" ; sleep 10 sudo mount /dev/md0 /NAME done echo \"storage mounted\" # Start irssi in a screen session as user 'X' if ! screen -list | grep irssi ; then sudo -u X /usr/bin/screen -dmS irssi irssi ; fi echo \"irssi screen session started\" This script is triggered by cron after every reboot. @reboot start - communication > / tmp / startup . log And the output is logged in the file /tmp/startup.log. Backup image Once a week I overnight I have a cron job running which creates a compressed image file of the whole sd card of my running pi and pushes it to a mounted network share on my boxee iomega device. #!/bin/bash DATE = $( date +%d-%m-%Y ) if /usr/bin/mount | grep MOUNT ; then dd if = /dev/mmcblk0 | gzip -1 - | dd of = MOUNT/backup- $DATE .img.gz fi That way I can easily pull this image onto a new sd card if there goes something wrong with the existing one without the need of reinstalling everything directory backup For some minor applications like irssi I use lsyncd to sync between different machines. I have one daemon running which is used to backup directories to a network share and another one to mirror between different machines through ssh. VPN One of the next steps will be a vpn setup based on this tutorial Wake on lan Once I got configured the VPN setup I will reconfigure my old laptop as being an sms-service . Since I don't need this service being up all the time I will configure the wake on lan service on that laptop. That way I can get him up from remote by logging in at my pi and sending the magical WOL packet to that laptop.","tags":"linux","url":"https://visibilityspots.github.io/blog/raspberry-pi.html","loc":"https://visibilityspots.github.io/blog/raspberry-pi.html"},{"title":"Octoprint","text":"At our office we have an ultimaker 2 installed, we have it now for some weeks and already printed out a bunch of stuff. It's cool to see, amazed by the technology. Expect the manner to start a print job, you have to save your generated gcode files onto an SD card, stick it into the ultimaker and use the rather simple menu from the tiny display to start the print job. The time of a print job is rather long depending on the object you want to print, so many times you got up and went seeing if everything went alright, so quickly distracted.. This sounds really crazy for such a piece of high tech technology and therefore I did a bit research on the web for a nice remote web interface. It didn't cost me a lot of time/energy to find out about octoprint Raspberry I first started by installing the octoprint manually on a raspbian distribution. But it took really long, I mean REALLY long so I downloaded the preconfigured image , maintained by Guy Sheffer , and installed it on a 4GB SD card. I did some tweaks on the raspbian distribution, configured a for the pi user, disabled root login for the ssh service, updated the whole distribution and configured the wlan0 interface to connect wireless. I struggled a bit with the configuration of the wireless part. Merely the pairwise parameter. I forgot to configure it to CCMP for the WPA2 settings of the wireless network. Resulting in the iwconfig output: unassociated Nickname : \"<WIFI@REALTEK>\" Once I've setted up the configuration as you can see, network traffic came up on the wlan0 interface: File: /etc/network/interfaces allow-hotplug wlan0 auto wlan0 iface wlan0 inet dhcp wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf File: /etc/wpa_supplicant/wpa_supplicant.conf ssid=\"\" scan_ssid=0 psk=\"\" proto=RSN key_mgmt=WPA-PSK pairwise=CCMP auth_alg=OPEN Raspberry cam We also bought a camera module which is discovered automatically by octoprint. Once you've connected the hardware as described in the movie and booted the raspberry the camera is immediately picked up by the octoprint service. So therefore you can't connect to the hardware when testing out raspistill and others to test the cam. It took me some time to figure that out ;) You should be able to catch the output of the mjpeg-streamer on port 8080 of the raspberry. Octoprint The octoprint interface is served on port 80 . The account details are: Where you can see the operational state of the printer if everything went well. The connection settings are: Serial Port: /dev/ttyACM0 Baudrate: 250000 Normally the raspberry should connect using those settings automatically at startup to the ultimaker as you could configure in the settings page of octoprint. Usage You can look on for example thingiverse for source files of objects you want to print. (the .stl files). Which you have to slice using cura . Be aware you set the machine settings of the cura to reprap (File -> Machine Settings -> GCodeFlavor -> RepRap (Marlin/Sprinter) ) so the octoprint engine can process your saved gcode files. Once you've saved your gcode file you need to upload it using the octoprint webpage. After the upload is complete you can search your project in the Files section and click on the little printer icon on the right of the name. You should see the temperature of the head is increasing to 220 degrees followed by the bed temperature of 70 degrees. Once the ultimaker is warmed up your print will start. And you should see the progress of it in the octoprint interface. Pictures Happy printing!","tags":"linux","url":"https://visibilityspots.github.io/blog/octoprint.html","loc":"https://visibilityspots.github.io/blog/octoprint.html"},{"title":"Pakiti setup","text":"Nowadays it becomes more and more relevant knowing which version of a package is installed and having an overview of the packages infected by some bugs or security holes. That way you could see which servers are possibly vulnerable for those on the dark side. That's where pakiti comes in a clean web based overview of your servers listing all packages vulnerable against the CVE in particular. It's a client server setup where the client reports the version of the packages to the pakiti server. The server on his turn checks those versions against CVE to see if there are issues. It is only a reporting tool, so pakiti will never install any package update, that still is and should be a controlled step which prevents unforeseen calls in the middle on the night when auto updates are breaking stuff. Pakiti server On the pakiti website they provide packages you could download and install out of the box. The pakiti-server needs a mysql database to store his data. The whole installation process is pretty well documented in the documentation section of their homepage You also need a webserver to deliver the pakiti web service. A final step is the configuration from where pakiti would fetch the information about which packages are or are not a possible security risk for the server. Pakiti client On the servers you want to monitor the pakiti client package should be installed. Once a day this client will send a list with all packages installed on the system to the pakiti server through HTTP. The configuration file could be found in /etc/pakiti2/pakiti-client.conf Puppet module Since I believe in automation I also wrote a puppet-pakiti module which can be implemented in an existing puppet tree so you don't have to install and configure all your servers with the pakiti client. Enjoy the overview, and happy updating ;)","tags":"linux","url":"https://visibilityspots.github.io/blog/pakiti.html","loc":"https://visibilityspots.github.io/blog/pakiti.html"},{"title":"Mkdocs documentation","text":"To make our and other lives less painful writing documentation is a good start to decrease the level of frustration when working on a shared project. It's a common feeling writing documentation isn't something we are all waiting for to do. In an effort to make it easier for all of us an automatically way of deployment can be managed by our good friend jenkins in combination with docker. The details about this flow is been described on this page. After reading through this documentation section you should be aware of the general deployment idea so you can implement it yourself and start writing documentation without any hassle. Mark down The goal is that you write documentation using mark down in a git repository, that way you can easily write together with others on the same documentation in a structured and versionned manner. By using mark down we can easily convert those md documents to whatever you want and gives us an easy syntax to write documentation. Mkdocs Using mkdocs a nice and easy manner has been found to generate a clean static html site based on the md files without much effort. The installation is quite straight forward using python-pip: $ sudo pip install mkdocs Once the installation process ended successfully you should be able to run the mkdocs engine: $ mkdocs help mkdocs [ help | new | build | serve ] { options } Usage Starting a new project is easy as hell: $ mkdocs new PROJECT-NAME Creating project directory: PROJECT-NAME Writing config file: PROJECT-NAME/mkdocs.yml Writing initial docs: PROJECT-NAME/docs/index.md the mkdocs structure is automatically generated as you can see in a brand new PROJECT-NAME directory: $ ls PROJECT-NAME docs mkdocs.yml As you could see the repository exists of a docs directory containing the md files with the actual content and a mkdocs.yml file which is used to generate the sites index and menus local preview The first thing you could do is to build a local preview of the html structure so you have a real time preview of your modifications: $ cd PROJECT-NAME $ mkdocs serve Running at: http://127.0.0.1:8000/ Live reload enabled. Hold ctrl+c to quit. When the mkdocs engine started successfully you could surf through your browser to localhost:8000 and start watching the preview of your documentation on your local machine. You should see a site/ directory has been generated containing the static html structure based on the docs/ md files. After editing the md files and saving your modifications they should appear immediately on your local preview when the mkdocs server command is running. mkdocs.yaml As mentioned a mkdocs.yml file manages the index and menus of the site: site_name : My Docs pages : - [ index.md , Home ] #theme: readthedocs images Using images is quite easy, add your jpg, png or whatever files into the docs/img/ directory and reference to them in your md file as follow: ! [ reference name ]( img/imagefile.png ) Migrate existing documentation Using pandoc we could convert the majority of source files to markdown: $ pandoc source.txt -f textile -t markdown -o output.md Be aware you should review the generated output cause the human intellect still cannot be fully replaced by bits and bytes.. Automation Once you've written the documentation in markdown, checked locally the layout and ran through a spell checker you could push them to git repository. A jenkins build flow could be triggered using a post-receive hooks. This flow on his turn will orchestrates some jobs: build ( \"package-doc\" ) build ( \"repository\" ) build ( \"deploy-package\", packagename: \"infra-doc\", node: \"webserver.domain.org\") Package-doc This job will use the git repository as a source to generate the html site/ directory by the mkdocs build command. (tip: create a .gitignore file in the root of your git repo with .~ .swp and site/ so you don't upload swap files or you local generated site/ directory) The nifty tool fpm is used to generate an rpm package of that freshly created site/ directory to be deployed on hosting. if [ -f *.rpm ] then rm *.rpm fi if [ -d \"site/\" ] then rm site/ -rf fi mkdocs build cd site/ RELEASE = ` git rev-list --all | wc -l ` fpm -s dir -t rpm \\ --name \"doc\" \\ --version \"1.0\" \\ --iteration \" ${ RELEASE } \" \\ --architecture noarch \\ --prefix /var/www/ \\ --rpm-user apache \\ --rpm-group apache \\ --description 'The html files for documentation' \\ --maintainer 'Jenkins' \\ --epoch '1' \\ . mv *.rpm ../ cd .. rpm -qlp *.rpm A brand new shiny rpm package artifact then could be archived so the next step in the flow could use it. Repository The rpm artifact of the package-doc job could then be used to deploy on your favorite repository service, from createrepo , pulp , yum-repo-server , prm to packagecloud so the next job can be triggered to install/update the package on your webserver Deploy-package Next you could configure a jenkins job which for examples logs in through ssh and installs the package you've pushed to your repository. Configuration management Instead of the deployment-package job you could also use a configuration management tool which does the installation/upgrade for you ;) Docker Instead of installing the tools on your local machine or your build server you could also opt for docker, there are a lot of preconfigured docker containers available on the internet or you could start making your own docker file relying on for example a centos official docker container and only mount your markdown documents into the container. That way you have more control over the environment and releases independent of the host system both by the ones who are writing the documentation as your build system.. Useful links Adam-p Basics mkdocs pandoc ispell","tags":"linux","url":"https://visibilityspots.github.io/blog/mkdocs.html","loc":"https://visibilityspots.github.io/blog/mkdocs.html"},{"title":"CloudCollab Amsterdam #CCCEU13","text":"Cloudstack, an item I had on my todo list with some lower priority against daily maintenance of our server park. But since attending David Nalley's talk on LinuxCon I shifted it up some places. Although I expected a real hands on session the talk he gave about a cloudstack environment for development was really intriguing and matched completely with what I had in mind. Being fully convinced it fits in my idea of a fully automated development environment which meets to all the needs of developers to start writing code real quickly on machines similar to the production environment. At that same conference LinuxCon I also attended a talk from James Bottomley about a container based cloud. I already did some stuff with lxc containers on my local machine hoping I could get it working one day so I could get rid of virtualbox and using containers for my puppet development instead. But I never thought about integrating containers in a cloud instead of the traditional hypervisors. James gave a really inspiring talk about containers and passed his enthusiasm about it! Nevertheless it's a huge step to migrate from a traditional virtual based setup to a cloud based on containers. A huge challenge, but a challenge I can't wait to start on. Some weeks after LinuxCon, CloudCollab took place, a chance I took with both hands when being asked to keep those days free. And so I drove to Amsterdam the evening before the event. I learned to read through the whole registration process and not let emotions and enthusiasm take it over so you forget or over read some major things. Thanks at those who fixed that the day before the event. Hackathon Workshop day I registered early at the Beurs van Berlage where the conference is held cause I hate waiting queues. The opening talk was quite clear, we cloud/system admins must prevent the end users application developers and such to have frustrating moments where they banging their heads to the table. With that said the workshops begun, I registered for the one-day cloudstack bootcamp by shape blue . After dealing with the exfat file system a friendly neighbor and changing some setting in the provisioned ova file for the xenserver I managed to go through the whole cycle of setting up domains, groups, accounts, networks, offering templates to finally getting up some vm's running and being able to access through ssh. To do this we used the GUI, I'm looking forward to use the api they showed at the end of the day which looked far more my kinda usage then the GUI. After the bootcamp my head was still dizzy but I took the opportunity to attend the elastic-search user group meeting being held at booking.com. Ralph Meijer spoke about Logstach, Kibana and elasticsearch. Where vulcan popped out for me as being interesting in combination with such a setup. Conference day one After being waked up 5 min early, I started the second day at CloudCollab by attending the Keynote of John Willis talking about the next frontier for devops. An entertaining talk but also very interesting topic. It seems like the history will repeat but now in networking area. After the break I attended a talk about Devops, Killing of the Dinosaurs. Where all kind of culture troubles and people are compared to dinosaurs and how they achieved to kill them to get on. I figured out I forgot my notebook at the keynote, luckily it was still where I left it, but I missed therefore the talks during that time. So I continued writing this post. Next where the ignite talks, short talks where the slides are automatically flipped each 15 seconds. Unless you cheat and create duplicate slides off course. John Willis can talk like a machine, really really fast but still understandable. Lunch being served stopped by some boots I started the afternoon by attending a talk about vagrant-cloudstack , which is really cool, finally I could perhaps using vagrant boxes exactly the same as a normal production server for development of puppet-modules without having to kickstart manually some boxes. This cloudstack virus is really getting me. The future of sysadmins , finally I could attend a talk of our colleague Kris Buytaert . Beside that fact I really was astonished that the way we are working at Inuits using automated pipelines, vagrant development, jenkins even pulp isn't yet commonly used. I couldn't believe I was like the only knowing some of the answers on Kris's questions. Obviously he would have nailed me when I didn't, but the only one? It's a mixed feeling being proud that we doing all those cool stuff, a bit disappointed not the majority of organizations are taking advantage of it. After Kris's talk I went to a talk about monitoring a cloudstack environment. It felt like a sales talk for ca technologies own proprietary tool. Bit disappointing that it wasn't what I expected to be after reading the summary on lanyrd about the talk. So I went for a coffee and bumped into some guys of the University Library of Cambridge at the Pinball machine in the dev room. Cool to see the story of their environment is quite the same as ours at the University Library of Ghent. Being at the end of the day I followed a user panel about 4 organizations who implemented cloudstack for their business all with a different approach and goals. The one that popped out for me was Greenqloud an Icelandic cloud provider running on 100% renewable energy (as everyone in Iceland), but which also does effort in other areas, like their hardware itself and the buildings their datacenters are deployed in. After dinner we had a great time with the folks of shape blue and schuberg philis at the pub. It's really fascinating to see such a dynamic atmosphere. Conference day two The last day of the conference started by checking out the hotel and attending the delayed keynote of Mark Burgess about Uncertain Cloud Infrastructures. His book In search of certainity is added to my wish list for the upcoming holiday gifts. Next talk I joined was about the Netapp cloudstack plugin which was really interesting, I hope I can get my hands on the beta version of the VSC for cloudstack software soon so I can start playing around with it on our test lab. After being disappointed by a vendor talking about a topic which ended up in a sales talk I didn't had big expectations for the talk of Mike Tutkowski from Solid-fire about Guaranteed storage performance. But man how I was wrong. What a great talk. The guy really knew what he was talking about, explained how the storage area of cloudstack works and how they integrated it with their products. All vendor based sales talks should attend this talk and learn from it. That way more people could be becoming interested in your product only because of the clear and transparent explanation. Because I'm looking for some scalable storage solution I attended the talk of Wido den Hollander about ceph . Wido is a passionate ceph lover who gave a crash course of ceph in 30 minutes. In that little time he really gave a clear overview of how ceph could be used together with cloudstack. Using little pizza box servers with one cpu and four disks you could easily manage your own ceph storage cluster. After those 2 storage talks I came to this conclusion for myself that ceph would be a great challenge if you want to keep control over your own storage soft- and hardware based besides the fact you also have to keep in mind about the physical space. Another solution could be a solid-fire solution where you move the responsibility to a vendor. A great advantage of solid-file is that you can start with a small amount of data and grow your storage on a flexible and scalable manner to your own needs by just adding an extra node like the ceph solution and not like other vendors where you need to review the whole license contract. I decided to attend the storage panel after those 2 talks being convinced that not only the cloud solution is important and changing the traditional ways of Virtualization but also storage is moving over to some more advanced flexible solutions. Nevertheless I couldn't really hold my focus to the discussions being overwhelmed of the idea of the flexibility of those storage clusters being scalable, reliable and flexible volumes along on or more racks in multiple datacenters. I can only remember the statement of Wido: 'We still have storage problems. They are called NFS and iSCSI' because of my daydreams about storage clusters. Being already 16hrs and a bit mental overwhelmed I was hesitating to leave already to home or attending the latest slots of talks. I decided to stay being interested about Tim Mackey 's talk on the different hypervisors and how to choose between them to drive your cloud solution. He made a clear comparison between the different options. I hope I can catch his slides soon to share with you. The closing note ended with a nice video about the conference was a great closing for a conference where I learned so many new technologies, options between the different solutions and inspiring people. I want to thank hereby the people of Schuberg philis for the organization!","tags":"conferences","url":"https://visibilityspots.github.io/blog/cloudcollab-amsterdam.html","loc":"https://visibilityspots.github.io/blog/cloudcollab-amsterdam.html"},{"title":"Git server","text":"For some of my development projects I'm using git repositories because of the flexibility of it. But the initial beta phase I don't want to keep private until I created something working. Normally I use github.com repositories for them, a good service except you have to pay for private repositories. So I searched the internet for private alternatives and installed gitlab on my CentOS 6 machine. It worked fine, but it was a bit of an overkill to manage about 10 repositories for only one user, myself. So I decided to migrate it back to the essence. The essence as: the command line git server with a nice web interface on top of it to have a quick overview of the changes made in which repositories. I based my git server setup on the git-scm tutorial after reading the chapter about the git-server . It a clear and detailed explanation of the different steps to configure your own private git server. Once the server was running and I could create new repositories, clone them and push to them from the outside I looked for a nice web frontend. My first choice was the git-web interface with lighttpd as the backend web service. The installation of the gitweb service could also been found on git-scm. For the lighttpd configuration I created a virtualhost pointing to the gitweb directory in /var/www/gitweb/. /etc/lighttpd/vhosts.d/gitweb.conf: $HTTP[\"url\"] =~ \"&#94;/gitweb/\" { setenv.add-environment = ( \"GITWEB_CONFIG\" => \"/etc/gitweb.conf\" ) url.redirect += ( \"&#94;/gitweb$\" => \"/gitweb/\" ) alias.url += ( \"/gitweb/\" => \"/var/www/gitweb/\" ) cgi.assign = ( \".cgi\" => \"\" ) server.indexfiles = ( \"gitweb.cgi\" ) debug.log-request-header = \"enable\" } I used this interface for quite some time, but recently I found out about gitalist , a more modern approach to give and overview of your git repositories. Gitalist is available as a perl-cpan module and could also been installed as such on a CentOS 6 server: # cpan -i Gitalist Until today I didn't got enough time to get it fully up and running, mainly because I already have something working so it's not that high on my priority list :) Resources: git-scm git web-interfaces","tags":"linux","url":"https://visibilityspots.github.io/blog/git-server.html","loc":"https://visibilityspots.github.io/blog/git-server.html"},{"title":"Dashing","text":"Using multiple nice interface dashboards to get an overview of your services is a great thing. But navigating to them all separately could sometimes be rather pain full. Therefore I looked for some central place to give a broad overview of all of them. During last year many passed through during my search on the internet. The 2 most interesting ones where team dashboard and dashing . Team dashboard is a promising one which could gather extremely specific data and give those back in some nice graphics. That way you could create your own very specific dashboard with all graphics and measurements in the same theme/layout on one central page. But I was looking for something more simpler and that's what I found with dashing . By using some custom jobs and views I gathered data from icinga , jenkins , foreman & bacula . As you can see the square's are showing the total amount of checks from the different dashboard services, if there is one check failing the square of the service will change to a red blinking background. If everything is alright (as it should be) the square is green. To achieve this I have implemented some checks I found on the internet and wrote some myself: First row icinga-checks foreman-overview jenkins-jobs bacula-state The first three are using the simplemon widget available in the dashing-scripts repo from roidelaplui Second row web-services jenkins-build-progress foursquare-checkins tomtom For the tomtom check the api explorer and lat-lon coordinates which can be a real help to configure this check. It's also real easy to configure a raspberry pi which you can connect to a screen using hdmi. Therefore I suggest screenly which can iterate through a list of assets like web pages (your custom dashing screen ;), images and videos. That way you could afford a cheap and brilliant monitor screen! Keep an eye on it ;)","tags":"linux","url":"https://visibilityspots.github.io/blog/dashing.html","loc":"https://visibilityspots.github.io/blog/dashing.html"},{"title":"LinuxCon Edinburgh","text":"I got a great opportunity by attending LinuxCon in Edinburgh. Will try to share my experiences there in this blogpost by listing the topics and people I found interesting so perhaps others could take also advantage of it. Topics Apache mesos a cluster manager that provides efficient resource isolation and sharing across distributed applications or frameworks. Zuul a pipeline oriented project gating and automation system. LTTng The LTTng project aims at providing highly efficient tracing tools for Linux. Its tracers help tracking down performance issues and debugging problems involving multiple concurrent processes and threads. Tracing across multiple systems is also possible. SAF OpenSAF is an open source project focused on Service Availability (SA) that goes beyond High Availability (HA) requirements. The goal of the OpenSAF project is to develop middle ware based on open and industry standard interfaces for applications requiring uninterrupted 24x7 service. The phoenix project A novel about IT, Devops and helping your business win. Buildbot Buildbot is a continuous integration system designed to automate the build/test cycle Opendaylight OpenDaylight's mission is to facilitate a community-led, industry-supported open source platform, including code and architecture, to accelerate adoption of Software-Defined Networking and Network Functions Virtualization. hydrogen: http://www.opendaylight.org/news/2013/09/converge-network-digest-opendaylight-hydrogen-open-source-sdn Blackduck Offers some services about open-source licensing Interesting people: Colin Charles Gave an entertaining talk about databases solutions in the cloud, with neat comparisons using relevant pro's and contra's between the different cloud based database providers without being boring just listening facts and numbers. James Bottomley A true container based cloud believer who inspired almost everyone in the room to use containers for their virtual appliances instead of the traditional virtual machines. David Nalley Gave an interesting presentation walk through how you could use cloudstack for a development environment. Mikko Hypponen In times as today where the whole NSA topic is as hot as hell this guy talked about surveillance and got a great response from the audience talking as a keynote.","tags":"conferences","url":"https://visibilityspots.github.io/blog/linuxcon-edinburgh.html","loc":"https://visibilityspots.github.io/blog/linuxcon-edinburgh.html"},{"title":"CPAN rpm packages","text":"I went crazy from perl and the installation of their modules. For some icinga checks we need to install a few base perl packages using cpanminus . It's taking a long time before the installation succeeds depending on the internet connection or server specifications. Using a puppet exec to automate this installation is frustrating because the timeout is unpredictable and could take hours from time to time! So I started to look for a way to package it into an rpm which I can distribute over our own yum repository. The first software I got reviewed is cpan2rpm , it looked promising. You could give a text file containing the names of the modules to package. That way I could use a git repo containing this file which triggers an automated jenkins job which creates the packages and uploads them to the repo. Unfortunately it doesn't package the cpanminus module. So I had to look further. Last week I got the solution by cpanspec , a piece of software I read about on nailingjelly 's blogpost. And yes, I achieved to package it. Installation & configuration of the required tools: $ sudo yum install rpmdevtools perl perl-devel perl-Test-Base $ sudo curl -L http://cpanmin.us | perl - --sudo App::cpanminus $ sudo /usr/local/bin/cpanm CPAN::DistnameInfo $ sudo yum install cpanspec $ cd ~ $ rpmdev-setuptree Create spec file and source rpm from a cpan module: $ cpanspec --follow --srpm CPAN::Module --packager YOURNAME Install the source rpm to create a package from it using the new generated spec file: $ rpm -i name-of-module.src.rpm You should see there is a SPEC file generated in the rpmbuild tree: $ cd ~/rpmbuild/SPECS $ vim cpan-module-name.spec Finally give it a shot and build a fresh rpm package: $ rpmbuild -ba cpan-module-name.spec The first time trying to build App::cpanminus I had to add some missing file declarations to the spec file. Spawning the error: RPM build errors: Installed (but unpackaged) file(s) found: /usr/bin/cpanm /usr/share/man/man1/cpanm.1.gz So I added the 2 unpacked files to the %files section: %files %defattr(-,root,root,-) %doc Changes cpanfile LICENSE META.json README %{perl_vendorlib}/* %{_mandir}/man3/* /usr/bin/cpanm /usr/share/man/man1/cpanm.1.gz Running the rpmbuild now resulted in a fresh rpm: $ ls ../RPMS/noarch/ perl-App-cpanminus-1.7001-1.el6.noarch.rpm I installed the rpm on a development system and successfully installed a perl module with the cpanm command afterwards: $ yum localinstall name-of-the-module.rpm So from now on our servers are hooked up with those create packages distributed by our own yum repository. And the whole initialization process of a fresh server gained in time and therefore in efficiency in our environment this way! Resources: nailingjelly man cpanspec Centos.org wiki","tags":"linux","url":"https://visibilityspots.github.io/blog/cpan-rpm-packages.html","loc":"https://visibilityspots.github.io/blog/cpan-rpm-packages.html"},{"title":"Taskwarrior","text":"I've used a lot's of tools to get a grip on my todo lists for work, for the scouting movement, for technical projects, household, etc. Started by using pen and paper, switched to a little notebook (which I still use for short-term tasks) to start using software to organize them. I've used evernote, gtasks, tracks, github issues, gitlab issues, redmine tickets, in short plenty passed by only tracks survived. I still use it for my work related projects, everyday at 8:30AM I get my list of tasks for that day. That way I have some sort of control on my projects. Nevertheless there was still some sort of missing feature, an integration with the other issue trackers I use like github and redmine for example. I dreamed of one central overview of all my tasks/issues/projects. And some weeks ago I just stumbled into the solution of that dream, taskwarrior will organize my life from now on. It's a nifty command line based piece of software with all the features I needed, due dates, projects, tags, customized reports, etc. I completely get enthusiastic when finding out the bugwarrior module from Ralph Bean which let you to import tasks from many different services like github, redmine & trac. So I started on this new project by adding a new task to my tracks instance: \"Migrate to taskwarrior\". Installation of the task service # yum install task By following the 30 sec tutorial you get an idea of the basics, but for a full experience and howto I recommend reading the full tutorial . I created a dedicated user for managing my todo list on my CentOS 6.4 machine. Configuration of the task service is done in the ~/.taskrc file where you can change the data & log files locations, setting a theme a other configuration parameters. Installation of task-web , a nice and clear frontend (make sure to use ruby 1.9.3, I had performance issues when using ruby 2.0.0): # gem install taskwarrior-web thin $ task-web -s thin -L & I added the task-web.user & task-web.passwd parameters to my ~/.taskrc file for basic http authentication, and opted for the thin webserver rather than the default webrick when using the task-web frontend. Once you've stared the service your instance should be accessible on http://your.ip.of.the.server:5678 in your web browser. (make sure to open the port in your servers firewall) You can choose your own port by adding the option -p XXX in your command (task-web -s thin -p XXX -L &). All the options are listed in the help menu (task-web --help). Installation of bugwarrior : As mentioned before the biggest advantage of using taskwarrior to me is the import feature of some several third party services. It's easy to install by using the pip installer : $ pip install bugwarrior After that you can configure the ~/.bugwarriorrc file to your needs. After some struggling I got it working with the great help of the developer Ralph Bean. Example of my ~/.bugwarriorrc file: [general] targets = github, redmine log.level = INFO log.file = /var/log/tasks/bugwarrior.log bitly.api_user = USERNAME bitly.api_key = API-KEY multiprocessing = True [notifications] notifications = False [github] service = github username = USERNAME default_priority = M login = USERNAME passw = PASSWORD [redmine] service = redmine url = https://redmine.url key = REDMINE-API-KEY user_id = NUMERIC-USER-ID project_name = NAME-OF-THE-PROJECT-TASK-WILL-GET-ON-IMPORT Once configured you can run the server and check the log's: $ bugwarrior-pull $ cat /var/log/tasks/bugwarrior.log $ task list Once you initialized the import you can create a cronjob for it: $ crontab -e # Bugwarrior import 30 5 * * * /usr/bin/bugwarrior-pull That way every day at 5:30AM the tasks from 3Th party services will be imported. The only feature I'm still missing is a 2 way synchronization. So I can edit the tasks in taskwarrior too, but that's something for utopia :) Conky monitoring: Is a already wrote about before I'm using conky as a dashboard together with my ratpoison setup. I already wrote a script to fetch my tracks issues . But now I need to fetch my task list from taskwarrior. So I created a custom task report configured in my ~/.taskrc file: # Custom reports report.conky.description=Conky report report.conky.columns=project,description.truncated,depends.indicator,priority report.conky.labels=Project,Desc,D,P report.conky.sort=due+,project+,priority+ report.conky.filter=status:pending limit:page Using a ssh connection you can then fetch the output from the command 'task conky' and parse it into a file using a bash script. Because all my project definitions containing a hyphen I can parse them so I can grep titles and create new lines so I can parse them using the conky syntax. I do still have 2 things I need to investigate time into: Mail weekly tasks Using taskreport but I got some errors after installing using 'pip install taskreport': $ taskreport File \"/usr/bin/taskreport\", line 51 for key in ['userName', 'server', 'port']} &#94; SyntaxError: invalid syntax Installation of taskd server (for synchronization with mirakel): Until today the mirakel app always crashes when trying to sync after initialized with the created key. # git clone git://tasktools.org/taskd.git # wget http://pkgs.repoforge.org/rpmforge-release/rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm # rpm -Uvh rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm # yum install cmake28 # yum install gnutls-devel # yum install libuuid-devel # cmake28 . # make # make install # yum install gnutls-utils # find and replace gnutls-certtool with certtool # cd pki # ./generate # add_user.sh script","tags":"linux","url":"https://visibilityspots.github.io/blog/taskwarrior.html","loc":"https://visibilityspots.github.io/blog/taskwarrior.html"},{"title":"Upgrade to puppet 3.3.0","text":"I finally got to the point where I upgraded a whole puppet infrastructure from puppet 2.6.x to the last stable version of puppet, 3.3.0 . And after finding out the way to go it was surprisingly easy and no big issues came across. One of the main reasons to upgrade was to start using the latest version of foreman, were we used 0.4, so we can start provisioning our own development vm's with some fancy cloud solution like for example cloudstack using our production puppet tree. Before the upgrade we had the puppet-client & server (2.6.18), puppetdb (1.4), (ruby 1.8.7) and foreman (0.4.2) running on a CentOS 6.3 machine. After upgrading we are running puppet-client & server (3.3.0) puppetdb (1.4), ruby (1.8.7) and foreman (1.2) all managed by puppet itself. (feels quite satisfying ;) ) The very fist time I started upgrading the puppet master, but instead of upgrading the puppet-server package from the yum puppetlabs repository I upgraded only the agent. After I figured that out I could kill myself but ran out of time so needed to stop the process. The second time I started totally in the wrong direction. I started with foreman, read about needing ruby 1.9.3. So I started looking for a CentOS 6.3 ruby 1.9.3 package. Didn't find any started compiling it from source, but that came out on a total mess so I reverted my upgrade and postponed it for some days. The final 3Th time I started in the right order. This order I will describe here: (Before all those steps, make sure to disable puppet on your clients to have more control during the process) Configuring the puppetlabs repository I like to install software from packages, so I started by configuring the puppetlabs repository. I use a puppet-repo module for configuring repo's on our machines but you can quite easy install it from the command line. This command is executed on a Cent0S 6.3 x86_64 machine: # rpm -ivh http://yum.puppetlabs.com/el/6.0/products/x86_64/puppetlabs-release-6-7.noarch.rpm Upgrading the puppetmaster So after shamelessly updated only the puppet package the first time, this time I did upgrade the puppet-server package without any issue. Be sure to read the docs first about upgrading! # yum update puppet-server Once the puppetmaster is updated we can try our first puppet runs against the upgraded version. Start a native puppet master process for testing Before I get further in our upgrade process on passenger and stuff I wanted to know if the client is still able to do a puppet run without the passenger setup. So I had to start the puppetmaster as a daemon, did a local puppet noop run on the master itself and stopped the puppetmaster daemon after I checked the run. # puppet resource service puppetmaster ensure=running enable=true # puppet agent --test --noop # puppet resource service puppetmaster ensure=stopped enable=true Upgrade the passenger setup We are using a passenger setup to have our puppet master in a scalable setup. Therefore we also needed to upgrade passenger on our puppetmaster and adopt the puppetmaster vhost to the upgraded environment. To accomplish this I simply followed the passenger documentation of puppetlabs which was quite easy to follow. Client Once the puppetmaster was upgraded I tested a puppet run with a still not updated client against the upgraded puppetmaster. It did the job except from sending reports. Since I planned to upgrade the clients too I did not invest time into this issue. There fore I just upgraded the client itself where the puppetlabs repository already was enabled: # yum update puppet Issues 403: authentication error By running my first 3.3.0 client vs the 3.3.0 master I got an authentication error 403 forbidden request. Did some research on the net, and found about an issue in the puppetmaster's auth.conf file. Once I added this to the file: # allow nodes to retrieve their own node definition path ~ &#94;/node/([&#94;/]+)$ method find allow $1 The run did what I had to do configuring the server by using puppet! undefined method 'symbolize' On some clients I got this error message when trying to run puppet. On somethingsinistral.net I found out it had to see with multiple puppet versions on your machine. By looking into the installed gems (make sure to check also possible rvm environments) and cleaned the ancient ones out I got the puppet run up and running again. icinga check_puppet We are using ripienaar's icinga check_puppet to monitor the puppet functionality. The became all red indicating puppet had too long not ran on the server. In the troubleshooting process I figured out the nagios user which is running the check over the NRPE protocol wasn't able to read the /var/lib/puppet/state/last_run_summary.yaml file. By checking permissions I found out the default settings of the /var/lib/puppet directory are 0750 when installing puppet. Once I've changed them to 755 all check's became green again! Foreman Once the puppet master was running fine again I also upgraded theforeman service running on the same machine as the puppetmaster. This went smoothly once I figured out the ruby and rake commands in the documentation must be replaced with ruby193-rake/ruby193-ruby when installed foreman from their repository. Also do not forget to install foreman-mysql / foreman-sqlite etc when using those extra features.","tags":"puppet","url":"https://visibilityspots.github.io/blog/puppet-3-upgrade.html","loc":"https://visibilityspots.github.io/blog/puppet-3-upgrade.html"},{"title":"Command line printing & scanning","text":"Since I discovered the joy of using the ratpoison window manager I'm trying to do all tasks I need to perform on my system using the command line. One of those frequently used tasks is printing out documents or scanning in files. Until today I used the software viewer of my documents to print and simple-scan to scan my files. Nowadays I use the command line to perform those tasks. To print out documents I use the lp command: \"Get the status off all printers on your system\" $ lpc status all \"Print the desired file to a specific printer\" $ lpr -P PRINTERNAME FILE/TO/PRINT.XX \"Show the printing queue\" $ lpq -P PRINTERNAME \"Cancel a specific print job using the queue id\" $ lprm ID \"Cancel all printing jobs\" $ lprm - Those are the commands I regularly use to print my documents. For scanning I use scanimage from sane. There are too many options to explain so I just give hereby the one I use to scan A4 formatted files to pdf: \"List your scan devices\" $ scanimage -L \"Scan the image to a pdf file\" $ scanimage -p > fileName.pdf Off course there are many ways to perform those tasks using the command line. Those are only the ones I use on my fedora machine. I'm always open for suggestions!","tags":"linux","url":"https://visibilityspots.github.io/blog/printing-scanning.html","loc":"https://visibilityspots.github.io/blog/printing-scanning.html"},{"title":"CentOS 6.4 software raid & LVM","text":"Been asked to setup a software raid of 12TB on a minimal CentOS 6.4 installation with 5 disks of 3TB each. Never played with raid nor lvm before so the challenge was great! I started by doing research about RAID . Came to the conclusion that RAID 5 was the best option for our purpose. So kept looking for a way to implement a software raid and stumbled into mdadm . Using the information of Richard 's and Zack Reed 's blogs I easily setted up the raid array and created some lvm volumes on top of that. Creating of 3TB partitions on the physical disks # parted /dev/sdX # (parted) mklabel gpt # (parted) unit TB # (parted) mkpart primary 0.00TB 3.00TB # (parted) print Creating the raid5 array with all the prepared disks # mdadm --create /dev/md0 --level=raid5 --raid-devices=5 /dev/sdX# /dev/sdX# /dev/sdX# /dev/sdX# /dev/sd# Viewing the state of creation of the new array # watch cat /proc/mdstat Once the array is successfully created you have to store it into the config file # echo \"DEVICE partitions\" > /etc/mdadm.conf # echo \"MAILADDR root@localhost\" >> /etc/mdadm.conf # mdadm --detail --scan >> /etc/mdadm.conf so we can start creating lvm volumes on top of them # pvcreate /dev/md0 # vgcreate vg_NAME /dev/md0 # lvcreate --name lv_NAME -l 100%FREE vg_NAME We now created a physical volume, a volume group and a logical volume which can easily be resized and moved on top of the raid5 setup To start using the volume we finally have to create a file system on it and check if everything went alright # mkfs.ext4 /dev/vg_NAME/lv_NAME # fsck.ext4 -f /dev/vg_NAME/lv_NAME After the succesfull file system chack I came to a working raid setup. Nevertheless I figured out that by rebooting the machine the raid array wasn't initializing as I expected. Instead of the md0 as configured the raid array was coming up as a read-only one named md127. I did some research and found a usefull topic on it on the redhat bugzilla forum. There I found out that this can be reactivated manually by stopping the read-only instance and reassambling the array based on the /etc/mdadm.conf file: # mdadm --stop /dev/md172 # mdadm --assemble --scan Although that's not really the best solution because you have to do this by every reboot of your system. So I looked a bit further and found out you can regenerate your initramfs image using your mdadm.conf file using dracut: # mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.old # dracut --mdadmconf --force /boot/initramfs-$(uname -r).img $(uname -r) Once that images is build I successfully rebooted the system and the md0 came up without any problem. The final step is to get the new logical volume mounted automatically at boot, therefore you have to add something in your /etc/fstab file: # /dev/mapper/vg_NAME-lv_NAME /var/NAME ext4 defaults 1 1 Some useful commands ## Stop raid array # mdadm --stop /dev/md0 ## Start raid array # mdadm --assemble --scan Resources: tcpdump: removing tcpdump: restarting cheat sheet raid states howtoforge initramfs","tags":"linux","url":"https://visibilityspots.github.io/blog/raid.html","loc":"https://visibilityspots.github.io/blog/raid.html"},{"title":"Hubot, the github chat automated bot","text":"Some weeks ago I was asked by a customer to implement a bot on an IRC channel. Did some research about this topic and stumbled on the github hubot . The installation on a dedicated server running CentOS 6, using the irc adapter isn't hard. By following those steps you can easily start your own bot on a specified IRC channel. You need some pre installed packages: # yum install openssl openssl-devel openssl-static crypto-utils expat expat-devel gcc-c++ git After installed those pre requirements nodejs is the next service we need. You can install the newest version using rpm packages you can find on the internet. For example on my repo or building it from source: $ wget http://nodejs.org/dist/v0.8.17/node-v0.8.17.tar.gz $ tar xf node-v0.8.17.tar.gz -C /usr/local/src && cd /usr/local/src/node-v0.8.17 $ ./configure && make && make install As you can see in the output, npm is installed into the '/usr/local/bin/' directory. To get this working in bash you could add this directory into your $PATH environment $ PATH=$PATH:/usr/local/bin/ So now we can use npm to install hubot and coffee-script: $ npm install -g hubot coffee-script You could now create your very own dedicated hubot by declaring the necessary files into your preferred path: $ hubot -c /opt/hubot/ That way the core hubot you can use is installed in its own directory. We now have to install and configure the irc-adapter . Therefore you need to adapt the package.json file in your newly created hubot folder (/opt/hubot/) by inserting the hubot-irc dependency: \"dependencies\": { \"hubot-irc\": \">= 0.0.1\", \"hubot\": \">= 2.0.0\", ... } Once that's done you can install the dependencies by using npm: $ npm install Last thing you have to do is configure the needed irc parameters. This can be done by exporting the environment parameters. I decided to use a file to accomplish this. In the /opt/hubot/ directory I created a hubot.env file containing the necessary parameters: # IRC adapter parameters export HUBOT_IRC_NICK=\"NAMEOFYOURBOT\" export HUBOT_IRC_ROOMS=\"#CHANNELONIRC\" export HUBOT_IRC_SERVER=\"irc.freenode.net\" export HUBOT_IRC_DEBUG=\"false\" export HUBOT_IRC_UNFLOOD=\"false\" export HUBOT_IRC_SERVER_FAKE_SSL\"false\" export HUBOT_IRC_USESSL\"false\" Most of those params are quite obvious, the unflood param configured to false prevented the hubot to crash when someone asked hubot: help ;) After I tested this standard setup out I started to write a puppet-hubot module to automate those steps and configuration on a CentOS machine. Using a the puppet-nodejs module which installs the nodejs rpm I packaged on my visibilityspots repo the installation become easy peasy. By using this puppet setup a hubot init script is automatically deployed so a hubot init service can be used for starting, stopping, restarting and getting the status of the hubot service on your dedicated machine. As you can see in the init script I use a hubot user to run the hubot. That way it's a bit more secure to run the hubot service on your server. A 2nd script which is deployed using the puppet-hubot module is the hubot-plugin.sh script. By using this script you can automatically install a script from the hubot scripts catalog . If the author of the script uses the standard documentation rules the scripts will declare it self in your hubot-scripts.json file, declaring it's dependencies in the package.json file, if there are adding it's needed configuration parameters in plugins.env and restarting the hubot service. If you notice a script which hasn't been documented the standard way, you can easily use pull requests, the author of the github hubot-scripts repository really takes it serious and merge those requests on a regular base. Last but not least I also created a hubot instance using the xmpp-adapter . After some desperate debugging and failing I figured out that for the irc adapter it doesn't matter which nodejs version you installed. For the xmpp adapter on the other hand it only worked by installing nodejs v0.8.17 build from the sources and by never ever use npm update but npm install instead to install the npm dependencies. It saves you a lot of time if you take that tip in memory :) Enjoy using your hubot","tags":"linux","url":"https://visibilityspots.github.io/blog/hubot.html","loc":"https://visibilityspots.github.io/blog/hubot.html"},{"title":"Chromium eid","text":"During this time of the year in Belgium most people needs to fill in their taxes forms. Since a few years the Belgium government provided an electronic way to accomplish this task. Using a digital passport you can authenticate yourself. I wanted to use this nice tool so I had to configure my local setup to have it all glued together on my linux machine. The necessary steps I described in this post so other interested people can use their linux setups also to fill in the tax forms. Installation The mayor package to install on a fedora machine is the eid-mw package: $ wget https://eid-mw.googlecode.com/files/eid-mw-4.0.0-0.925.fc16.x86_64.rpm $ sudo rpm -Uvh eid-mw-4.0.0-0.925.fc16.x86_64.rpm If you are using archlinux on a dell latitude e6530 you can use the internal card reader by installing the drivers of the Common Access Card $ sudo pacman -S pcsclite $ sudo vim /etc/opensc.conf In the opensc.conf file you need to uncomment the setting enable_pinpad = false on two places before you enable the process at boot and run it: $ sudo systemctl enable pcscd $ sudo systemctl start pcscd So you could install the eid-mw package from the AUR repository $ yaourt eid-mw Once you've installed the eid-mw package on fedora and configured the pcscd service on archlinux you could install the firefox eid addon if you are using the firefox browser. Once that's accomplished you could test if it all works using the test page provided by the Belgium government. You can also use the eid-viewer package, which provides you with a graphical piece of software so you can read out your passport, printing it out. Testing your pin code (if you forgot you're pincode you still have to go to your town services. For fedora $ wget https://eid-viewer.googlecode.com/files/eid-viewer-4.0.0-0.52.fc16.x86_64.rpm $ sudo rpm -Uvh eid-viewer-4.0.0-0.52.fc16.x86_64.rpm For archlinux install the eid-viewer package $ yaourt eid-viewer Once the installation finished successfully you can run the software to view the information of your passport $ eid-viewer Still I'm not using firefox but the chromium-browser to accomplish than I had to add the eid interface into the chromium security settings. I found an explanation on google code and copied those steps into this post to be completed. $ # only for fedora install nss-tools $ sudo yum install nss-tools $ killall chromium-browser or $ killall chromium $ cd $ modutil -dbdir sql:.pki/nssdb/ -add \"Belgium eID\" -libfile /usr/lib/libbeidpkcs11.so.0 $ modutil -dbdir sql:.pki/nssdb/ -list So if you now start your chromium browser you could test if it all works on your machine too :) Troubleshooting Since I only use this eid once a year and my system evolves in the meantime by installing rolling updates obviously issues arise.. modutil: function failed: SEC_ERROR_LEGACY_DATABASE: The certificate/key database is in an old, unsupported format. To solve his one I had to recreate my key database $ mv .pki/nssdb .pki/nssdb.BAK $ mkdir .pki/nssdb $ certutil -N -d .pki/nssdb/ $ modutil -create -dbdir .pki/nssdb/ $ certutil -L -d .pki/nssdb/ $ modutil -dbdir sql:.pki/nssdb/ -add \"Belgium eID\" -libfile /usr/lib/libbeidpkcs11.so.0 $ certutil -L -d .pki/nssdb/ -h \"all\" $ certutil -L -d .pki/nssdb/ When I achieved doeing so I could go ahead once again and fill in my taxes. Resources: eid-belgium","tags":"linux","url":"https://visibilityspots.github.io/blog/chromium-eid.html","loc":"https://visibilityspots.github.io/blog/chromium-eid.html"},{"title":"Ratpoison window manager","text":"My first steps in linux where on a ubuntu distribution, when you could order the ISO images on a cd-rom delivered by the post. I liked it a lot and ever since I only used linux on my home based devices. Following the releases of Ubuntu. Starting at inuits I tried something else and installed CentOS desktop on my laptop. The idea behind this was to gain experience on the CentOS distributions. Once I figured out that it didn't made sense since a laptop has other purposes then a server. By the time we got new machines I decided to install fedora on it. Nevertheless I don't like the gnome 3 unity layer. It's not that it's bad, But I just don't like it. So I started by installing mate-desktop . By playing around and looking how other people are configuring and using their local machines a colleague pointed me to ratpoison . Because I could install this window manager nicely next to the existing mate-desktop I gave it a try. Shameful I have to admit that when I first gave it a try I thought I did something wrong on the installation. That installation is not that hard on fedora, since they packaged it in their own repository: $ sudo yum install ratpoison Once installed you can logout and try to log in after changed your window manager. For me that first introduction was like I already mentioned a bit shameful. Nothing happened, I only saw a black background and couldn't do anything. It took me some time to figuring out that you had to use a keyboard pre configured strike to get started using the ratpoison functionality. And by default that is CTRL-T. So if you for example tap in CTRL-T and then SHIFT-V you should see the ratpoison version disclaimer. Once I figured that out I started to configure the whole environment for my needs. After some try and error I finally became in love with it. My screen movements are a lot faster by moving around screens, windows and applications trough my keyboard without physical moving my hands! The configuration is done in the ~/.ratpoisonrc file: # Ratpoison configuration startup_message off set winname class # Desktop set padding 0 0 0 93 exec conky -c ~/.conky/conkyrc feh --bg-scale ~/path/to/background/picture.png exec xscreensaver -nosplash To begin I disable the startup message which only says what your keystroke is. Then I configure my desktop, setting a padding at the bottom of my screen so my conky setup is displayed smoothly on my screen. Starting the conky daemon, setting a background picture and starting the xscreensaver daemon. # Startup programs exec dropbox start exec dropbox2 exec /home/Jan/.scripts/fnotify -s exec /home/Jan/.scripts/ratcpi exec /home/Jan/.scripts/detectPhone The second part of my .ratpoisonrc file is the file with my startup scripts. To start my dropbox scripts as explained on a previous post , the fnotify script to display irssi notifications, ratcpi for displaying battery notifications and detectPhone which looks for my phone by bluetooth to decide to lock my laptop yes or no. # Aliasses alias showroot exec ratpoison -c $HOME/.rpfdump; ratpoison -c 'select -' -c only alias unshowroot exec ratpoison -c \"frestore at $HOME/.rpfdump\" alias showpadding set padding 0 0 0 93 alias showfullscreen set padding 0 0 0 0 alias term exec terminator # Bindings unbind n unbind c unbind s unbind Q ## Software bindings bind d exec chromium-browser bind c exec terminator bind C-c exec terminator bind l exec xscreensaver-command -lock bind C-s exec spotify bind s exec synapse ## Move bindings bind C-k delete bind r restart bind n nextscreen bind C-n nextscreen bind b showroot bind B unshowroot bind p showpadding bind f showfullscreen bind v hsplit bind h vsplit bind q only This last part is about setting my key stroke bindings. Most of them are self explaining, all those keys need a pre hit of CTRL-T before called.","tags":"linux","url":"https://visibilityspots.github.io/blog/ratpoison.html","loc":"https://visibilityspots.github.io/blog/ratpoison.html"},{"title":"Conky-colors","text":"Back in the days I once wrote a blogpost about a conky setup on a Ubuntu desktop. In the meantime I'm not using ubuntu anymore and kinda tweaked my whole conky setup. I switched to fedora 18 and using conky-colors those days in front of the ratpoison window manager. This post will go trough all the steps I did to came to the actual result. When something isn't clear, or could be done on a more smoother/better way, please feel free to bug me about it! Installing some required packages before actually compiling the conky-colors source: $ sudo yum install hddtemp curl lm-sensors conky Next thing is to scan your local machine for all available sensors (answering yes on all questions): $ sudo sensors-detect Once the pre requirements are successfully installed on your system we can start compiling the source of conky colors: $ cd into/unzipped/source/directory $ make $ sudo make install Once that is done we can start by actually generating your desired conky setup. I based my setup on the SLIM theme and adopted it afterwards. Your custom weather code is based on the yahoo service. $ conky-colors --slim --w=1440 --h=900 --theme=white --weather=brxx0043 Two important files will be generated by this command. One will be located in your homedir and should be named like ~/.conkycolors/conkyrc. This is the config file we need to call when actually starting the conky daemon afterwards. Some configuration parameters to your specific layout can be adopted. I changed some of those params in the conkyrc file, so the conky bar is located at the bottom om my screen (dell latitude E6530) with yellow colors and a border around it. own_window_type override alignment bottom_left gap_x 5 gap_y 5 minimum_size 1910 80 maximum_width 1910 80 draw_borders yes color0 white color1 CE5C00 color2 white color3 CE5C00 The command to run the conky daemon: $ conky -c ~/.conkyrc $ CTRL-C to stop the job # Move the conky job in the background $ conky -c ~/.conkyrc & We now should have a standard conky-colors theme running. But it wasn't sufficient for me. I kinda tweaked the whole setup and added some additional features: All those tweaks were added to the conkySlim.lua file: $ sudo vim /usr/share/conkycolors/scripts/conkySlim.lua The fist tweak is the linux logo at the beginning of the conky screen (using the OpenLogos font): --LOGO COLUMN settings = {--VARIA txt='Z', x=50 , y=95 , txt_weight=0 , txt_size=100 , txt_fg_colour=theme , txt_fg_alpha=fga , font=\"OpenLogos\" };display_text(settings) The second tweak is about the 2nd column in the conky output. I'll start with the uptime and update config which are quite straightforward: --SECOND COLUMN settings = {--UPTIME TITLE txt=conky_parse(\"Uptime: \"), x=100 , y=20 , txt_weight=0 , txt_size=12 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) settings = {--UPTIME TITLE txt=conky_parse(\"${uptime}\"), x=160 , y=20 , txt_weight=1 , txt_size=12 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) settings = {--UPDATES TITLE txt=conky_parse(\"Updates: \"), x=100 , y=35 , txt_weight=0 , txt_size=12 , txt_fg_colour=fgc , txt_fg_alpha=fga , };display_text(settings) updates = conky_parse(\"${execi 360 yum -e0 -d0 check-update | wc -l}\") if updates > '9' then color = theme weight = '1' message = 'available' xAs = '175' elseif updates > '0' then color = theme weight = '1' message = 'available' xAs = '180' else color = fgc weight = '0' message = '' xAs = '175' end settings = {--# UPDATES txt=updates, x=160 , y=35 , txt_weight=weight , txt_size=12 , txt_fg_colour=color , txt_fg_alpha=fga , };display_text(settings) settings = {--UPDATES MESSAGE txt=message, x=xAs , y=35 , txt_weight=0 , txt_size=12 , txt_fg_colour=fgc , txt_fg_alpha=fga , };display_text(settings) Following with my own irssi status based on an html file which is generated on the server where the irssi daemon is running using the irssi script away2web . The username and password in this example are needed for basic htaccess authentication: irssiState= conky_parse(\"${exec curl --user USERNAME:PASSWORD https://URLTOGENERATEDAWAY2WEBFILE/status.html -k -s | head -1}\") if irssiState == '1' then color = theme message = '' state = 'Online' else color = theme message = conky_parse(\"${exec curl --user USERNAME:PASSWORD https://URLTOGENERATEDAWAY2WEBFILE/status.html -k -s | tail -1}\") state = 'Offline ' end settings = {--IRSSI TITLE txt='Irssi:', x=100 , y=51 , txt_weight=0 , txt_size=12 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) settings = {--IRSSI STATE txt=state, x=160 , y=51 , txt_weight=1 , txt_size=12 , txt_fg_colour=color , txt_fg_alpha=fga , };display_text(settings) settings = {--IRSSI MESSAGE txt=message, x=207 , y=51 , txt_weight=0 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) And as last informational message in this column a minimal overview of your infrastructure based on an icinga instance based on a conky-icinga bash script: --ICINGA STATE IcingaState=conky_parse(\"${execpi 53 PATH/TO/conky-icinga.sh}\") if IcingaState == 'OK' then color = fgc elseif IcingaState == 'WARN' then color = fgc else color = fgc end settings = {--ICINGA TITLE txt='Icinga:', x=100 , y=80 , txt_weight=0 , txt_size=12 , txt_fg_colour=fgc , txt_fg_alpha=fga , };display_text(settings) settings = {--ICINGA STATE txt=IcingaState, x=160 , y=80 , txt_weight=1 , txt_size=12 , txt_fg_colour=color , txt_fg_alpha=fga , };display_text(settings) As you can see in the middle section I added a counter for incoming mails based on my maildir folders: settings = {--MAILS txt=conky_parse(\"Inuits: ${new_mails PATH/TO/MAILDIR}\"), x=(w/2)-160 , y=65 , txt_weight=0 , txt_size=12 , txt_fg_colour=fgc , txt_fg_alpha=fga , };display_text(settings) Depending if the spotify service is running conky will display the 'Now playing song - artist': settings = {--SPOTIFY MUSIC SYMBOL txt=conky_parse(\"${if_running spotify}z${endif}\"), x=(w/2)+60 , y=83 , txt_weight=0 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , font=\"musicelements\" };display_text(settings) settings = {--SPOTIFY txt=conky_parse(\"${if_running spotify}${exec sudo spotify-nowplaying}${endif}\"), x=(w/2)+67 , y=83 , txt_weight=0 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) The same counts for cmus a command line music player which will show 'Now playing song - artist when active' using a very basic script settings = {--CMUS MUSIC SYMBOL txt=conky_parse(\"${if_running cmus}z${endif}\"), x=(w/2)+60 , y=83 , txt_weight=0 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , font=\"musicelements\" };display_text(settings) settings = {--CMUS txt=conky_parse(\"${if_running cmus}${execi 2 ~/PATH/TO/cmus.sh}${endif}\"), x=(w/2)+67 , y=83 , txt_weight=0 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) 4 cpu's will be used to draw the CPU graphics and showing the load of the local machine: settings = {--CPU GRAPH CPU1 value=tonumber(conky_parse(\"${cpu cpu1}\")), value_max=100 , x=xp , y=yp , graph_radius=22 , graph_thickness=5 , graph_start_angle=180 , graph_unit_angle=2.7 , graph_unit_thickness=2.7 , graph_bg_colour=bgc , graph_bg_alpha=bga , graph_fg_colour=theme , graph_fg_alpha=fga , hand_fg_colour=theme , hand_fg_alpha=0.0 , txt_radius=35 , txt_weight=1 , txt_size=8.0 , txt_fg_colour=fgc , txt_fg_alpha=fga , graduation_radius=28 , graduation_thickness=0 , graduation_mark_thickness=1 , graduation_unit_angle=27 , graduation_fg_colour=theme , graduation_fg_alpha=0.3 , caption='CPU' , caption_weight=1 , caption_size=10.0 , caption_fg_colour=fgc , caption_fg_alpha=fga , };draw_gauge_ring(settings) settings = {--CPU GRAPH CPU2 value=tonumber(conky_parse(\"${cpu cpu2}\")) , value_max=100 , x=xp , y=yp , graph_radius=17 , graph_thickness=5 , graph_start_angle=180 , graph_unit_angle=2.7 , graph_unit_thickness=2.7 , graph_bg_colour=bgc , graph_bg_alpha=bga , graph_fg_colour=theme , graph_fg_alpha=fga , hand_fg_colour=theme , hand_fg_alpha=0.0 , txt_radius=9 , txt_weight=1 , txt_size=8.0 , txt_fg_colour=fgc , txt_fg_alpha=fga , graduation_radius=28 , graduation_thickness=0 , graduation_mark_thickness=1 , graduation_unit_angle=27 , graduation_fg_colour=theme , graduation_fg_alpha=0.3 , caption='' , caption_weight=1 , caption_size=10.0 , caption_fg_colour=fgc , caption_fg_alpha=fga , };draw_gauge_ring(settings) settings = {--CPU GRAPH CPU3 value=tonumber(conky_parse(\"${cpu cpu3}\")) , value_max=100 , x=xp , y=yp , graph_radius=17 , graph_thickness=5 , graph_start_angle=180 , graph_unit_angle=2.7 , graph_unit_thickness=2.7 , graph_bg_colour=bgc , graph_bg_alpha=bga , graph_fg_colour=theme , graph_fg_alpha=fga , hand_fg_colour=theme , hand_fg_alpha=0.0 , txt_radius=0 , txt_weight=1 , txt_size=8.0 , txt_fg_colour=fgc , txt_fg_alpha=fga , graduation_radius=28 , graduation_thickness=0 , graduation_mark_thickness=1 , graduation_unit_angle=27 , graduation_fg_colour=theme , graduation_fg_alpha=0.3 , caption='' , caption_weight=1 , caption_size=10.0 , caption_fg_colour=fgc , caption_fg_alpha=fga , };draw_gauge_ring(settings) settings = {--CPU GRAPH CPU4 value=tonumber(conky_parse(\"${cpu cpu4}\")) , value_max=100 , x=xp , y=yp , graph_radius=17 , graph_thickness=5 , graph_start_angle=180 , graph_unit_angle=2.7 , graph_unit_thickness=2.7 , graph_bg_colour=bgc , graph_bg_alpha=bga , graph_fg_colour=theme , graph_fg_alpha=fga , hand_fg_colour=theme , hand_fg_alpha=0.0 , txt_radius=-9 , txt_weight=1 , txt_size=8.0 , txt_fg_colour=fgc , txt_fg_alpha=fga , graduation_radius=28 , graduation_thickness=0 , graduation_mark_thickness=1 , graduation_unit_angle=27 , graduation_fg_colour=theme , graduation_fg_alpha=0.3 , caption='' , caption_weight=1 , caption_size=10.0 , caption_fg_colour=fgc , caption_fg_alpha=fga , };draw_gauge_ring(settings) settings = {--LOAD txt=conky_parse(\"${loadavg}\"), x=xp+10 , y=yp+38, txt_weight=0 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) I also added a additional graph for the temperature based on acpi: settings = {--TEMP GRAPH value=tonumber(conky_parse(\"${acpitemp}\")), value_max=100 , x=xp , y=yp , graph_radius=22 , graph_thickness=5 , graph_start_angle=180 , graph_unit_angle=2.7 , graph_unit_thickness=2.7 , graph_bg_colour=bgc , graph_bg_alpha=bga , graph_fg_colour=theme , graph_fg_alpha=fga , hand_fg_colour=theme , hand_fg_alpha=0.0 , txt_radius=0 , txt_weight=1 , txt_size=8.0 , txt_fg_colour=fgc , txt_fg_alpha=fga , graduation_radius=22 , graduation_thickness=4 , graduation_mark_thickness=2 , graduation_unit_angle=27 , graduation_fg_colour=theme , graduation_fg_alpha=0.5 , caption='TEMP' , caption_weight=1 , caption_size=10.0 , caption_fg_colour=fgc , caption_fg_alpha=fga , };draw_gauge_ring(settings) I'm moving around a lot, connecting to wifi or wired depending on location. To let conky graph the right interface I wrote a wrapper around that: iface = conky_parse(\"${exec ip n | awk {'print $3'} | head -1}\") if iface == 'em1' then ifaceCaption = 'EM1' else ifaceCaption = 'WLAN0' end settings = {--NETWORK GRAPH UP value=tonumber(conky_parse(\"${upspeedf \" .. iface .. \"}\")), value_max=100 , x=xp , y=yp , graph_radius=17 , graph_thickness=5 , graph_start_angle=180 , graph_unit_angle=2.7 , graph_unit_thickness=2.7 , graph_bg_colour=bgc , graph_bg_alpha=bga , graph_fg_colour=theme , graph_fg_alpha=fga , hand_fg_colour=theme , hand_fg_alpha=0.0 , txt_radius=0 , txt_weight=1 , txt_size=8.0 , txt_fg_colour=fgc , txt_fg_alpha=fga , graduation_radius=28 , graduation_thickness=0 , graduation_mark_thickness=1 , graduation_unit_angle=27 , graduation_fg_colour=theme , graduation_fg_alpha=0.3 , caption='' , caption_weight=1 , caption_size=10.0 , caption_fg_colour=fgc , caption_fg_alpha=fga , };draw_gauge_ring(settings) settings = {--NETWORK GRAPH DOWN value=tonumber(conky_parse(\"${downspeedf \" .. iface .. \"}\")), value_max=100 , x=xp , y=yp , graph_radius=22 , graph_thickness=5 , graph_start_angle=180 , graph_unit_angle=2.7 , graph_unit_thickness=2.7 , graph_bg_colour=bgc , graph_bg_alpha=bga , graph_fg_colour=theme , graph_fg_alpha=fga , hand_fg_colour=theme , hand_fg_alpha=0.0 , txt_radius=35 , txt_weight=1 , txt_size=8.0 , txt_fg_colour=fgc , txt_fg_alpha=fga , graduation_radius=28 , graduation_thickness=0 , graduation_mark_thickness=1 , graduation_unit_angle=27 , graduation_fg_colour=theme , graduation_fg_alpha=0.3 , caption=ifaceCaption , caption_weight=1 , caption_size=10.0 , caption_fg_colour=fgc , caption_fg_alpha=fga , };draw_gauge_ring(settings) Depending on those locations I get other ip addresses on other networks and therefore other SMTP services. To tackle those smtp service I wrote a setsmtp script which will be called from within my conky setup based on the ip addresses: if iface =='em1' then ip = conky_parse(\"${addr em1}\") if ip == 'IP AT WORK PLACE ONE' then conky_parse(\"${exec setsmtp -b}\") todo='work' elseif ip == 'IP AT WORK PLACE TO' then todo='work' else conky_parse(\"${exec setsmtp -t}\") todo='personal' end settings = {--IP ADDRESS txt=ip, x=xp+10 , y=83, txt_weight=0 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) elseif iface == 'wlan0' then ssid = conky_parse(\"${wireless_essid wlan0}\") if ssid == 'SSID WORK PLACE ONE' then conky_parse(\"${exec setsmtp -b}\") todo='work' elseif ssid == 'SSID HOME' then conky_parse(\"${exec shares -a}\") conky_parse(\"${exec setsmtp -t}\") todo='personal' else conky_parse(\"${exec setsmtp -t}\") todo='personal' end settings = {--WIRELESS INFO txt=conky_parse(\"${wireless_link_qual wlan0} %\"), x=xp+10 , y=83, txt_weight=1 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) else iface='' end To monitor my battery state I added this graph: settings = {--BATTERY GRAPH value=tonumber(conky_parse(\"${battery_percent}\")), value_max=100 , x=xp , y=yp , graph_radius=22 , graph_thickness=5 , graph_start_angle=180 , graph_unit_angle=2.7 , graph_unit_thickness=2.7 , graph_bg_colour=bgc , graph_bg_alpha=bga , graph_fg_colour=theme , graph_fg_alpha=fga , hand_fg_colour=theme , hand_fg_alpha=0.0 , txt_radius=0 , txt_weight=1 , txt_size=8.0 , txt_fg_colour=fgc , txt_fg_alpha=fga , graduation_radius=22 , graduation_thickness=4 , graduation_mark_thickness=2 , graduation_unit_angle=27 , graduation_fg_colour=theme , graduation_fg_alpha=0.5 , caption='BATTERY' , caption_weight=1 , caption_size=10.0 , caption_fg_colour=fgc , caption_fg_alpha=fga , };draw_gauge_ring(settings) settings = {--BATTERY CHARGING STATE txt=conky_parse(\"${acpiacadapter} ${battery_time}\"), x=xp-25 , y=83, txt_weight=0 , txt_size=10 , txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) As you saw in the network topic I set a todo variable based on my location. That variable will point to a specific tracks-work/tracks-personal bash script which will grab my todo's for work or for leisure ;) -- TODO COLUMN conky_parse(\"${execpi 53 ~/.conky/scripts/tracks-\" .. todo .. \".sh}\") arrayYfactors={'20', '35', '51', '65'} for i, Yfactor in ipairs(arrayYfactors) do firstchar=conky_parse(\"${exec head -\" .. i .. \" ~/.conky/scripts/todo-\" .. todo .. \" | tail -1 | sed -r 's/&#94; //' | cut -d ' ' -f 1}\") if firstchar == '*' then tmpweight='0' tmpcolour=fgc elseif firstchar == '-' then tmpweight='0' tmpcolour=fgc else tmpweight='1' tmpcolour=theme end settings = { --TODO column txt=conky_parse(\"${exec head -\" .. i .. \" ~/.conky/scripts/todo-\" .. todo .. \" | tail -1 | sed -r 's/&#94; //' | cut -d '(' -f 1}\"), x=xp+80 , y=Yfactor, txt_weight=tmpweight , txt_size=12, txt_fg_colour=tmpcolour , txt_fg_alpha=fga , };display_text(settings) end And last but not least based on location I monitor also a specific jenkins job using the conky hudson script settings = { --JENKINS TITLE txt=conky_parse(\"${exec ~/.conky/scripts/hudson/conkyhudson.py -t ~/.conky/scripts/hudson/\" .. todo .. \".template | cut -d '|' -f 1 | head -1}\"), x=xp+80 , y=80, txt_weight=1 , txt_size=9, txt_fg_colour=theme , txt_fg_alpha=fga , };display_text(settings) settings = { --JENKINS line txt=conky_parse(\"${exec ~/.conky/scripts/hudson/conkyhudson.py -t ~/.conky/scripts/hudson/\" .. todo .. \".template | cut -d '|' -f 2 | sed 's/_/ /' | head -1}\"), x=xp+178 , y=80, txt_weight=0 , txt_size=10, txt_fg_colour=fgc , txt_fg_alpha=fga , };display_text(settings) The conky hudson template used in this last feature is looking like: [job;1;jenkinsurl.com;nameofyourjenkinsjob] nameofyourjenkinsjob|#[1;number] [1;result] [1;id]","tags":"linux","url":"https://visibilityspots.github.io/blog/conky-colors.html","loc":"https://visibilityspots.github.io/blog/conky-colors.html"},{"title":"Tracks","text":"To get an overview of my todo's I used to list them in google tasks. Back in time I was convinced it would nicely integrate with all tools software and distributions I would use. After some month's I figured out it wouldn't. So I searched on the web for software which would take that task over from google. I used to play with several tools, from trac , to chiliproject to redmine . All those tools worked very nice but were some overkill to only manages todo lists. In the meantime I installed gitlabhq , tried to abuse the issues there to manage my todo's. But that went into chaos when managing repo's for household tasks etc. When creating that repo I figured out it wasn't logical neither. So I tumbled into a rails application. Tracks , and hell I like it a lot. It's easy, it can be viewed with android devices, it mails every week an overview of upcoming tasks for that week and I stripped it out in my conky setup. Today I have 2 instances running one on my personal server and one for my tasks at the customer. Depending on location conky shows me the right tasks. The setup is rather easy, it's all explained clear on their website. I opted for a sqlite3 database and running thin to host it. Nevertheless I still suffer 2 major issues, the first one is related to the android app. Seems like it shows also closed tasks as being open. The 2nd one is the frustration of integration github issues. Until today I didn't find a tool which is able to synchronizes all your github issues into whatever application. The only tool I found was ghi which is just a command line overview of your github tasks. So please if you found a solution for that don't hesitate to contact me about it! You could make my day!","tags":"linux","url":"https://visibilityspots.github.io/blog/tracks.html","loc":"https://visibilityspots.github.io/blog/tracks.html"},{"title":"Irssi bitlbee channel","text":"Every time I want to join a channel on a jabber account using bitlbee I'm a bit confused and have to search the whole inter-webs before actually finding out howto configure my chat setup to do so. My online search points me out to the bitlbee wiki . Nevertheless those commands never got to the point to have it actually working. After many attempts a colleague pointed me to the right solution. To never forget it anymore and sharing the working setup with the world I summarize it in this blog post. In your bitlbee control channel &bitlbee : chat add [account id] room@conference.jabber.link #room /j #room As you can see in the chat add command the ending #room was missing in the online documentation. Hope to tackle many frustrations by this one :)","tags":"linux","url":"https://visibilityspots.github.io/blog/irssi-bitlbee-channel.html","loc":"https://visibilityspots.github.io/blog/irssi-bitlbee-channel.html"},{"title":"GTalkSMS mobile alerting through xmpp protocol","text":"Recently I bumped into GTalkSMS when I was surfing the net for manuals on irssi & bitlbee using to chat so I could move away from pidgin. This GTalkSMS tool is quite cool. When am at work or at home my mobile isn't always in my sight. Therefore it could happen someone has to call me a few times before I answer the call. (biggest frustration of the girlfriend meaning you have to answer your call within a time period of 3 seconds because you're working in IT) Using this nifty tool you can control your mobile through your favorite chat client by the xmpp protocol. You install the app from the google play market on your phone, choose if you want to use a separate account (preferred by the guys who developed it) I created one on jabber.org. Fill those credentials into the mobile app and connect your chat client to the chosen account. Once configured you can start sending commands to your phone for example the command 'where' and you get the google maps location link. Have fun controlling your phone the geek way!","tags":"android","url":"https://visibilityspots.github.io/blog/gtalksms.html","loc":"https://visibilityspots.github.io/blog/gtalksms.html"},{"title":"Lighttpd change tcp port in CentOS","text":"It seems like a very simple job, and in fact it is. But I had an issue when I tried to change this in my Cent OS 6.3 setup. After some digging on the internet I found out selinux was the blocking factor. The configuration of the new port has to be done in the lighttpd conf file. /etc/lighttpd/lighttpd.conf server.port = 2080 When I changed the config file and restarted the /etc/init.d/lighttpd service I got following error: (network.c.379) can't bind to port: 2080 Permission denied I checked that I added the port to iptables, tried other ports, nothing worked. Until I found out it was related to the default selinux configuration. On many forums was indicated that the problem is solved by disabling the selinux service. Nevertheless I wanted to do it the right way and after some try and error found out that by installing the package policycoreutils-python you can look up the status of the selinux feature # sestatus SELinux status: enabled SELinuxfs mount: /selinux Current mode: enforcing Mode from config file: enforcing Policy version: 24 Policy from config file: targeted # yum provides /usr/sbin/semanage Loaded plugins: fastestmirror, presto, priorities Loading mirror speeds from cached hostfile * base: centos.weepeetelecom.be * epel: be.mirror.eurid.eu * extras: centos.weepeetelecom.be * remi: remi-mirror.dedipower.com * updates: centos.weepeetelecom.be 187 packages excluded due to repository priority protections policycoreutils-python-2.0.83-19.24.el6.x86\\_64 : SELinux policy core python utilities Repo : base Matched from: Filename : /usr/sbin/semanage # yum install policycoreutils-python Once this is done we can use the semanage command to add our new port to the selinux security feature. First we can list all already configured ports for the http service: semanage port -l \\| grep http\\_port\\_t If the desired port isn't listed we can add this with following command: semanage port -a -t http\\_port\\_t -p tcp 2080 You don't need to restart the selinux feature, the setting will take effect immediately after you added the last command. Once that is done you can restart the lighttpd service without the permission denied issue! If you also want to have tcp port 9000 be able to work for php you also have to add this one to selinux: semanage port -a -t http\\_port\\_t -p tcp 9000 Sources: http://www.howtoforge.com/installing-lighttpd-with-php5-php-fpm-and-mysql-support-on-centos-6.3 http://www.cyberciti.biz/faq/rhel-fedora-redhat-selinux-protection/ http://www.cyberciti.biz/faq/redhat-install-semanage-selinux-command-rpm/","tags":"linux","url":"https://visibilityspots.github.io/blog/selinux.html","loc":"https://visibilityspots.github.io/blog/selinux.html"},{"title":"Dropbox","text":"Reading this article will go through the process I went through configuring multiple dropbox accounts on my centos machine (one personal and one for work) and encrypting them both using encfs. That way I'm sure dropbox can't read the data stored into it. Because no I don't trust anybody on the cloud! In the first part I will configure 2 dropbox services using a CentOS 6 Desktop, in the second part I will encrypt those 2 dropbox services using encfs. The first account you can just install and configure the normal way provided by dropbox itself. Here I configured my work account on so all work related data will be stored in my home/Dropbox folder later on. Once this is done we have to install the 2nd service and configure the personal account into it. To do this we have to create 2 new folders into your home directory. One named .dropbox-personal where the 2nd dropbox service configuration files will be stored in. And another named for example Personal where the actual Dropbox folder for this 2nd account will be written to. When you created those 2 fresh directories you can open a terminal and run the command HOME=$HOME/.dropbox-personal /usr/bin/dropbox start -i so the 2nd dropbox service will be started from this newly .dropbox-personal folder. Follow the wizard, but make sure you enter a CUSTOM folder (e.g. ~/Personal) where you point to the 2nd new folder you've created ~/Dropbox-personal/! To start this 2nd instance of dropbox we have to create a script which will start this instance and add it to our Startup programs. Create an executable (chmod +x file.sh) .sh file with this content: #!/bin/bash HOME=$HOME/.dropbox-personal /usr/bin/dropbox start And add a new entry via System -> Preferences -> Startup Applications for this script. Or you can copy the script.sh file to your /usr/bin/ director without the extension so your script will be available as a command in your terminal. So right now we have 2 dropbox accounts uploading the folders ~/Dropbox for work related stuff and ~/Personal/Dropbox/ for personal data. Next step will be encryption for those folders. By encrypting your dropbox folders we make sure our data will not be readable when stored on the dropbox servers and sent over the internet. To accomplish this encryption I opted for encfs. On a CentOS machine you can install it using yum: sudo yum install fuse-encfs Once the package is installed we can configure our encrypted volumes. The way this encryption will work is quite simple. You have 2 folders, a private folder which is the working directory where you can edit delete and create files and folders to work with. All content in your private folder will be encrypted to you encrypted folder which will be synchronized to the dropbox online services. So in our case we have to create 4 folders, 2 for our work account (~/Private & ~/Dropbox/.encrypted) and 2 for our personal account (~/Personal/Private-personal & ~/Personal/Dropbox/.encrypted-personal). Once those folders are created we can configure the encfs volumes: encfs ~/Dropbox/.encrypted ~/Private encfs ~/Personal/Dropbox/.encrypted-personal ~/Personal/Private-personal where you can use the paranoia mode to encrypt your files enter \"p\" for pre-configured paranoia mode with your chosen password. When you created your encrypted volumes you can administer your data in the private folders, this data will be encrypted automatically to your .encrypted* folders which will be uploaded to dropbox. When you install dropbox on another computer you also have to install encfs on it to decrypt your files. In the dropbox folders you can see there is a .encfs6.xml file. To be completely sure dropbox can't do anything with your files you can exclude this file to be synchronized online. But make some copies of it to a secure place (usb stick or your phone) before you continue. This can be done with the command: dropbox exclude add ~/Dropbox/.encrypted/.encfs6.xml dropbox exclude list And remove the .encfs6.xml file from the online dropbox account using the web service. For the 2nd service you have to use the following commands: HOME=$HOME/.dropbox-personal /usr/bin/dropbox exclude add ~/Personal/Dropbox/.encrypted-personal/.encfs6.xml HOME=$HOME/.dropbox-personal /usr/bin/dropbox exclude list`` On every computer where you install encfs to decrypt those files you have to copy the proper .encfs6.xml file in the .encrypted* folders so you can decrypt the encfs volumes. Be aware you can't use your encrypted files using the dropbox web interface. On your android phone you can install cryptonite which will decrypt your files so you can use them on your phone. I created a Startup script which can decrypt and umount the encrypted folders and shared it on github by adding the script with the preferred parameters to your Startup programs you have to fill in the passwords each time you log in so your folders are decrypted and you can start using them. (or add the script to your /usr/bin/ folder named encryption so you can handle it as a command called encryption as you named it in your terminal) You can also use this setup to share for example your evolution mails via dropbox on an encrypted way so nobody can read your mails except your on your different computers with evolution. (Make sure the evolution versions match and point your evolution working directory to the private one using symlinks - e.g. ln -s ~/Private/.evolution ~/.evolution) Feel free to add patches, send remarks about this topic.) by adding the script with the preferred parameters to your Startup programs you have to fill in the passwords each time you log in so your folders are decrypted and you can start using them. Resources: http://maketecheasier.com https://help.ubuntu.com http://janaksingh.com","tags":"linux","url":"https://visibilityspots.github.io/blog/dropbox.html","loc":"https://visibilityspots.github.io/blog/dropbox.html"},{"title":"Puppet sslv3 alert certificate revoked","text":"I started the day with ssl issues using puppet. Last week I cleaned 2 hosts in our tree using the puppet command # puppet node clean [hostname] on the puppetmaster. I did this to clean out the stored configs for those nodes. But I didn't realized this also cleaned out the ssl certificates for those clients. So I started the new week with this uncomfortable issue: [root@agent ~]# puppet agent --test err: Could not retrieve catalog from remote server: SSL_connect returned=1 errno=0 state=SSLv3 read server session ticket A: sslv3 alert certificate revoked warning: Not using cache on failed catalog err: Could not retrieve catalog; skipping run err: Could not send report: SSL_connect returned=1 errno=0 state=SSLv3 read server session ticket A: sslv3 alert certificate revoked After some digging on the internet I achieved to solve this issue. Here under I described the steps to breath again: To be sure the certificates are completely removed on the puppetmaster I explicitly cleaned them again [root@master ~]#puppet cert -c hostname Now we are sure those certificates are cleaned up on the master we have to do this also on the agent Looking for the directory where those certificates are stored [root@agent ~]# puppet --genconfig | grep certdir # The default value is '$certdir/$certname.pem'. # The default value is '$certdir/ca.pem'. certdir = /var/lib/puppet/ssl/certs For older versions of puppet [root@agent ~]# puppet config print | grep certdir Removing the existing certificates on the client: [root@agent ~]# rm /var/lib/puppet/ssl -rf Once the certificates are completely removed on the master and the client we have to regenerate them from the agent using the puppet daemon [root@agent ~]# puppet agent --test or by manually regenerating them [root@agent ~]# puppet certificate generate hostname.domain --ca-location remote true As soon as new certificates are generated and we got the true back from the agent we can sign the fresh certificate on the master List the certificates which are waiting to get signed and sign them [root@master ~]# puppet cert -l \"hostname.domain\" (XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX) [root@master ~]# puppet cert sign hostname.domain notice: Signed certificate request for hostname.domain notice: Removing file Puppet::SSL::CertificateRequest hostname.domain at '/var/lib/puppetmaster/ssl/ca/requests/hostname.domain.pem' If everything went well you should be able to run puppet again on the client puppet agent --test --noop and relax again! Digging the internet I crossed honglus blog and an issue on puppetlabs projects which made my day.","tags":"puppet","url":"https://visibilityspots.github.io/blog/puppet-revoked-certificate.rst.html","loc":"https://visibilityspots.github.io/blog/puppet-revoked-certificate.rst.html"},{"title":"Writing customized icinga checks","text":"Recently I started to try writing a customized script for the icinga monitoring tool. I will try to describe the steps I went trough to achieve this in this post. I assume you already have a working icinga setup. If not you can find documentation about this on http://docs.icinga.org/ . First of all you need to script. I created a script which will check if a service is running using the command # /etc/init.d/service status to see how to implement this in icinga. The script can be found on my github repo. Once you have tested the script you have to make sure it is copied to the scripts directory on the server you want to monitor. Usually this directory can be found in /usr/lib64/nagios/plugins/ on a CentOS 6 machine. Also make sure your script is executable (chmod +x). Next we have to configure the NRPE daemon on this remote host. The nrpe configuration file can be found in etc/nagios/nrpe.cfg found using the mlocate software. # updatedb # locate nrpe.cfg. Here you have to make sure that you point the command to your script location by adding the underlying line and restarting the NRPE service command[check_NAME]=/usr/lib64/nagios/plugins/check_NAME /etc/init.d/nrpe restart The server side where we have to configure the command and the service itself into the icinga service. We have to add the command check_NAME into the file /etc/icinga/objects/commands.cfg define command { command_name check_NAME command_line $USER1$/check_NAME } To configure the specified service you have to configure a node with this newly created command for in example /etc/icinga/objects/services/node.cfg define service { service_description DESCRIPTION check_command check_nrpe_command! check_NAME use generic-service notification_period 24x7 host_name HOSTNAME.OF.SERVER } And restart the icinga service # /etc/init.d/icinga restart After a few minutes the check should appear into your icinga front-end. Enjoy scripting your own custom scripts!","tags":"linux","url":"https://visibilityspots.github.io/blog/icinga-checks.html","loc":"https://visibilityspots.github.io/blog/icinga-checks.html"},{"title":"SMS server using CentOS, kannel and playsms","text":"On this page I will describe the way I went trough to configure an sms gateway using a laptop, huawei modem, falcom A2D-1 or the option Globetrotter hardware using the open source software kannel & playsms . The main goal of this project was related to the scouting movement in Belgium I'm active. We wanted to interrogate all of our members who were on a start weekend of the next scouting year. To do this we had the idea to use the sms communication channel. This because almost every youngster has the possibility to send sms messages without a big effort. To achieve this I searched on the internet and found the playsms software. Using this software you can easily add an interactive flow to communicate with people using sms. We used the sms quiz where we added some questions with keywords were people could answer to and we replied with a next question. As mobile provider we choose for Mobile Vikings a belgium operator with an open-mind. They were very helpfull when I contacted them to see if they could monitor some mobile traffic for my sim. But before this software can handle your sms messages they have to be captured and received using a SIM card and pushed to the software. This step in the whole process can be done by kannel . Process using CentOS 6.3 minimal installation Install some required dependency packages: # yum install gcc libxml2-devel mysql-server wvdial vim Looking for the modem: # wvdialconf /etc/wvdial.conf Found a modem on **/dev/ttyUSB0** Minicom Before starting configuring the services which were going to communicate with the modem I wanted to make sure I could send text messages with it. To check that functionality I installed the minicom serial communication program: # yum install minicom # minicom In this terminal you can control the modem using AT commands. A nice tutorial about those commands is available on qualityguru . Steps for entering the PIN. AT+CPIN=XXXX OK AT+CPIN? +CPIN: READY OK Checking if the SMS center is configured: AT+CSCA? +CSCA: \"+32XXXXXXXXX\",145 OK If not configured, configure it by: AT+CSCA=\"+32XXXXXXXXX\" OK The steps for sending a text message: AT+CMGF=1 OK AT+CMGS=\"+32XXXXXXXXX\" > This is the text message. > (CTRL-Z) +CMGS: XX OK If you received the message on your phone its working obviously and we can start configuring kannel. If not, check the troubleshoot page of qualityguru for some common mistakes. Kannel At the moment of writing this post the last stable version is 1.4.3. Using CentOS 6.4 you can install kannel from the epel repository : # yum install kannel Or you can choose to compile it from source: # wget http://www.kannel.org/download/1.4.3/gateway-1.4.3.tar.gz # tar zxvf gateway-1.4.3.tar.gz -C /usr/local/src/ # cd /usr/local/src/gateway-1.4.3/ # mkdir -p /etc/kannel # ./configure --prefix=/etc/kannel # make # make install I installed the kannel service from the repository and created a symlink from /etc/kannel.conf to the /etc/kannel/kannel.conf so the playsms service could read the configuration afterwards: # mkdir /etc/kannel/ # ln -s /etc/kannel.conf /etc/kannel/kannel.conf Once you configured your device you start kannel by starting the kannel service: # service kannel start If everything went well you can see that there are 2 services started: # ps aux | grep kannel kannel 9611 1.9 0.1 750424 6684 ? Sl 13:14 2:37 /usr/sbin/bearerbox /etc/kannel.conf kannel 9636 0.0 0.1 674228 4676 ? Sl 13:14 0:00 /usr/sbin/smsbox /etc/kannel.conf In the /var/log/kannel/kannel.log file you can follow the state of the kannel service. I struggled a bit with this to find out the reset string for the modems I used. By searching the internet you can find the particular string for your device. For example the option one I found on enterprisemobiletoday.com by try & error in the minicom terminal. I used different sorts of hardware and listed the specific kannel.conf files here under per device. In the first phase I used a huawei USB dongle: #CORE group = core admin-port = 13000 admin-password = #PASSWORD status-password = #PASSWORD log-file = \"/var/log/kannel/kannel.log\" log-level = 0 access-log = \"/var/log/kannel/access.log\" smsbox-port = 13001 store-type = file store-location = \"/var/log/kannel/kannel.store\"* #SMSC MODEM GSM group = smsc smsc = at connect-allow-ip = 127.0.0.1 port = 13013 host = \"localhost\" smsc-id = Huawei modemtype = Huawei device = /dev/ttyUSB0 speed = 9600 sms-center = \"+32486000005\" my-number = \"+324XXXXXXXX\" pin = XXXX group = modems id = huawei name = huawei detect-string = \"huawei\" init-string = \"AT+CNMI=2,1,0,0,0;+CMEE=1\" #SMSBOX SETUP group = smsbox bearerbox-host = 127.0.0.1 bearerbox-port = 130X01 sendsms-port = 13131 sendsms-chars = \"0123456789+\" global-sender = 00324XXXXXXXX log-file = \"/var/log/kannel/smsbox.log\" log-level = 0 access-log = \"/var/log/kannel/access.log\" #SEND-SMS USERS group = sendsms-user username = #USERNAME password = #PASSWORD user-allow-ip = \"\\*.\\*.\\*.\\*\" #SMS SERVICE group = sms-service keyword = default accept-x-kannel-headers = true #accepted-smsc = Huawei accepted-smsc = at2 max-messages = 0 assume-plain-text = true catch-all = true get-url = \"http://localhost/playsms/index.php?app=call&cat=gateway&plugin=kannel&access=geturl&t=%t&q=%q&a=%a\" During the event was in the possession of a falcom A2D-1 gateway which was connected from serial to usb: group = core admin-port = 13000 admin-password = playsms log-file = \"/var/log/kannel/kannel.log\" log-level = 0 access-log = \"/var/log/kannel/access.log\" smsbox-port = 13001 store-type = file store-location = \"/var/log/kannel/kannel.store\"* group = smsc smsc = at modemtype = falcom device = /dev/ttyUSB0 my-number = \"+324XXXXXXXX\" sms-center = \"+32486000005\" pin = XXXX group = modems id = falcom name = \"Falcom\" detect-string = \"Falcom\" reset-string = \"AT+CFUN=1\" #SMSBOX SETUP group = smsbox bearerbox-host = localhost sendsms-port = 13131 log-file = \"/var/log/kannel/smsbox.log\" log-level = 0 access-log = \"/var/log/kannel/access.log\" #SEND-SMS USERS group = sendsms-user username = #USER password = #PASSWORD #SMS SERVICE group = sms-service keyword = default accept-x-kannel-headers = true max-messages = 0 assume-plain-text = true catch-all = true get-url = \"http://127.0.0.1:2080/playsms/index.php?app=call&cat=gateway&plugin=kannel&access=geturl&t=%t&q=%q&a=%a\" After the event I had to give back the falcom and got my hands on an option globetrotter HSPDA card connected on a pcmci slot of the laptop I configured as CentOS server: #CORE group = core admin-port = 13000 admin-password = playsms status-password = playsms log-file = /var/log/kannel/kannel.log log-level = 0 access-log = /var/log/kannel/access.log smsbox-port = 13001 store-type = file store-location = /var/log/kannel/kannel.store #SMSC MODEM GSM group = smsc smsc = at connect-allow-ip = 127.0.0.1 port = 13013 host = \"localhost\" smsc-id = Option modemtype = Option device = /dev/ttyUSB0 speed = 9600 sms-center = \"32486000005\" my-number = \"324XXXXXXXX\" pin = XXXX # If modemtype=auto, try everyone and defaults to this one group = modems id = generic name = \"Generic Modem\" reset-string = \"AT&F\" #SMSBOX SETUP group = smsbox bearerbox-host = 127.0.0.1 bearerbox-port = 13001 sendsms-port = 13131 sendsms-chars = \"0123456789+\" global-sender = 0032485550261 log-file = \"/var/log/kannel/smsbox.log\" log-level = 0 access-log = \"/var/log/kannel/access.log\" #SEND-SMS USERS group = sendsms-user username = playsms password = playsms #SMS SERVICE group = sms-service keyword = default accept-x-kannel-headers = true accepted-smsc = at max-messages = 0 assume-plain-text = true catch-all = true Web service For the playsms service we need to have a webserver configured. You can use every webserver you want, I tried with xampp and lighttpd. During the event I used with the xampp web service because it was working after all by following the howto of kasrut . After the event was finished I migrated to lighttpd mainly because I already had some other applications running on that service. Xampp # wget http://nchc.dl.sourceforge.net/project/xampp/XAMPP%20Linux/1.7.4/xampp-linux-1.7.4.tar.gz # tar zxvf xampp-linux-1.7.4.tar.gz -C /opt/ # cd /opt/lampp # ./lampp start Lighttpd For the installation of lighttpd I refer to a clear tutorial on howtoforge playsms playsms is a free and open-source gateway. I used this software to configure a big quiz to set up a cool and trendy communication flow between people and our scouting movement. I used the git repository to easily update my instance to the newest releases: # cd /usr/local/src/ # git clone git@github.com:antonraharja/playSMS.git # cd playSMS/ Creation of the necessary directories and copy the web files to the webserver directory # mkdir -p /var/www/html/playsms /var/spool/playsms /var/log/playsms /var/lib/playsms # cp -r usr/local/src/playSMS/web/* /var/www/html/playsms/ Creation of a mysql db and user: # mysql -u root -p # Enter password: # mysql> create database playsms; Query OK, 1 row affected (0.00 sec) # mysql> grant usage on *.* to USER@localhost identified by ‘PASSWORD'; Query OK, 0 rows affected (0.00 sec) # mysql> grant all privileges on playsms.* to USER@localhost ; Query OK, 0 rows affected (0.00 sec) # mysql> quit # msql -u root -p playsms < /usr/local/src/playSMS/db/playsms.sql Next step is to configure the playsms web service. Therefore follow those steps: # cd /var/www/html/playsms # cp config-dist.php config.php Edit this config.php file to your own needs. Now we configured the parameters we can start to install the services: # mkdir -p /etc/default /usr/local/bin # cp /usr/local/src/playSMS/daemon/linux/etc/playsms /etc/default/ # cp /usr/local/src/playSMS/daemon/linux/bin/* /usr/local/bin/ # vim /etc/default/playsms # edit the paths to your environment ones I've used rc.local to start the service at boot: # vim /etc/rc.d/rc.local and put /usr/local/bin/playsmsd_start at the end of that file Next I configured 2 new aliases in my ~/.bashrc to easily start and stop the service: alias playsms-start='/usr/local/bin/playsmsd_start' alias playsms-stop='/usr/local/bin/playsmsd_stop' By re-logging in you can start the service by: # playsms-start And check if the necessary services are started: # ps aux | grep playsms root 7735 0.0 0.0 103236 868 pts/4 S+ 15:52 0:00 grep playsms root 21845 0.0 0.0 106312 1660 pts/4 S 12:25 0:06 /bin/bash ./playsmsd /var/www/html/playsms root 21847 0.0 0.0 106184 1536 pts/4 S 12:25 0:05 /bin/bash ./sendsmsd /var/www/html/playsms Finally you can browse http:/ /<your web server IP>/playsms/ and login using username: admin password: admin Where you need to configure kannel in the menu: Gateway > Manage Kannel > kannel (Inactive) (click here to activate) and adopt the parameters to the ones you configured in kannel.conf After filling in your preferences you should be able to send and receive messages through this nifty web console. (TIP: Using twice the same keyword for a quiz resulted in the fact that only this word is needed to send to the sms server to start the interactivity) Have fun with it!","tags":"linux","url":"https://visibilityspots.github.io/blog/sms-server.html","loc":"https://visibilityspots.github.io/blog/sms-server.html"},{"title":"SSH acces on Lacie Network Space 2","text":"Recently we installed a Lacie Network Space 2 at home. Easy to share documents on the LAN network, having a central place for common media etc. After playing around with it I wanted to see if it's possible to gain access to the underlying operating system of it. On that way I could for example use this access to wake up a pc with wake on LAN. And guess what, it can be done and thanks to a script of a guy Andreus it's even very easy! I found a forum post about his work and tested it successfully with the latest firmware version 2.2.8! After you created the right capsule the best way do update is to force it manually . When you achieved to get ssh access to the device you can play around with it, by for example installing debian squeeze in a chkroot environment. On the wiki of nas-central.org you can find more information how to play around with your ssh access. Have fun with it!","tags":"linux","url":"https://visibilityspots.github.io/blog/lacie-networkspace-2.html","loc":"https://visibilityspots.github.io/blog/lacie-networkspace-2.html"},{"title":"Create and distribute .rpm package","text":"You wrote a piece of software and want to distribute it on an easy way through a yum repository? That can be done, by making in the first place an rpm package of your code. In the first place you need to set up a directory structure. This can be done using the tool rpmdevtools on a rhel based machine: # yum install rpmdevtools Once you installed the software you need to setup the directory tree: $ rpmdev-setuptree This will install the necessary rmpbuild directory tree. You will see there is create a SOURCES directory, you need to get your software source into here. Best is to copy your source code into this directory named like * namoOfYourSoftware-0.0* where 0.0 stands for the release version. Once you copied your source code you need to package it into a tar file: $ tar -cvzf nameOfYourSoftware-0.0.tar.gz namoOfYourSoftware-0.0 Once you packaged your source code we need to create a spec file in the SPEC directory. When you created and configured your spec file the last thing we need to do is to build the actual rpm package: $ rpmbuild -ba SPECS/name.spec If everything went smooth you should find your rpm package in the RPMS directory. To install your rpm package to see if it actually works: rpm -ivh name-package.rpm Now you have your own rpm package you can distribute. A nice and clean distribution solution could be your very own yum repository Resources: hello world package tutorial from The Linux Documentation project","tags":"linux","url":"https://visibilityspots.github.io/blog/rpm-package.html","loc":"https://visibilityspots.github.io/blog/rpm-package.html"},{"title":"Puppet module mumble-server","text":"Mumble is an open source, low-latency, high quality voice chat software primarily intended for use while gaming. Puppet is a tool designed to manage the configuration of Unix-like and Microsoft Windows systems decoratively. The puppet-mumble module installs a mumble server (version 1.2.3) automatically on a CentOS 6.x machine using the puppet software based on mumble-documentation . The module needs a repository which contains the mumble-server package. I distribute this package on my own visibilityspots repository. Using puppet this will create the necessary mumble user and group and will configure the mumble-server using your desired settings, like username, password, and tcp port the daemon will listen on.","tags":"puppet","url":"https://visibilityspots.github.io/blog/puppet-mumble.html","loc":"https://visibilityspots.github.io/blog/puppet-mumble.html"},{"title":"Cisco HWIC 3G configuration to 2G","text":"In some cisco routing devices you have the possibility to extend the features with a HWIC 3G card so mobile connectivity is added to your network infrastructure. This can be interesting for a mobile fail-over connection. But as we all now, the mobile reception isn't always that good. To see the signal strength on your cisco device you can use: show cellular 0/0/0 connection depending on which slot you plugged the HWIC card into. If the measured value is beneath the -100 dBm then you have sufficient signal to setup a 3G ( CDMA - WCDMA) connection on. If that's not the case or the values of different measurements are very different you should consider to downgrade too a 2G connection because else your 3G connection will be very wonky! The default setup would mostly be auto-band. This means that if there is a little chance to connection over 3G your device will try to connect to this 3G connection. If the 3G signals gets lost it REconnects to 2G and therefore connectivity interruption will take place. cellular 0/0/0 gsm band auto-band So if your 3G signals isn't strong enough it's a good idea to force your device to connect only on 2G. This can easily be done by: cellular 0/0/0 gsm band gsm-all-bands That way your wonky 3G connection will became a stable 2G connection!","tags":"networking","url":"https://visibilityspots.github.io/blog/cisco-3g.html","loc":"https://visibilityspots.github.io/blog/cisco-3g.html"},{"title":"BGP announcing default route","text":"Advertising default route with BGP If you want to announce the default route which is statically routed then you have to add following commands to the working BGP configuration: ip route 0.0.0.0 0.0.0.0 192.168.1.1 router bgp 65001 network 0.0.0.0 default-information originate when you then clear the ip bgp routing softly (so the current connecting will not be broken) clear ip bgp soft in clear ip bgp soft out you should see that the default route is will be advertised: sh ip bgp summary sh ip bgp neighbors IP.ADDRESS advertised-routes","tags":"networking","url":"https://visibilityspots.github.io/blog/bgp-default-route.html","loc":"https://visibilityspots.github.io/blog/bgp-default-route.html"},{"title":"Apple remote (A1156) - MacBook Pro 3.1 & Ubuntu 10.04","text":"It isn't supported by default using Ubuntu but it's as handy as hell, the apple infrared remote control. After some mayor headaches I finally succeeded to configure it manually on my MacBook Pro 3.1 running Ubuntu 10.04. It's quite easy once you know how. Installation of the lirc library: $ sudo apt-get install lirc Adapting the configuration files (make sure to backup them first!): $ sudo cp /old/file /new/file.bak /etc/lirc/hardware.conf # /etc/lirc/hardware.conf # #Chosen Remote Control REMOTE=\"Apple Mac mini USB IR Receiver\" REMOTE_MODULES=\"uinput\" REMOTE_DRIVER=\"macmini\" REMOTE_DEVICE=\"/dev/usb/hiddev0\" REMOTE_SOCKET=\"\" REMOTE_LIRCD_CONF=\"\" REMOTE_LIRCD_ARGS=\"--uinput\" #Chosen IR Transmitter TRANSMITTER=\"None\" TRANSMITTER\\_MODULES=\"\" TRANSMITTER\\_DRIVER=\"\" TRANSMITTER\\_DEVICE=\"\" TRANSMITTER\\_SOCKET=\"\" TRANSMITTER\\_LIRCD\\_CONF=\"\" TRANSMITTER\\_LIRCD\\_ARGS=\"\" #Enable lircd START\\_LIRCD=true #Don't start lircmd even if there seems to be a good config file #START\\_LIRCMD=\"false\" #Try to load appropriate kernel modules LOAD\\_MODULES=\"true\" # Default configuration files for your hardware if any LIRCMD\\_CONF=\"\" #Forcing noninteractive reconfiguration #If lirc is to be reconfigured by an external application #that doesn't have a debconf frontend available, the noninteractive frontend can be invoked and set to parse REMOTE and TRANSMITTER #It will then populate all other variables without any user input #If you would like to configure lirc via standard methods, be sure #to leave this set to \"false\" FORCE\\_NONINTERACTIVE\\_RECONFIGURATION=\"false\" START\\_LIRCMD=\"\" # Remote settings required by gnome-lirc-properties REMOTE\\_MODEL=\"A1156\" REMOTE\\_VENDOR=\"Apple\" # Receiver settings required by gnome-lirc-properties RECEIVER\\_MODEL=\"Built-in\\\\ IR\\\\ Receiver\\\\ \\\\(0x8242\\\\)\" RECEIVER\\_VENDOR=\"Apple\" **/etc/lirc/lircd.conf** ``# This configuration has been automatically generated # by the GNOME LIRC Properties control panel. # # Feel free to add any custom remotes to the configuration # via additional include directives or below the existing # include directives from your selected remote and/or # transmitter. #`` # Configuration selected with GNOME LIRC Properties # include begin remote name AppleRemote bits 8 eps 30 aeps 100 one 0 0 zero 0 0 pre\\_data\\_bits 24 pre\\_data 0x87EE81 gap 211982 toggle\\_bit\\_mask 0x0 ignore\\_mask 0x0000ff01 begin codes KEY\\_VOLUMEUP 0x0B KEY\\_VOLUMEDOWN 0x0D KEY\\_PREVIOUSSONG 0x08 KEY\\_NEXTSONG 0x07 KEY\\_PLAYPAUSE 0x04 KEY\\_MENU 0x02 end codes end remote /etc/modules # /etc/modules: kernel modules to load at boot time. # # This file contains the names of kernel modules that should be loaded # at boot time, one per line. Lines beginning with \"#\" are ignored. lp usbhid applesmc /etc/modprobe.d/blacklist blacklist applesmc blacklist usbhid Restart the lirc daemon after adopted the configuration: $ /etc/init.d/lirc restart To see if the daemon successfully started and is using the right driver: $ ps aux | grep lirc If everything went well you should be able to use the remote without any hassle and you could use the apple hardware user experience on a linux distribution!","tags":"apple","url":"https://visibilityspots.github.io/blog/apple-linux-remote.html","loc":"https://visibilityspots.github.io/blog/apple-linux-remote.html"},{"title":"Symbolic linux links","text":"It's rather simple, but I used to look for it a while when writing my first bash/python scripts. Wanted to typing in one command so I would need to type in every time the whole path to my newly written script. That way routine tasks could be called much faster and easier. This can be done by creating a symlink to your /usr/bin directory: ln -s /path/to/your/script /usr/bin/nameOfTheOverallCommmandYouWantToUseForYourScript","tags":"linux","url":"https://visibilityspots.github.io/blog/symbolic-links.html","loc":"https://visibilityspots.github.io/blog/symbolic-links.html"},{"title":"Permissions website","text":"The most recommended permissions for files and directories on the web are 0755 and 0644. If you have shell access to your webserver you can set those permissions using those commands: find -type d -print0 | xargs -0 chmod 755 find -type f -print0 | xargs -0 chmod 644","tags":"security","url":"https://visibilityspots.github.io/blog/web-permissions.html","loc":"https://visibilityspots.github.io/blog/web-permissions.html"},{"title":"Conky","text":"To monitor the different resources of my local system I use conky. After you installed the conky software you can start with the configuration of it. $ apt-get install conky conky-colors After I adapted the configuration my desktop became like this: At the left side there is a pane which only monitors my system resources. The config file for it, conkyrc should be placed in your home directory as a hidden file (naming it .conkyrc). When you now type in conky in your terminal, you should see appearing the pane on your desktop: On the right bottom I created an rss feeds pane. In the file conkyrc2 some parameters needs your attention. After you found your rss feeds, you can displays them by this instruction (10 stands for the refresh interval in minutes, 5 for the last 5 items of your feed): ${rss http://jouwfeed 10 item_titles 5} Once you adopted the file and placed in as a hidden file in your home directory you can start the monitor by: conky -c ~/.conkyrc2 Using the parameter -d you can force the service to start up as a background daemon process: conky -c ~/.conkyrc2 -d The last pane, on the right top, I configured with for monitoring my social networks and communication. In the conkyrc3 file you need to adopt the twitter and linkedin feed. For the twitter rss feed you need 3 params: username, password, and a token. To find your token, surf to and click on the right bottom to get your rss feed. In your browser url address bar you can find the token at the end of the url XXXXXXXX.rss. ${rss http://twitternaam:twitterwachtwoord@twitter.com/statuses/friends_timeline/twittercode.rss 20 item_titles 2} For the linkedin rss feed, you need to log in on linkedin.com and search for your own rss feed on the homepage. To display pidgin statuses I used the conkyPidgin module. After you adopted the last configuration file once again, place it in your homedir, hide it and start it in your terminal: conky -c ~/.conkyrc2 To automatically start up the conky daemons, you could call a .startConky.sh script with this code: #!/bin/bash sleep 30 && conky -d; sleep 40 && conky -c ~/.conkyrc2 -d; sleep 50 && conky -c ~/.conkyrc3 -d; chmod +x .startConky.sh Then add this command to menu System - Preferences - Startup Applications: ~/.start_conky.sh This way you also could monitor your system in a fancy way :)","tags":"linux","url":"https://visibilityspots.github.io/blog/conky.html","loc":"https://visibilityspots.github.io/blog/conky.html"},{"title":"Showing birthdays using php","text":"For our local scouting group it seemed nice to write a birthday script which displays for every member on the day of his/her birthday the name and age on our homepage. Something like \"We wish XXX a happy # anniversary!' To accomplish this I wrote a php script which gets the data of our members from a mysql db and shows the messages on the right day on our website. In the meantime also an automatic mail will be send to the person with some sort of 'personal' message. database connection (db_connectPear.php) < ?php $dsn = array( 'phptype' => 'mysql', 'username' => 'DBUSERNAME', 'password' => 'DBPASSWORD', 'hostspec' => 'localhost', 'database' => $database, ); $db_object = DB::connect($dsn, TRUE); if(DB::isError($db_object)) { die($db_object->getMessage()); } $db_object->setFetchMode(DB_FETCHMODE_ASSOC); ?> birthday script (birthday.php) < ?php $database = 'DBNAME'; require 'db_connectPear.php'; $selectLeden = 'SELECT *, round((to_days(now())-to_days(Geboortedatum))/365) as leeftijd FROM `ledenlijst` WHERE ' . ' DAYOFMONTH(Geboortedatum)=date_format(now(),\\'%d\\') AND ' . ' MONTH(Geboortedatum)=date_format(now(),\\'%c\\') ORDER BY voornaam ASC'; $queryLeden = mysql_query($selectLeden)or die(mysql_error()); $selectStam = 'SELECT *, round((to_days(now())-to_days(Geboortedatum))/365) as leeftijd FROM `stam` WHERE ' . ' DAYOFMONTH(Geboortedatum)=date_format(now(),\\'%d\\') AND ' . ' MONTH(Geboortedatum)=date_format(now(),\\'%c\\') ORDER BY voornaam ASC'; $queryStam = mysql_query($selectStam)or die(mysql_error()); if (mysql_num_rows($queryLeden) == 0) { if (mysql_num_rows($queryStam) == 0) { } else { echo \" Wij wensen\"; while($list = mysql_fetch_object($queryStam)){ $naam = $list->voornaam.\" \".$list->achternaam; echo (\"$naam, \"); if ($list->mailVerjaardag == 'n') { $tekst=\"Beste \".$list->voornaam.\"\\n \\n Een gelukkige verjaardag, \".$list->leeftijd .\" jaar is niet niks, geniet van deze mooie dag. \\n \\n De leiding \\n \"; $email = $list->email; $onderwerp=\"Gelukkige Verjaardag!\"; $headers = \"From: Naam \\r\\n\"; mail($email,$onderwerp,$tekst,$headers); $sql = \"UPDATE stam SET mailVerjaardag = 'y' WHERE id = '$list->id'\"; mysql_query($sql)or die(mysql_error()); } } echo \"een gelukkige verjaardag!\"; } } else { echo \" Wij wensen \"; $setIntro = 1; while($list = mysql_fetch_object($queryLeden)){ $naam = $list->voornaam.\" \".$list->achternaam; echo (\"$naam - ($list->leeftijd jaar), \"); if ($list->mailVerjaardag == 'n') { $tekst=\"Beste \".$list->voornaam.\"\\n \\n Een gelukkige verjaardag, \".$list->leeftijd .\" jaar is niet niks, geniet van deze mooie dag. \\n \\n De leiding\"; $email = $list->email; $onderwerp=\"Gelukkige Verjaardag!\"; $headers = \"From: Naam \\r\\n\"; mail($email,$onderwerp,$tekst,$headers); $sql = \"UPDATE ledenlijst SET mailVerjaardag = 'y' WHERE ledenlijst_id = '$list->ledenlijst_id'\"; mysql_query($sql)or die(mysql_error()); } } if (mysql_num_rows($queryStam) == 0) { } else { if ($setIntro != 1){ echo \" Wij wensen \"; } while($list = mysql_fetch_object($queryStam)){ $naam = $list->voornaam.\" \".$list->achternaam; echo (\"$naam, \"); if ($list->mailVerjaardag == 'n') { $tekst=\"Beste \".$list->voornaam.\"\\n \\n Een gelukkige verjaardag, \".$list->leeftijd .\" jaar is niet niks, geniet van deze mooie dag. \\n \\n De leiding\"; $email = $list->email; $onderwerp=\"Gelukkige Verjaardag!\"; $headers = \"From: Naam \\r\\n\"; mail($email,$onderwerp,$tekst,$headers); $sql = \"UPDATE stam SET mailVerjaardag = 'y' WHERE id = '$list->id'\"; mysql_query($sql)or die(mysql_error()); } } } echo \"een gelukkige verjaardag!\"; } ?>","tags":"php","url":"https://visibilityspots.github.io/blog/birthday-script.html","loc":"https://visibilityspots.github.io/blog/birthday-script.html"}]};